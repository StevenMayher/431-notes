[{"path":"index.html","id":"working-with-these-notes","chapter":"Working with These Notes","heading":"Working with These Notes","text":"document broken multiple chapters. Use table contents left side screen navigate chapters, use right side navigate within current chapter.can also search document, using automated index.code provided document can copied clipboard using Copy icon top right code block.document updated (unpredictably) throughout semester.","code":""},{"path":"index.html","id":"the-431-course-online","chapter":"Working with These Notes","heading":"The 431 Course online","text":"main web page 431 course Fall 2021 https://thomaselove.github.io/431/. Go information related course.","code":""},{"path":"index.html","id":"what-youll-find-here","chapter":"Working with These Notes","heading":"What You’ll Find Here","text":"Notes provide series examples using R work issues likely come PQHS/CRSP/MPHP 431. mostly find brief explanations key idea summary, accompanied (time) R code demonstration results applying code.Notes share features textbook, neither comprehensive completely original. main purpose give 431 students set common materials draw course. class, sometimes:reiterate points made document,amplify ,simplify presentation things done ,use new examples show techniques,refer issues mentioned document,don’t follow notes precisely. assume instead read materials try learn , just attend classes try learn . welcome feedback kinds document anything else.Everything see available HTML. also access R Markdown files, contain code generates everything document, including R results. demonstrate use R Markdown (document generated additional help R package called bookdown) RStudio (“program” use interface R language) class.data R code related notes also available .","code":""},{"path":"index.html","id":"setting-up-r","chapter":"Working with These Notes","heading":"Setting Up R","text":"Notes make extensive use ofthe statistical software language R, andthe development environment R Studio,free, ’ll need install machine. Instructions found course syllabus.need even gentler introduction, ’re just new R RStudio need learn , encourage take look http://moderndive.com/, provides introduction statistical data sciences via R Chester Ismay Albert Y. Kim.1These notes written using R Markdown. R Markdown, like R R Studio, free open source.R Markdown described authoring framework data science, lets yousave execute R codegenerate high-quality reports can shared audienceThis description comes http://rmarkdown.rstudio.com/lesson-1.html can visit get overview quick tour ’s possible R Markdown.Another excellent resource learn R Markdown tools Communicate section (especially R Markdown chapter) Garrett Grolemund Hadley Wickham.2","code":""},{"path":"index.html","id":"initial-setup-of-r-packages","chapter":"Working with These Notes","heading":"Initial Setup of R Packages","text":"start, ’ll present series commands run (silently) beginning chapter Notes. particular commands set R use several packages (libraries) functions expand capabilities, make specific change want R output displayed (’s comment = NA piece) sets theme graphs build one called theme_bw(). chunk code like occur near top R Markdown work.deliberately set list loaded packages relatively small. need install package , need reload every time start new session.","code":"\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(knitr)\nlibrary(magrittr)\nlibrary(janitor)\nlibrary(NHANES)\nlibrary(palmerpenguins)\nlibrary(patchwork)\nlibrary(rms)\nlibrary(mosaic)\nlibrary(Epi)\nlibrary(naniar)\nlibrary(simputation)\nlibrary(broom) # note: tidymodels includes the broom package\nlibrary(tidyverse) # note: tidyverse includes the dplyr and ggplot2 packages\n\ntheme_set(theme_bw())"},{"path":"index.html","id":"the-love-boost.r-script","chapter":"Working with These Notes","heading":"The Love-boost.R script","text":"Starting October, ’ll make use scripts ’ve gathered . necessary, ’ll source code using following command…","code":"\nsource(\"data/Love-boost.R\")"},{"path":"index.html","id":"additional-r-packages-installed-for-this-book","chapter":"Working with These Notes","heading":"Additional R Packages installed for this book","text":"packages need installed user’s system, need loaded R order run code presented notes except specific settings. additional packages include following.","code":"boot\ncar\nequatiomatic\nGGally\nggridges\ngt\npsych\nmodelsummary\nvisdat"},{"path":"data-science.html","id":"data-science","chapter":"1 Data Science","heading":"1 Data Science","text":"definition data science can little slippery. One current view data science, exemplified Steven Geringer’s 2014 Venn diagram.\nFigure 1.1: Data Science Venn Diagram Steven Geringer\nfield encompasses ideas mathematics statistics computer science, heavy reliance subject-matter knowledge. case, includes clinical, health-related, medical biological knowledge.Andrew Gelman Deborah Nolan3 suggest, experience intuition necessary good statistical practice hard obtain, teaching data science provides excellent opportunity reinforce statistical thinking skills across full cycle data analysis project.principal form computer science (coding/programming) play role course provide form communication. ’ll need learn express ideas just orally writing, also code.Data Science team activity. Everyone working data science brings part necessary skill set, one person can cover three areas alone excellent projects.[individual truly expert three key areas (mathematics/statistics, computer science subject-matter knowledge) ] mythical beast magical powers ’s rumored exist never actually seen wild.http://www.kdnuggets.com/2016/10/battle-data-science-venn-diagrams.html","code":""},{"path":"data-science.html","id":"data-science-project-cycle","chapter":"1 Data Science","heading":"1.1 Data Science Project Cycle","text":"typical data science project can modeled follows, comes introduction amazing book R Data Science, Garrett Grolemund Hadley Wickham, key text course.4\nFigure 1.2: Source: R Data Science: Introduction\ndiagram sometimes referred Krebs Cycle Data Science. steps data science project, encourage read Introduction Grolemund Wickham.5","code":""},{"path":"data-science.html","id":"data-science-and-the-431-course","chapter":"1 Data Science","heading":"1.2 Data Science and the 431 Course","text":"’ll discuss elements 431 course, focusing start understanding data transformation, modeling (especially early stages) visualization. 431, learn get things done.get people working R R Studio R Markdown, even completely new coding. gentle introduction provided Ismay Kim6We learn use tidyverse (http://www.tidyverse.org/), array tools R (mostly developed Hadley Wickham colleagues R Studio) share underlying philosophy make data science faster, easier, reproducible fun. critical text understanding tidyverse Grolemund Wickham.7 Tidyverse tools facilitate:\nimporting data R, can source intense pain things, really quite easy 95% time right tool.\ntidying data, , storing format includes one row per observation one column per variable. harder, important, might think.\ntransforming data, perhaps identifying specific subgroups interest, creating new variables based existing ones, calculating summaries.\nvisualizing data generate actual knowledge identify questions data - area R really shines, ’ll start class.\nmodeling data, taking approach modeling complementary visualization, allows us answer questions visualization helps us identify.\nlast, definitely least, communicating results, models visualizations others, way reproducible effective.\nimporting data R, can source intense pain things, really quite easy 95% time right tool.tidying data, , storing format includes one row per observation one column per variable. harder, important, might think.transforming data, perhaps identifying specific subgroups interest, creating new variables based existing ones, calculating summaries.visualizing data generate actual knowledge identify questions data - area R really shines, ’ll start class.modeling data, taking approach modeling complementary visualization, allows us answer questions visualization helps us identify.last, definitely least, communicating results, models visualizations others, way reproducible effective.programming/coding inevitable requirement accomplish aims. leery coding, ’ll need get past , help course stellar teaching assistants. Getting started always challenging part, experience pain developing new skills evaporates early October.","code":""},{"path":"data-science.html","id":"what-the-course-is-and-isnt","chapter":"1 Data Science","heading":"1.3 What The Course Is and Isn’t","text":"431 course getting things done. developing course, adopt modern approach places data center work. goal teach truly reproducible research modern tools. want able collect use data effectively address questions interest.curriculum includes several topics might expect standard graduate introduction biostatistics.data gatheringdata wranglingexploratory data analysis visualizationmultivariate modelingcommunicationIt also nearly completely avoids formalism extremely applied - absolutely course theoretical mathematical statistics, Notes reflect approach.’s little mathematical underpinnings :\\[\nf(x) = \\frac{e^{-(x - \\mu)^{2}/(2\\sigma^{2})}}{\\sigma{\\sqrt{2 \\pi }}} \n\\]Instead, notes (course) focus get R things want , interpret results work. next Chapter provides first example.","code":""},{"path":"the-palmer-penguins.html","id":"the-palmer-penguins","chapter":"2 The Palmer Penguins","heading":"2 The Palmer Penguins","text":"data palmerpenguins package R include size measurements, clutch observations, blood isotope ratios adult foraging Adelie, Chinstrap, Gentoo penguins observed islands Palmer Archipelago near Palmer Station, Antarctica8. data collected made available Dr. Kristen Gorman Palmer Station Long Term Ecological Research (LTER) Program.palmerpenguins package, visit https://allisonhorst.github.io/palmerpenguins/.","code":""},{"path":"the-palmer-penguins.html","id":"package-loading-then-dealing-with-missing-data","chapter":"2 The Palmer Penguins","heading":"2.1 Package Loading, then Dealing with Missing Data","text":"start, let’s load necessary R packages manage data summarize small table, plot. ’ve actually done previously, ’ll repeat steps , ’s worth seeing R .case, ’ll load five packages.’s worth remembering everything # line just comment reader, ignored R. ’ll see later loading single package (called tidyverse) gives us dplyr ggplot2 packages, well several useful things.Next, let’s take penguins data palmerpenguins package, identify observations complete data (, missing values) four variables interest. ’ll store result new data frame (think data set) called new_penguins take look result using following code.","code":"\nlibrary(palmerpenguins)  # source for the data set\nlibrary(janitor)         # some utilities for cleanup and simple tables\nlibrary(magrittr)        # provides us with the pipe %>% for code management\nlibrary(dplyr)           # part of the tidyverse: data management tools\nlibrary(ggplot2)         # part of the tidyverse: tools for plotting data\nnew_penguins <- penguins %>%\n    filter(complete.cases(flipper_length_mm, body_mass_g, species, sex))\n\nnew_penguins# A tibble: 333 x 8\n   species island    bill_length_mm bill_depth_mm\n   <fct>   <fct>              <dbl>         <dbl>\n 1 Adelie  Torgersen           39.1          18.7\n 2 Adelie  Torgersen           39.5          17.4\n 3 Adelie  Torgersen           40.3          18  \n 4 Adelie  Torgersen           36.7          19.3\n 5 Adelie  Torgersen           39.3          20.6\n 6 Adelie  Torgersen           38.9          17.8\n 7 Adelie  Torgersen           39.2          19.6\n 8 Adelie  Torgersen           41.1          17.6\n 9 Adelie  Torgersen           38.6          21.2\n10 Adelie  Torgersen           34.6          21.1\n# ... with 323 more rows, and 4 more variables:\n#   flipper_length_mm <int>, body_mass_g <int>, sex <fct>,\n#   year <int>"},{"path":"the-palmer-penguins.html","id":"counting-things-and-making-tables","chapter":"2 The Palmer Penguins","heading":"2.2 Counting Things and Making Tables","text":", many penguins new_penguins data? printed result, got answer, (many things R) many ways get result.new_penguins data break sex species?Note strange spelling tabyl . output reasonably clear, make table little prettier, ’re , can add row column totals ?","code":"\nnrow(new_penguins)[1] 333\nnew_penguins %>% \n    tabyl(sex, species) # tabyl comes from the janitor package    sex Adelie Chinstrap Gentoo\n female     73        34     58\n   male     73        34     61\nnew_penguins %>% \n    tabyl(sex, species) %>%\n    adorn_totals(where = c(\"row\", \"col\")) %>% # add row, column totals\n    kable  # one convenient way to make the table prettier"},{"path":"the-palmer-penguins.html","id":"visualizing-the-data-in-a-graph-or-a-few","chapter":"2 The Palmer Penguins","heading":"2.3 Visualizing the Data in a Graph (or a few…)","text":"Now, let’s look two variables interest. Let’s create graph showing association body mass flipper length across complete set 333 penguins.may want include straight-line model (fit classical linear regression) plot. One way R involves addition single line code, like :Whenever build graph , default choices may sufficient. ’d like see prettier version going show someone else. , might use different color species, might neaten theme (get rid default grey background) add title, like .","code":"\nggplot(new_penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n    geom_point() \nggplot(new_penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x,\n                col = \"red\", se = FALSE)\nggplot(new_penguins, aes(x = body_mass_g, y = flipper_length_mm, col = species)) +\n    geom_point() + \n    theme_bw() + \n    labs(title = \"Flipper Length and Body Mass for 333 of the Palmer Penguins\")"},{"path":"the-palmer-penguins.html","id":"six-ways-to-improve-this-graph","chapter":"2 The Palmer Penguins","heading":"2.4 Six Ways To “Improve” This Graph","text":"Now, let’s build new graph. , want :plot relationship body mass flipper length light Sex Speciesincrease size points add little transparency can see points overlap,add smooth curves summarize relationships two quantities (body mass flipper length) within combination species sex,split graph two “facets” (one sex),improve axis labels,improve titles adding subtitle, also adding code count penguins (rather hard-coding total number.)","code":"\nggplot(new_penguins, aes(x = body_mass_g, y = flipper_length_mm, \n                         col = species)) +\n    geom_point(size = 2, alpha = 0.5) + \n    geom_smooth(method = \"loess\", formula = y ~ x, \n                se = FALSE, size = 1.5) +\n    facet_grid(~ sex) +\n    theme_bw() + \n    labs(title = \"Flipper Length and Body Mass, by Sex & Species\",\n         subtitle = paste0(nrow(new_penguins), \" of the Palmer Penguins\"),\n         x = \"Body Mass (g)\", \n         y = \"Flipper Length (mm)\")"},{"path":"the-palmer-penguins.html","id":"a-little-reflection","chapter":"2 The Palmer Penguins","heading":"2.5 A Little Reflection","text":"can learn plots construction? particular,plots suggest center distribution quantity (body mass flipper length) overall, within combination Sex Species?final plot suggest spread distribution quantities combination Sex Species?plots suggest association body mass flipper length across complete set penguins?shape nature body mass - flipper length relationship change based Sex Species?think helpful plot straight-line relationship (rather smooth curve) within combination Sex Species final plot? ? (Also, code accomplish ?)R code plot revised accomplish six “wants” specified ?","code":""},{"path":"nhanes1.html","id":"nhanes1","chapter":"3 NHANES: A First Look","heading":"3 NHANES: A First Look","text":"Next, ’ll explore data US National Health Nutrition Examination Survey, NHANES.’ll display R code go, ’ll return key coding ideas involved later Notes.","code":""},{"path":"nhanes1.html","id":"the-nhanes-data-a-first-sample","chapter":"3 NHANES: A First Look","heading":"3.1 The NHANES data: A First Sample","text":"NHANES package provides sample 10,000 NHANES responses 2009-10 2011-12 administrations, data frame also called NHANES. can obtain dimensions data frame (think rectangle data) dim() function.see 10000 rows 76 columns NHANES data frame.moment, let’s gather random sample 1,000 responses 10000 rows listed NHANES data frame, look three variables (labeled Gender, Age Height) describe subjects9. motivation example came Figure Benjamin S. Baumer, Daniel T. Kaplan, Nicholas J. Horton.10We 1000 rows (observations) 5 columns (variables) describe responses listed rows.","code":"\ndim(NHANES)[1] 10000    76\n# library(NHANES) # already loaded NHANES package/library of functions, data\n\nset.seed(431001) \n# use set.seed to ensure that we all get the same random sample \n# of 1,000 NHANES subjects in our nh_1 collection\n\nnh_1 <- \n    slice_sample(NHANES, n = 1000, replace = FALSE) %>%\n    select(ID, SurveyYr, Gender, Age, Height)\n\nnh_1# A tibble: 1,000 x 5\n      ID SurveyYr Gender   Age Height\n   <int> <fct>    <fct>  <int>  <dbl>\n 1 69638 2011_12  female     5   106.\n 2 70782 2011_12  male      64   176.\n 3 52408 2009_10  female    54   162.\n 4 59031 2009_10  female    15   155.\n 5 64530 2011_12  male      53   185.\n 6 71040 2011_12  male      63   169.\n 7 55186 2009_10  female    30   168.\n 8 60211 2009_10  male       5   103.\n 9 55730 2009_10  male      66   161.\n10 68229 2011_12  female    36   170.\n# ... with 990 more rows"},{"path":"nhanes1.html","id":"a-quick-numerical-summary","chapter":"3 NHANES: A First Look","heading":"3.2 A Quick Numerical Summary","text":"two variables R recognizes describing categories, SurveyYr Gender, numeric summary provides small table counts. Age Height variables, see minimum, mean, maximum summary statistics.","code":"\nsummary(nh_1)       ID           SurveyYr      Gender         Age       \n Min.   :51624   2009_10:512   female:504   Min.   : 0.00  \n 1st Qu.:57011   2011_12:488   male  :496   1st Qu.:18.00  \n Median :61979                              Median :36.00  \n Mean   :61903                              Mean   :37.42  \n 3rd Qu.:67178                              3rd Qu.:56.00  \n Max.   :71875                              Max.   :80.00  \n                                                           \n     Height     \n Min.   : 85.0  \n 1st Qu.:156.2  \n Median :165.0  \n Mean   :162.3  \n 3rd Qu.:174.5  \n Max.   :195.9  \n NA's   :37     "},{"path":"nhanes1.html","id":"plotting-age-vs.-height","chapter":"3 NHANES: A First Look","heading":"3.3 Plotting Age vs. Height","text":"Suppose want visualize relationship Height Age 1,000 NHANES observations. best choice likely scatterplot.note several interesting results .warning, R tells us “Removed 37 rows containing missing values (geom_point).” 963 subjects plotted , remaining 37 people missing (NA) values either Height, Age .Unsurprisingly, measured Heights subjects grow Age 0 Age 20 , see typical Height increases rapidly across Ages. middle distribution later Ages pretty consistent Height somewhere 150 175. units aren’t specified, expect must centimeters. Ages clearly reported Years.Age reported 80, appears large cluster Ages 80. may due requirement Ages 80 reported 80 help mask identity individuals.11As case, ’re going build visualizations using tools ggplot2 package, part tidyverse series packages. ’ll see similar coding structures throughout Chapter, covered well Chapter 3 Grolemund Wickham.12","code":"\nggplot(data = nh_1, aes(x = Age, y = Height)) +\n    geom_point()Warning: Removed 37 rows containing missing values\n(geom_point)."},{"path":"nhanes1.html","id":"restriction-to-complete-cases","chapter":"3 NHANES: A First Look","heading":"3.4 Restriction to Complete Cases","text":"move , let’s manipulate data frame bit, focus subjects complete data Age Height. help us avoid warning message.Note units explanations variables contained NHANES help file, available via typing ?NHANES Console R Studio, typing NHANES Search bar R Studio’s Help window.","code":"\nnh_1cc <- nh_1 %>%\n    filter(complete.cases(Age, Height)) \n\nsummary(nh_1cc)       ID           SurveyYr      Gender         Age       \n Min.   :51624   2009_10:487   female:484   Min.   : 2.00  \n 1st Qu.:57034   2011_12:476   male  :479   1st Qu.:19.00  \n Median :62056                              Median :37.00  \n Mean   :61967                              Mean   :38.29  \n 3rd Qu.:67269                              3rd Qu.:56.00  \n Max.   :71875                              Max.   :80.00  \n     Height     \n Min.   : 85.0  \n 1st Qu.:156.2  \n Median :165.0  \n Mean   :162.3  \n 3rd Qu.:174.5  \n Max.   :195.9  "},{"path":"nhanes1.html","id":"the-distinction-between-gender-and-sex","chapter":"3 NHANES: A First Look","heading":"3.5 The Distinction between Gender and Sex","text":"Gender variable mis-named. data refer biological status subjects, Sex, social construct Gender can quite different. effort avoid confusion, ’ll rename variable Gender Sex accurately describe actually measured ., can use approach…’s better. many observations now? use dim find number rows columns new data frame., simply list data frame read result.","code":"\nnh_1cc <- nh_1 %>%\n    rename(Sex = Gender) %>%\n    filter(complete.cases(Age, Height)) \n\nsummary(nh_1cc)       ID           SurveyYr       Sex           Age       \n Min.   :51624   2009_10:487   female:484   Min.   : 2.00  \n 1st Qu.:57034   2011_12:476   male  :479   1st Qu.:19.00  \n Median :62056                              Median :37.00  \n Mean   :61967                              Mean   :38.29  \n 3rd Qu.:67269                              3rd Qu.:56.00  \n Max.   :71875                              Max.   :80.00  \n     Height     \n Min.   : 85.0  \n 1st Qu.:156.2  \n Median :165.0  \n Mean   :162.3  \n 3rd Qu.:174.5  \n Max.   :195.9  \ndim(nh_1cc)[1] 963   5\nnh_1cc# A tibble: 963 x 5\n      ID SurveyYr Sex      Age Height\n   <int> <fct>    <fct>  <int>  <dbl>\n 1 69638 2011_12  female     5   106.\n 2 70782 2011_12  male      64   176.\n 3 52408 2009_10  female    54   162.\n 4 59031 2009_10  female    15   155.\n 5 64530 2011_12  male      53   185.\n 6 71040 2011_12  male      63   169.\n 7 55186 2009_10  female    30   168.\n 8 60211 2009_10  male       5   103.\n 9 55730 2009_10  male      66   161.\n10 68229 2011_12  female    36   170.\n# ... with 953 more rows"},{"path":"nhanes1.html","id":"age-height-by-sex","chapter":"3 NHANES: A First Look","heading":"3.6 Age-Height by Sex?","text":"Let’s add Sex plot using color, also adjust y axis label incorporate units measurement.","code":"\nggplot(data = nh_1cc, aes(x = Age, y = Height, color = Sex)) +\n    geom_point() +\n    labs(title = \"Height-Age Relationship in NHANES sample\", \n         y = \"Height in cm.\")"},{"path":"nhanes1.html","id":"can-we-show-the-female-and-male-relationships-in-separate-panels","chapter":"3 NHANES: A First Look","heading":"3.6.1 Can we show the Female and Male relationships in separate panels?","text":"Sure.","code":"\nggplot(data = nh_1cc, aes(x = Age, y = Height, color = Sex)) +\n    geom_point() + \n    labs(title = \"Height-Age Relationship in NHANES sample\", \n         y = \"Height in cm.\") +\n    facet_wrap(~ Sex)"},{"path":"nhanes1.html","id":"can-we-add-a-smooth-curve-to-show-the-relationship-in-each-plot","chapter":"3 NHANES: A First Look","heading":"3.6.2 Can we add a smooth curve to show the relationship in each plot?","text":"Yes, adding call geom_smooth() function.","code":"\nggplot(data = nh_1cc, aes(x = Age, y = Height, color = Sex)) +\n    geom_point() + \n    geom_smooth(method = \"loess\", formula = y ~ x) +\n    labs(title = \"Height-Age Relationship in NHANES sample\", \n         y = \"Height in cm.\") +\n    facet_wrap(~ Sex)"},{"path":"nhanes1.html","id":"what-if-we-want-to-assume-straight-line-relationships","chapter":"3 NHANES: A First Look","heading":"3.6.3 What if we want to assume straight line relationships?","text":"look linear model part plot instead. make sense ?seems like complex relationship Height Age isn’t well described straight line model.","code":"\nggplot(data = nh_1cc, aes(x = Age, y = Height, color = Sex)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", formula = y ~ x) +\n    labs(title = \"Height-Age Relationship in NHANES sample\", \n         y = \"Height in cm.\") +\n    facet_wrap(~ Sex)"},{"path":"nhanes1.html","id":"combining-plots-with-patchwork","chapter":"3 NHANES: A First Look","heading":"3.7 Combining Plots with patchwork","text":"patchwork package R allows us use simple commands put two plots together.Suppose create plots called p1 p2, follows.Now, suppose want put together single figure. Thanks patchwork, can simply type following.can place images next , add annotation, like :patchwork package website provides lots great examples guides make easy combine separate ggplots graphic. packages (gridExtra cowplot nice, instance) task, think patchwork user-friendly, ’s focus notes.","code":"\np1 <- ggplot(data = nh_1cc, aes(x = Age, y = Height)) +\n    geom_point() + \n    labs(title = \"Height and Age\")\n\np2 <- ggplot(data = nh_1cc, aes(x = Sex, y = Height)) +\n    geom_point() +\n    labs(title = \"Height, by Sex\")\np1 / p2\np1 + p2 +\n    plot_annotation(title = \"Our Combined Plots\")"},{"path":"nhanes1.html","id":"coming-up","chapter":"3 NHANES: A First Look","heading":"3.8 Coming Up","text":"Next, ’ll select new sample NHANES respondents bit carefully, introduce new ways thinking data variables, ’ll study subjects greater detail.","code":""},{"path":"data-structures-variable-types-sampling-nhanes.html","id":"data-structures-variable-types-sampling-nhanes","chapter":"4 Data Structures, Variable Types & Sampling NHANES","heading":"4 Data Structures, Variable Types & Sampling NHANES","text":"","code":""},{"path":"data-structures-variable-types-sampling-nhanes.html","id":"data-require-structure-and-context","chapter":"4 Data Structures, Variable Types & Sampling NHANES","heading":"4.1 Data require structure and context","text":"Descriptive statistics concerned presentation, organization summary data, suggested Geoffrey R. Norman David L. Streiner.13 includes various methods organizing graphing data get idea data can tell us.Eric Vittinghoff et al.14 suggest, nature measurement determines best describe statistically, main distinction numerical categorical variables. Even little tricky - plenty data can values look like numerical values, just numerals serving labels.David E. Bock, Paul F. Velleman, Richard D. De Veaux15 point , truly critical notion, course, data values, matter kind, useless without contexts. Five W’s (, [units], , , , often ) just useful establishing context data journalism. can’t answer , particular, don’t useful information.general, row data frame corresponds individual (respondent, experimental unit, record, observation) characteristics gathered columns (characteristics may called variables, factors data elements.) Every column / variable name indicates measuring, every row / observation name indicates measured.","code":""},{"path":"data-structures-variable-types-sampling-nhanes.html","id":"newNHANES","chapter":"4 Data Structures, Variable Types & Sampling NHANES","heading":"4.2 Sampling Adults in NHANES","text":"Chapter 3, spent time sample National Health Nutrition Examination. Now, changing value set.seed function determines starting place random sampling, changing specifications, ’ll generate new sample describing 750 unique (distinct) adult subjects completed 2011-12 version survey ages 21 64.","code":""},{"path":"data-structures-variable-types-sampling-nhanes.html","id":"creating-a-temporary-cleaner-data-frame","chapter":"4 Data Structures, Variable Types & Sampling NHANES","heading":"4.2.1 Creating a Temporary, Cleaner Data Frame","text":"’ll start describing plan use create new data frame called nh_temp eventually build final sample. particular, let lay steps use create nh_temp frame original NHANES data frame available R package called NHANES.’ll filter original NHANES data frame include responses 2011-12 administration survey. cut sample half, 10,000 rows 5,000.’ll filter original NHANES data frame include responses 2011-12 administration survey. cut sample half, 10,000 rows 5,000.’ll filter restrict sample adults whose age least 21 also less 65. ’ll want avoid problems including children adults sample, also want focus population people US usually covered private insurance job, Medicaid insurance government, rather covered Medicare.’ll filter restrict sample adults whose age least 21 also less 65. ’ll want avoid problems including children adults sample, also want focus population people US usually covered private insurance job, Medicaid insurance government, rather covered Medicare.discussed previously, listed NHANES data frame Gender correctly referred Sex. Sex biological feature individual, Gender social construct. important distinction, ’ll change name variable.discussed previously, listed NHANES data frame Gender correctly referred Sex. Sex biological feature individual, Gender social construct. important distinction, ’ll change name variable.’ll also rename three variables, specifically ’ll use Race describe Race3 variable original NHANES data frame, well SBP refer average systolic blood pressure, specified BPSysAve, DBP refer average diastolic blood pressure, specified BPDiaAve.’ll also rename three variables, specifically ’ll use Race describe Race3 variable original NHANES data frame, well SBP refer average systolic blood pressure, specified BPSysAve, DBP refer average diastolic blood pressure, specified BPDiaAve.accomplished previous four steps, ’ll select variables want keep sample. (use select choosing variables columns data frame, filter selecting subjects rows.) sixteen variables select : ID, Sex, Age, Height, Weight, Race, Education, BMI, SBP, DBP, Pulse, PhysActive, Smoke100, SleepTrouble, MaritalStatus HealthGen.accomplished previous four steps, ’ll select variables want keep sample. (use select choosing variables columns data frame, filter selecting subjects rows.) sixteen variables select : ID, Sex, Age, Height, Weight, Race, Education, BMI, SBP, DBP, Pulse, PhysActive, Smoke100, SleepTrouble, MaritalStatus HealthGen.original NHANES data frame includes subjects (rows) multiple times effort incorporate sampling weights used NHANES analyses. purposes, though, ’d like include subject one time. use distinct() function limit data frame completely unique subjects (, example, don’t wind two rows ID number.)original NHANES data frame includes subjects (rows) multiple times effort incorporate sampling weights used NHANES analyses. purposes, though, ’d like include subject one time. use distinct() function limit data frame completely unique subjects (, example, don’t wind two rows ID number.)code used complete six steps listed create nh_temp data frame.resulting nh_temp data frame 1700 rows 16 columns.","code":"\nnh_temp <- NHANES %>%\n    filter(SurveyYr == \"2011_12\") %>%\n    filter(Age >= 21 & Age < 65) %>%\n    rename(Sex = Gender, Race = Race3, SBP = BPSysAve, DBP = BPDiaAve) %>%\n    select(ID, Sex, Age, Height, Weight, Race, Education, BMI, SBP, DBP, \n           Pulse, PhysActive, Smoke100, SleepTrouble, \n           MaritalStatus, HealthGen) %>%\n   distinct()\nnh_temp# A tibble: 1,700 x 16\n      ID Sex      Age Height Weight Race     Education   BMI\n   <int> <fct>  <int>  <dbl>  <dbl> <fct>    <fct>     <dbl>\n 1 62172 female    43   172    98.6 Black    High Sch~  33.3\n 2 62176 female    34   172.   68.7 White    College ~  23.3\n 3 62180 male      35   179.   89   White    College ~  27.9\n 4 62199 male      57   186    96.9 White    College ~  28  \n 5 62205 male      28   171.   84.8 White    College ~  28.9\n 6 62206 female    35   167.   81.5 White    Some Col~  29.1\n 7 62208 male      38   169.   63.2 Hispanic Some Col~  22.2\n 8 62209 female    62   143.   53.5 Mexican  8th Grade  26  \n 9 62220 female    31   167.  113.  Black    College ~  40.4\n10 62222 male      32   179    80.1 White    College ~  25  \n# ... with 1,690 more rows, and 8 more variables:\n#   SBP <int>, DBP <int>, Pulse <int>, PhysActive <fct>,\n#   Smoke100 <fct>, SleepTrouble <fct>,\n#   MaritalStatus <fct>, HealthGen <fct>"},{"path":"data-structures-variable-types-sampling-nhanes.html","id":"sampling-nh_temp-to-obtain-our-nh_adult750-sample","chapter":"4 Data Structures, Variable Types & Sampling NHANES","heading":"4.2.2 Sampling nh_temp to obtain our nh_adult750 sample","text":"established nh_temp sampling frame, now select random sample 750 adults 1700 available responses.use set.seed() function R set random numerical seed ensure redo work, obtain sample.\nSetting seed important part able replicate work later sampling involved.\nSetting seed important part able replicate work later sampling involved.use slice_sample() function actually draw random sample, without replacement.\n“Without replacement” means ’ve selected particular subject, won’t select .\n“Without replacement” means ’ve selected particular subject, won’t select .nh_adult750 data frame now includes 750 rows (observations) 16 variables (columns). Essentially, 16 pieces information 750 adult NHANES subjects included 2011-12 panel.","code":"\nset.seed(431002) \n# use set.seed to ensure that we all get the same random sample \n\nnh_adult750 <- slice_sample(nh_temp, n = 750, replace = F) \n\nnh_adult750# A tibble: 750 x 16\n      ID Sex      Age Height Weight Race     Education   BMI\n   <int> <fct>  <int>  <dbl>  <dbl> <fct>    <fct>     <dbl>\n 1 68648 female    30   181.   67.1 White    College ~  20.4\n 2 67200 male      30   180.   86.6 White    College ~  26.7\n 3 66404 female    35   160.   71.1 White    College ~  27.8\n 4 70535 male      40   177.   82   White    College ~  26.3\n 5 65308 female    54   151.   60.6 Mexican  8th Grade  26.6\n 6 67392 male      41   171.   90.7 Hispanic College ~  31.2\n 7 63218 male      35   163.   81   Mexican  8th Grade  30.3\n 8 65879 female    32   160.   66.4 Mexican  College ~  25.9\n 9 63617 male      29   189.   83.3 White    College ~  23.2\n10 64720 male      29   174.   62.3 Black    College ~  20.6\n# ... with 740 more rows, and 8 more variables: SBP <int>,\n#   DBP <int>, Pulse <int>, PhysActive <fct>,\n#   Smoke100 <fct>, SleepTrouble <fct>,\n#   MaritalStatus <fct>, HealthGen <fct>"},{"path":"data-structures-variable-types-sampling-nhanes.html","id":"summarizing-the-datas-structure","chapter":"4 Data Structures, Variable Types & Sampling NHANES","heading":"4.2.3 Summarizing the Data’s Structure","text":"can identify number rows columns data frame tibble dim function.str function provides lot information structure data frame tibble.see first observations, use head, see last , try tail…","code":"\ndim(nh_adult750)[1] 750  16\nstr(nh_adult750)tibble [750 x 16] (S3: tbl_df/tbl/data.frame)\n $ ID           : int [1:750] 68648 67200 66404 70535 65308 67392 63218 65879 63617 64720 ...\n $ Sex          : Factor w/ 2 levels \"female\",\"male\": 1 2 1 2 1 2 2 1 2 2 ...\n $ Age          : int [1:750] 30 30 35 40 54 41 35 32 29 29 ...\n $ Height       : num [1:750] 181 180 160 177 151 ...\n $ Weight       : num [1:750] 67.1 86.6 71.1 82 60.6 90.7 81 66.4 83.3 62.3 ...\n $ Race         : Factor w/ 6 levels \"Asian\",\"Black\",..: 5 5 5 5 4 3 4 4 5 2 ...\n $ Education    : Factor w/ 5 levels \"8th Grade\",\"9 - 11th Grade\",..: 5 5 5 5 1 5 1 5 5 5 ...\n $ BMI          : num [1:750] 20.4 26.7 27.8 26.3 26.6 31.2 30.3 25.9 23.2 20.6 ...\n $ SBP          : int [1:750] 103 113 116 130 130 124 128 104 105 127 ...\n $ DBP          : int [1:750] 59 68 80 79 64 82 96 70 72 60 ...\n $ Pulse        : int [1:750] 78 70 68 68 48 68 82 78 76 84 ...\n $ PhysActive   : Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 1 1 2 1 2 2 2 ...\n $ Smoke100     : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 2 1 2 1 2 2 ...\n $ SleepTrouble : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 1 1 1 1 1 2 1 ...\n $ MaritalStatus: Factor w/ 6 levels \"Divorced\",\"LivePartner\",..: 3 4 3 3 2 3 3 3 3 2 ...\n $ HealthGen    : Factor w/ 5 levels \"Excellent\",\"Vgood\",..: 1 1 1 2 4 3 NA 1 2 4 ...\ntail(nh_adult750, 5) # shows the last five observations in the data set# A tibble: 5 x 16\n     ID Sex      Age Height Weight Race  Education      BMI\n  <int> <fct>  <int>  <dbl>  <dbl> <fct> <fct>        <dbl>\n1 63924 female    29   165.  113.  Black High School   41.9\n2 69825 female    43   164.   63.3 White College Grad  23.7\n3 68109 male      45   170.   78.7 Black High School   27.1\n4 64598 female    60   158    74.5 White Some College  29.8\n5 64048 female    54   161.   67.5 White Some College  26.2\n# ... with 8 more variables: SBP <int>, DBP <int>,\n#   Pulse <int>, PhysActive <fct>, Smoke100 <fct>,\n#   SleepTrouble <fct>, MaritalStatus <fct>,\n#   HealthGen <fct>"},{"path":"data-structures-variable-types-sampling-nhanes.html","id":"what-are-the-variables","chapter":"4 Data Structures, Variable Types & Sampling NHANES","heading":"4.2.4 What are the variables?","text":"can use glimpse function get short preview data.variables collected described brief table below16.levels multi-categorical variables :Race: Mexican, Hispanic, White, Black, Asian, .Education: 8th Grade, 9 - 11th Grade, High School, College, College Grad.MaritalStatus: Married, Widowed, Divorced, Separated, NeverMarried LivePartner (living partner).HealthGen: Excellent, Vgood, Good, Fair Poor.details can obtained using summary function.Note appearance NA's (indicating missing values) columns, variables summarized list (categorical) values (counts) (quantitative/numeric) variables summarized minimum, quartiles means.","code":"\nglimpse(nh_adult750)Rows: 750\nColumns: 16\n$ ID            <int> 68648, 67200, 66404, 70535, 65308, 6~\n$ Sex           <fct> female, male, female, male, female, ~\n$ Age           <int> 30, 30, 35, 40, 54, 41, 35, 32, 29, ~\n$ Height        <dbl> 181.3, 180.2, 159.8, 176.6, 150.9, 1~\n$ Weight        <dbl> 67.1, 86.6, 71.1, 82.0, 60.6, 90.7, ~\n$ Race          <fct> White, White, White, White, Mexican,~\n$ Education     <fct> College Grad, College Grad, College ~\n$ BMI           <dbl> 20.4, 26.7, 27.8, 26.3, 26.6, 31.2, ~\n$ SBP           <int> 103, 113, 116, 130, 130, 124, 128, 1~\n$ DBP           <int> 59, 68, 80, 79, 64, 82, 96, 70, 72, ~\n$ Pulse         <int> 78, 70, 68, 68, 48, 68, 82, 78, 76, ~\n$ PhysActive    <fct> No, Yes, Yes, No, No, Yes, No, Yes, ~\n$ Smoke100      <fct> No, Yes, No, Yes, Yes, No, Yes, No, ~\n$ SleepTrouble  <fct> Yes, No, No, No, No, No, No, No, Yes~\n$ MaritalStatus <fct> Married, NeverMarried, Married, Marr~\n$ HealthGen     <fct> Excellent, Excellent, Excellent, Vgo~\nsummary(nh_adult750)       ID            Sex           Age       \n Min.   :62206   female:388   Min.   :21.00  \n 1st Qu.:64277   male  :362   1st Qu.:30.00  \n Median :66925                Median :40.00  \n Mean   :66936                Mean   :40.82  \n 3rd Qu.:69414                3rd Qu.:51.00  \n Max.   :71911                Max.   :64.00  \n                                             \n     Height          Weight             Race    \n Min.   :142.4   Min.   : 39.30   Asian   : 70  \n 1st Qu.:161.8   1st Qu.: 67.40   Black   :128  \n Median :168.9   Median : 80.00   Hispanic: 63  \n Mean   :168.9   Mean   : 83.16   Mexican : 80  \n 3rd Qu.:175.7   3rd Qu.: 95.30   White   :393  \n Max.   :200.4   Max.   :198.70   Other   : 16  \n NA's   :5       NA's   :5                      \n          Education        BMI             SBP       \n 8th Grade     : 50   Min.   :16.70   Min.   : 83.0  \n 9 - 11th Grade: 76   1st Qu.:24.20   1st Qu.:108.0  \n High School   :143   Median :27.90   Median :118.0  \n Some College  :241   Mean   :29.08   Mean   :118.8  \n College Grad  :240   3rd Qu.:32.10   3rd Qu.:127.0  \n                      Max.   :80.60   Max.   :209.0  \n                      NA's   :5       NA's   :33     \n      DBP             Pulse        PhysActive Smoke100 \n Min.   :  0.00   Min.   : 40.00   No :326    No :453  \n 1st Qu.: 66.00   1st Qu.: 66.00   Yes:424    Yes:297  \n Median : 73.00   Median : 72.00                       \n Mean   : 72.69   Mean   : 73.53                       \n 3rd Qu.: 80.00   3rd Qu.: 80.00                       \n Max.   :108.00   Max.   :124.00                       \n NA's   :33       NA's   :32                           \n SleepTrouble      MaritalStatus     HealthGen  \n No :555      Divorced    : 78   Excellent: 84  \n Yes:195      LivePartner : 70   Vgood    :197  \n              Married     :388   Good     :252  \n              NeverMarried:179   Fair     :104  \n              Separated   : 19   Poor     : 14  \n              Widowed     : 16   NA's     : 99  \n                                                "},{"path":"data-structures-variable-types-sampling-nhanes.html","id":"quantitative-variables","chapter":"4 Data Structures, Variable Types & Sampling NHANES","heading":"4.3 Quantitative Variables","text":"Variables recorded numbers use numbers called quantitative. Familiar examples include incomes, heights, weights, ages, distances, times, counts. quantitative variables measurement units, tell quantitative variable measured. Without units (like miles per hour, angstroms, yen degrees Celsius) values quantitative variable meaning.little good told price something don’t know currency used.little good told price something don’t know currency used.might surprised see someone whose age 72 listed database childhood diseases find age measured months.might surprised see someone whose age 72 listed database childhood diseases find age measured months.Often just seeking units can reveal variable whose definition challenging - just measure “friendliness,” “success,” example.Often just seeking units can reveal variable whose definition challenging - just measure “friendliness,” “success,” example.Quantitative variables may also classified whether continuous can take discrete set values. Continuous data may take value, within defined range. Suppose measuring height. height really continuous, measuring stick usually lets us measure certain degree precision. measurements trustworthy nearest centimeter ruler , might describe discrete measures. always get precise ruler. measurement divisions make moving continuous concept discrete measurement usually fairly arbitrary. Another way think , enjoy music, , suggested Norman Streiner,17 piano discrete instrument, violin continuous one, enabling finer distinctions notes piano capable making. Sometimes distinction continuous discrete important, usually, ’s .\nnh_adult750 data includes several quantitative variables, specifically Age, Height, BMI,SBP,DBPandPulse`.\nknow quantitative units: Age years, Height centimeters, BMI kg/m2, BP measurements mm Hg, Pulse beats per minute.\nDepending context, likely treat discrete given measurements fairly crude (certainly true Age, measured years) although BMI probably continuous settings, even though function two measures (Height Weight) rounded integer numbers centimeters kilograms, respectively.\nQuantitative variables may also classified whether continuous can take discrete set values. Continuous data may take value, within defined range. Suppose measuring height. height really continuous, measuring stick usually lets us measure certain degree precision. measurements trustworthy nearest centimeter ruler , might describe discrete measures. always get precise ruler. measurement divisions make moving continuous concept discrete measurement usually fairly arbitrary. Another way think , enjoy music, , suggested Norman Streiner,17 piano discrete instrument, violin continuous one, enabling finer distinctions notes piano capable making. Sometimes distinction continuous discrete important, usually, ’s .nh_adult750 data includes several quantitative variables, specifically Age, Height, BMI,SBP,DBPandPulse`.know quantitative units: Age years, Height centimeters, BMI kg/m2, BP measurements mm Hg, Pulse beats per minute.Depending context, likely treat discrete given measurements fairly crude (certainly true Age, measured years) although BMI probably continuous settings, even though function two measures (Height Weight) rounded integer numbers centimeters kilograms, respectively.also possible separate quantitative variables ratio variables interval variables. interval variable equal distances values, zero point arbitrary. ratio variable equal intervals values, meaningful zero point. example, weight example ratio variable, IQ example interval variable. know zero weight . intelligence score like IQ different matter. say average IQ 100, ’s convention. just easily decided add 400 every IQ value make average 500 instead. IQ’s intervals equal, difference IQ 70 IQ 80 difference 120 130. However, IQ 100 twice high IQ 50. point zero point artificial movable, differences numbers meaningful ratios . hand, lab test values ratio variables, physical characteristics like height weight. person weighs 100 kg twice heavy one weighs 50 kg; even convert kg pounds, still true. part, can treat analyze interval ratio variables way.\nquantitative variables nh_adult750 data can thought ratio variables.\nalso possible separate quantitative variables ratio variables interval variables. interval variable equal distances values, zero point arbitrary. ratio variable equal intervals values, meaningful zero point. example, weight example ratio variable, IQ example interval variable. know zero weight . intelligence score like IQ different matter. say average IQ 100, ’s convention. just easily decided add 400 every IQ value make average 500 instead. IQ’s intervals equal, difference IQ 70 IQ 80 difference 120 130. However, IQ 100 twice high IQ 50. point zero point artificial movable, differences numbers meaningful ratios . hand, lab test values ratio variables, physical characteristics like height weight. person weighs 100 kg twice heavy one weighs 50 kg; even convert kg pounds, still true. part, can treat analyze interval ratio variables way.quantitative variables nh_adult750 data can thought ratio variables.Quantitative variables lend many summaries discuss, like means, quantiles, various measures spread, like standard deviation inter-quartile range. also least chance follow Normal distribution.Quantitative variables lend many summaries discuss, like means, quantiles, various measures spread, like standard deviation inter-quartile range. also least chance follow Normal distribution.","code":""},{"path":"data-structures-variable-types-sampling-nhanes.html","id":"a-look-at-bmi-body-mass-index","chapter":"4 Data Structures, Variable Types & Sampling NHANES","heading":"4.3.1 A look at BMI (Body-Mass Index)","text":"definition BMI (body-mass index) adult subjects (expressed units kg/m2) :\\[\n\\mbox{Body Mass Index} = \\frac{\\mbox{weight kg}}{(\\mbox{height meters})^2} = 703 \\times \\frac{\\mbox{weight pounds}}{(\\mbox{height inches})^2}\n\\][BMI essentially] … measure person’s thinness thickness… BMI designed use simple means classifying average sedentary (physically inactive) populations, average body composition. individuals, current value recommendations follow: BMI 18.5 25 may indicate optimal weight, BMI lower 18.5 suggests person underweight, number 25 30 may indicate person overweight, number 30 upwards suggests person obese.Wikipedia, https://en.wikipedia.org/wiki/Body_mass_index","code":""},{"path":"data-structures-variable-types-sampling-nhanes.html","id":"qualitative-categorical-variables","chapter":"4 Data Structures, Variable Types & Sampling NHANES","heading":"4.4 Qualitative (Categorical) Variables","text":"Qualitative categorical variables consist names categories. names may numerical, numbers (names) simply codes identify groups categories individuals divided. Categorical variables two categories, like yes , , , generally, 1 0, called binary variables. two-categories sometimes called multi-categorical variables.categories included variable merely names, come particular order, sometimes call nominal variables. important summary variable usually table frequencies, mode becomes important single summary, mean median essentially useless.\nnh_adult750 data, Race nominal variable multiple unordered categories. MaritalStatus.\ncategories included variable merely names, come particular order, sometimes call nominal variables. important summary variable usually table frequencies, mode becomes important single summary, mean median essentially useless.nh_adult750 data, Race nominal variable multiple unordered categories. MaritalStatus.alternative categorical variable (order matters) called ordinal, includes variables sometimes thought falling right quantitative qualitative variables.\nExamples ordinal multi-categorical variables nh_adult750 data include Education HealthGen variables.\nAnswers questions like “overall physical health?” available responses Excellent, Good, Good, Fair Poor, often coded 1-5, certainly provide perceived order, group people average health status 4 (Good) necessarily twice healthy group average health status 2 (Fair).\nalternative categorical variable (order matters) called ordinal, includes variables sometimes thought falling right quantitative qualitative variables.Examples ordinal multi-categorical variables nh_adult750 data include Education HealthGen variables.Answers questions like “overall physical health?” available responses Excellent, Good, Good, Fair Poor, often coded 1-5, certainly provide perceived order, group people average health status 4 (Good) necessarily twice healthy group average health status 2 (Fair).Sometimes treat values ordinal variables sufficiently scaled permit us use quantitative approaches like means, quantiles, standard deviations summarize model results, times, ’ll treat ordinal variables nominal, tables percentages primary tools.Sometimes treat values ordinal variables sufficiently scaled permit us use quantitative approaches like means, quantiles, standard deviations summarize model results, times, ’ll treat ordinal variables nominal, tables percentages primary tools.Note binary variables may treated ordinal, nominal.\nBinary variables nh_adult750 data include Sex, PhysActive, Smoke100, SleepTrouble. can thought either ordinal nominal.\nNote binary variables may treated ordinal, nominal.Binary variables nh_adult750 data include Sex, PhysActive, Smoke100, SleepTrouble. can thought either ordinal nominal.Lots variables may treated either quantitative qualitative, depending use . instance, usually think age quantitative variable, simply use age make distinction “child” “adult” using describe categorical information. Just variable’s values numbers, don’t assume information provided quantitative.","code":""},{"path":"data-structures-variable-types-sampling-nhanes.html","id":"counting-missing-values","chapter":"4 Data Structures, Variable Types & Sampling NHANES","heading":"4.5 Counting Missing Values","text":"summary() command counts number missing observations variable, sometimes want considerably information.can use functions naniar package learn useful things missing data nh_adult750 sample.miss_var_table command provides table number variables 0, 1, 2, n, missing values percentage total number variables variables make ., instance, 9 variables missing data, constitutes 56.25% 16 variables nh_adult750 data.miss_var_summary() function tabulates number, percent missing, cumulative sum missing variable data frame, order least missing values., example, rmiss_var_summary(nh_temp) %>% slice_head(n = 1) %>% select(variable)variable one missing data anything else within thenh_adult750` data frame.graph information available, well.’ll note also functions count number missing observations case (observation) rather variable. example, can use miss_case_table.Now see 636 observations, 84.8% cases missing data.can use miss_case_summary() identify cases missing data, well.","code":"\nmiss_var_table(nh_adult750)# A tibble: 5 x 3\n  n_miss_in_var n_vars pct_vars\n          <int>  <int>    <dbl>\n1             0      9    56.2 \n2             5      3    18.8 \n3            32      1     6.25\n4            33      2    12.5 \n5            99      1     6.25\nmiss_var_summary(nh_adult750)# A tibble: 16 x 3\n   variable      n_miss pct_miss\n   <chr>          <int>    <dbl>\n 1 HealthGen         99   13.2  \n 2 SBP               33    4.4  \n 3 DBP               33    4.4  \n 4 Pulse             32    4.27 \n 5 Height             5    0.667\n 6 Weight             5    0.667\n 7 BMI                5    0.667\n 8 ID                 0    0    \n 9 Sex                0    0    \n10 Age                0    0    \n11 Race               0    0    \n12 Education          0    0    \n13 PhysActive         0    0    \n14 Smoke100           0    0    \n15 SleepTrouble       0    0    \n16 MaritalStatus      0    0    \ngg_miss_var(nh_adult750)\nmiss_case_table(nh_adult750)# A tibble: 6 x 3\n  n_miss_in_case n_cases pct_cases\n           <int>   <int>     <dbl>\n1              0     636    84.8  \n2              1      78    10.4  \n3              3      15     2    \n4              4      19     2.53 \n5              6       1     0.133\n6              7       1     0.133\nmiss_case_summary(nh_adult750)# A tibble: 750 x 3\n    case n_miss pct_miss\n   <int>  <int>    <dbl>\n 1   342      7     43.8\n 2   606      6     37.5\n 3   157      4     25  \n 4   169      4     25  \n 5   204      4     25  \n 6   234      4     25  \n 7   323      4     25  \n 8   415      4     25  \n 9   478      4     25  \n10   483      4     25  \n# ... with 740 more rows"},{"path":"data-structures-variable-types-sampling-nhanes.html","id":"nh_cc","chapter":"4 Data Structures, Variable Types & Sampling NHANES","heading":"4.6 nh_adults500cc: A Sample of Complete Cases","text":"wanted sample exactly 750 subjects complete data, needed add step development nh_temp sampling frame filter complete cases.Let’s check new sampling frame missing data.OK. Now, let’s create second sample, called nh_adult500cc, now, select 500 adults complete data variables interest, using different random seed. cc stands complete cases.","code":"\nnh_temp2 <- NHANES %>%\n    filter(SurveyYr == \"2011_12\") %>%\n    filter(Age >= 21 & Age < 65) %>%\n    rename(Sex = Gender, Race = Race3, SBP = BPSysAve, DBP = BPDiaAve) %>%\n    select(ID, Sex, Age, Height, Weight, Race, Education, BMI, SBP, DBP, \n           Pulse, PhysActive, Smoke100, SleepTrouble, \n           MaritalStatus, HealthGen) %>%\n    distinct() %>%\n    filter(complete.cases(.))\nmiss_var_table(nh_temp2)# A tibble: 1 x 3\n  n_miss_in_var n_vars pct_vars\n          <int>  <int>    <dbl>\n1             0     16      100\nset.seed(431003) \n# use set.seed to ensure that we all get the same random sample \n\nnh_adult500cc <- slice_sample(nh_temp2, n = 500, replace = F) \n\nnh_adult500cc# A tibble: 500 x 16\n      ID Sex      Age Height Weight Race     Education   BMI\n   <int> <fct>  <int>  <dbl>  <dbl> <fct>    <fct>     <dbl>\n 1 64079 female    25   159.   86.2 Hispanic Some Col~  34.2\n 2 64374 female    52   169    65.5 Asian    College ~  22.9\n 3 71875 male      42   182.   94.1 Black    College ~  28.5\n 4 66396 female    46   161.  107.  Asian    8th Grade  41.2\n 5 64315 female    52   161.   64.5 White    9 - 11th~  24.9\n 6 64015 male      32   168.   82.3 Mexican  Some Col~  29  \n 7 63590 male      21   181.   98.3 Black    Some Col~  29.9\n 8 70893 female    30   171.   65.7 White    9 - 11th~  22.5\n 9 70828 male      26   178.  100.  White    Some Col~  31.5\n10 67930 male      59   172.   91.7 Mexican  College ~  31  \n# ... with 490 more rows, and 8 more variables: SBP <int>,\n#   DBP <int>, Pulse <int>, PhysActive <fct>,\n#   Smoke100 <fct>, SleepTrouble <fct>,\n#   MaritalStatus <fct>, HealthGen <fct>"},{"path":"data-structures-variable-types-sampling-nhanes.html","id":"saving-our-samples-in-.rds-files","chapter":"4 Data Structures, Variable Types & Sampling NHANES","heading":"4.7 Saving our Samples in .Rds files","text":"’ll save nh_adult750 nh_adult500cc samples use later parts notes. , ’ll save .Rds files, advantages us later .also find .Rds files part 431-data repository course.Next, ’ll load, explore learn variables two samples.","code":"\nwrite_rds(nh_adult750, file = \"data/nh_adult750.Rds\")\nwrite_rds(nh_adult500cc, file = \"data/nh_adult500cc.Rds\")"},{"path":"visualizing-nhanes-data.html","id":"visualizing-nhanes-data","chapter":"5 Visualizing NHANES Data","heading":"5 Visualizing NHANES Data","text":"","code":""},{"path":"visualizing-nhanes-data.html","id":"loading-in-the-complete-cases-sample","chapter":"5 Visualizing NHANES Data","heading":"5.1 Loading in the “Complete Cases” Sample","text":"Let’s begin loading nh_500cc data frame information nh_adult500cc.Rds file created Section @ref(nh_cc).One obvious hurdle ’ll avoid moment missing data, since nh_500cc data specifically drawn complete responses. Working complete cases can introduce bias estimates visualizations, necessary time address complete-case analysis isn’t good choice. ’ll return issue chapters.","code":"\nnh_500cc <- read_rds(\"data/nh_adult500cc.Rds\")"},{"path":"visualizing-nhanes-data.html","id":"distribution-of-heights","chapter":"5 Visualizing NHANES Data","heading":"5.2 Distribution of Heights","text":"distribution height new sample?can several things clean .’ll change color lines bar histogram.’ll change fill inside bar make stand bit .’ll add title relabel horizontal (x) axis include units measurement.’ll avoid warning selecting number bins (’ll use 25 ) ’ll group heights drawing histogram.","code":"\nggplot(data = nh_500cc, aes(x = Height)) + \n    geom_histogram() `stat_bin()` using `bins = 30`. Pick better value with\n`binwidth`.\nggplot(data = nh_500cc, aes(x = Height)) + \n    geom_histogram(bins = 25, col = \"yellow\", fill = \"blue\") + \n    labs(title = \"Height of NHANES subjects ages 21-64\",\n         x = \"Height in cm.\")"},{"path":"visualizing-nhanes-data.html","id":"changing-a-histograms-fill-and-color","chapter":"5 Visualizing NHANES Data","heading":"5.2.1 Changing a Histogram’s Fill and Color","text":"CWRU color guide (https://case.edu/umc/-brand/visual-guidelines/) lists HTML color schemes CWRU blue CWRU gray. Let’s match color scheme. also change bins histogram, gather observations groups 2 cm. , specifying width bins, rather number bins.","code":"\ncwru.blue <- '#0a304e'\ncwru.gray <- '#626262'\n\nggplot(data = nh_500cc, aes(x = Height)) + \n    geom_histogram(binwidth = 2, \n                   col = cwru.gray, fill = cwru.blue) + \n    labs(title = \"Height of NHANES subjects ages 21-64\",\n         x = \"Height in cm.\") "},{"path":"visualizing-nhanes-data.html","id":"using-a-frequency-polygon","chapter":"5 Visualizing NHANES Data","heading":"5.2.2 Using a frequency polygon","text":"frequency polygon essentially smooths top histogram, can also used show distribution Height.","code":"\nggplot(data = nh_500cc, aes(x = Height)) +\n    geom_freqpoly(bins = 20) +\n    labs(title = \"Height of NHANES subjects ages 21-64\",\n         x = \"Height in cm.\")"},{"path":"visualizing-nhanes-data.html","id":"using-a-dotplot","chapter":"5 Visualizing NHANES Data","heading":"5.2.3 Using a dotplot","text":"dotplot can also used show distribution variable like Height, produces somewhat granular histogram, depending settings binwidth dotsize.","code":"\nggplot(data = nh_500cc, aes(x = Height)) +\n    geom_dotplot(dotsize = 0.75, binwidth = 1) +\n    scale_y_continuous(NULL, breaks = NULL) + # hide y axis\n    labs(title = \"Height of NHANES subjects ages 21-64\",\n         x = \"Height in cm.\")"},{"path":"visualizing-nhanes-data.html","id":"height-and-sex","chapter":"5 Visualizing NHANES Data","heading":"5.3 Height and Sex","text":"Let’s look impact respondent’s sex height, now within sample adults.plot isn’t useful. can improve things little jittering points horizontally, overlap reduced.Perhaps might better summarise distribution different way. might consider boxplot data.","code":"\nggplot(data = nh_500cc, \n       aes(x = Sex, y = Height, color = Sex)) + \n    geom_point() + \n    labs(title = \"Height by Sex for NHANES subjects ages 21-64\",\n         y = \"Height in cm.\")\nggplot(data = nh_500cc, aes(x = Sex, y = Height, color = Sex)) + \n    geom_jitter(width = 0.2) + \n    labs(title = \"Height by Sex (jittered) for NHANES subjects ages 21-64\",\n         y = \"Height in cm.\")"},{"path":"visualizing-nhanes-data.html","id":"a-boxplot-of-height-by-sex","chapter":"5 Visualizing NHANES Data","heading":"5.3.1 A Boxplot of Height by Sex","text":"boxplot shows summary statistics based percentiles. boxes middle line show data values include middle half data sorted. 25th percentile (value exceeds 1/4 data) indicated bottom box, top box located 75th percentile. solid line inside box indicates median (also called 50th percentile) Heights Sex.","code":"\nggplot(data = nh_500cc, aes(x = Sex, y = Height, fill = Sex)) + \n    geom_boxplot() + \n    labs(title = \"Boxplot of Height by Sex for NHANES subjects ages 21-64\",\n         y = \"Height in cm.\")"},{"path":"visualizing-nhanes-data.html","id":"adding-a-violin-plot","chapter":"5 Visualizing NHANES Data","heading":"5.3.2 Adding a violin plot","text":"boxplot often supplemented violin plot better show shape distribution.usually works better boxes given different fill violins, shown following figure.can also flip boxplots side, using coord_flip().","code":"\nggplot(data = nh_500cc, aes(x = Sex, y = Height, fill = Sex)) +\n    geom_violin() +\n    geom_boxplot(width = 0.3) +\n    labs(title = \"Boxplot of Height by Sex for NHANES subjects ages 21-64\",\n         y = \"Height in cm.\")\nggplot(data = nh_500cc, aes(x = Sex, y = Height)) +\n    geom_violin(aes(fill = Sex)) +\n    geom_boxplot(width = 0.3) +\n    labs(title = \"Boxplot of Height by Sex for NHANES subjects ages 21-64\",\n         y = \"Height in cm.\")\nggplot(data = nh_500cc, aes(x = Sex, y = Height)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = Sex), width = 0.3) +\n    labs(title = \"Boxplot of Height by Sex for NHANES subjects ages 21-64\",\n         y = \"Height in cm.\") +\n    coord_flip()"},{"path":"visualizing-nhanes-data.html","id":"histograms-of-height-by-sex","chapter":"5 Visualizing NHANES Data","heading":"5.3.3 Histograms of Height by Sex","text":"perhaps ’d like see pair histograms?Can redraw histograms little comparable, get rid unnecessary legend?","code":"\nggplot(data = nh_500cc, aes(x = Height, fill = Sex)) + \n    geom_histogram(color = \"white\", bins = 20) + \n    labs(title = \"Histogram of Height by Sex for NHANES subjects ages 21-64\",\n         x = \"Height in cm.\") + \n    facet_wrap(~ Sex)\nggplot(data = nh_500cc, aes(x = Height, fill = Sex)) + \n    geom_histogram(color = \"white\", bins = 20) + \n    labs(title = \"Histogram of Height by Sex for NHANES subjects ages 21-64 (Revised)\",\n         x = \"Height in cm.\") + \n    guides(fill = \"none\") +\n    facet_grid(Sex ~ .)"},{"path":"visualizing-nhanes-data.html","id":"looking-at-pulse-rate","chapter":"5 Visualizing NHANES Data","heading":"5.4 Looking at Pulse Rate","text":"Let’s look different outcome, pulse rate subjects.’s histogram, CWRU colors, pulse rates sample.Suppose instead bin groups 5 beats per minute together plot Pulse rates.useful representation depend lot questions ’re trying answer.","code":"\nggplot(data = nh_500cc, aes(x = Pulse)) + \n    geom_histogram(binwidth = 1, \n                   fill = cwru.blue, col = cwru.gray) + \n    labs(title = \"Histogram of Pulse Rate: NHANES subjects ages 21-64\",\n         x = \"Pulse Rate (beats per minute)\")\nggplot(data = nh_500cc, aes(x = Pulse)) + \n    geom_histogram(binwidth = 5, \n                   fill = cwru.blue, col = cwru.gray) + \n    labs(title = \"Histogram of Pulse Rate: NHANES subjects ages 21-64\",\n         x = \"Pulse Rate (beats per minute)\")"},{"path":"visualizing-nhanes-data.html","id":"pulse-rate-and-physical-activity","chapter":"5 Visualizing NHANES Data","heading":"5.4.1 Pulse Rate and Physical Activity","text":"can also split data groups based whether subjects physically active. Let’s try boxplot.accompanying numerical summary, might ask many people fall PhysActive categories, “average” Pulse rate.knitr::kable(digits = 2) piece command tells R Markdown generate table attractive formatting, rounding decimals two figures.","code":"\nggplot(data = nh_500cc, \n       aes(y = Pulse, x = PhysActive, fill = PhysActive)) + \n    geom_boxplot() + \n    labs(title = \"Pulse Rate by Physical Activity Status for NHANES ages 21-64\")\nnh_500cc %>%\n    group_by(PhysActive) %>%\n    summarise(count = n(), mean(Pulse), median(Pulse)) %>%\n    knitr::kable(digits = 2) "},{"path":"visualizing-nhanes-data.html","id":"pulse-by-sleeping-trouble","chapter":"5 Visualizing NHANES Data","heading":"5.4.2 Pulse by Sleeping Trouble","text":"many people fall SleepTrouble categories, “average” Pulse rate?","code":"\nggplot(data = nh_500cc, aes(x = Pulse, fill = SleepTrouble)) + \n    geom_histogram(color = \"white\", bins = 20) + \n    labs(title = \"Histogram of Pulse Rate by Sleep Trouble for NHANES subjects ages 21-64\",\n         x = \"Pulse Rate (beats per minute)\") + \n    guides(fill = \"none\") +\n    facet_grid(SleepTrouble ~ ., labeller = \"label_both\")\nnh_500cc %>%\n    group_by(SleepTrouble) %>%\n    summarise(count = n(), mean(Pulse), median(Pulse)) %>%\n    knitr::kable(digits = 2) "},{"path":"visualizing-nhanes-data.html","id":"pulse-and-healthgen","chapter":"5 Visualizing NHANES Data","heading":"5.4.3 Pulse and HealthGen","text":"can compare distribution Pulse rate across groups subject’s self-reported overall health (HealthGen), well.many people fall HealthGen categories, “average” Pulse rate?","code":"\nggplot(data = nh_500cc, aes(x = HealthGen, y = Pulse, fill = HealthGen)) + \n    geom_boxplot() +\n    labs(title = \"Pulse by Self-Reported Overall Health for NHANES ages 21-64\",\n         x = \"Self-Reported Overall Health\", y = \"Pulse Rate\") + \n    guides(fill = \"none\") \nnh_500cc %>%\n    group_by(HealthGen) %>%\n    summarise(count = n(), mean(Pulse), median(Pulse)) %>%\n    knitr::kable(digits = 2) "},{"path":"visualizing-nhanes-data.html","id":"pulse-rate-and-systolic-blood-pressure","chapter":"5 Visualizing NHANES Data","heading":"5.4.4 Pulse Rate and Systolic Blood Pressure","text":"","code":"\nggplot(data = nh_500cc, aes(x = SBP, y = Pulse)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x) +\n    labs(title = \"Pulse Rate vs. SBP for NHANES subjects, ages 21-64\")"},{"path":"visualizing-nhanes-data.html","id":"sleep-trouble-vs.-no-sleep-trouble","chapter":"5 Visualizing NHANES Data","heading":"5.4.5 Sleep Trouble vs. No Sleep Trouble?","text":"see whether subjects described SleepTrouble show different SBP-pulse rate patterns subjects haven’t?Let’s try changing shape color points based SleepTrouble.plot might easier interpret faceted SleepTrouble, well.","code":"\nggplot(data = nh_500cc, \n       aes(x = SBP, y = Pulse, \n           color = SleepTrouble, shape = SleepTrouble)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x) +\n    labs(title = \"Pulse Rate vs. SBP for NHANES subjects, ages 21-64\")\nggplot(data = nh_500cc, \n       aes(x = SBP, y = Pulse, \n           color = SleepTrouble, shape = SleepTrouble)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x) +\n    labs(title = \"Pulse Rate vs. SBP for NHANES subjects, ages 21-64\") +\n    facet_wrap(~ SleepTrouble, labeller = \"label_both\")"},{"path":"visualizing-nhanes-data.html","id":"general-health-status","chapter":"5 Visualizing NHANES Data","heading":"5.5 General Health Status","text":"’s Table General Health Status results. , self-reported rating subject’s health five point scale (Excellent, Good, Good, Fair, Poor.)HealthGen data categorical, means summarizing averages isn’t appealing looking percentages, proportions rates. tabyl function comes janitor package R.don’t actually like title percent , ’s really proportion, can adjusted, can add total.working unordered categorical variable, like MaritalStatus, approach can work.","code":"\nnh_500cc %>%\n    tabyl(HealthGen)  HealthGen   n percent\n Excellent  52   0.104\n     Vgood 167   0.334\n      Good 204   0.408\n      Fair  65   0.130\n      Poor  12   0.024\nnh_500cc %>%\n    tabyl(HealthGen) %>%\n    adorn_totals() %>%\n    adorn_pct_formatting() HealthGen   n percent\n Excellent  52   10.4%\n     Vgood 167   33.4%\n      Good 204   40.8%\n      Fair  65   13.0%\n      Poor  12    2.4%\n     Total 500  100.0%\nnh_500cc %>%\n    tabyl(MaritalStatus) %>%\n    adorn_totals() %>%\n    adorn_pct_formatting() MaritalStatus   n percent\n      Divorced  47    9.4%\n   LivePartner  46    9.2%\n       Married 256   51.2%\n  NeverMarried 125   25.0%\n     Separated  17    3.4%\n       Widowed   9    1.8%\n         Total 500  100.0%"},{"path":"visualizing-nhanes-data.html","id":"bar-chart-for-categorical-data","chapter":"5 Visualizing NHANES Data","heading":"5.5.1 Bar Chart for Categorical Data","text":"Usually, bar chart best choice graphing variable made categories.lots things can make plot fancier., can really go crazy…","code":"\nggplot(data = nh_500cc, aes(x = HealthGen)) + \n    geom_bar()\nggplot(data = nh_500cc, aes(x = HealthGen, fill = HealthGen)) + \n    geom_bar() + \n    guides(fill = \"none\") +\n    labs(x = \"Self-Reported Health Status\",\n         y = \"Number of NHANES subjects\",\n         title = \"Self-Reported Health Status in NHANES subjects ages 21-64\")\nnh_500cc %>%\n    count(HealthGen) %>%\n    mutate(pct = round_half_up(prop.table(n) * 100, 1)) %>%\n    ggplot(aes(x = HealthGen, y = pct, fill = HealthGen)) + \n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    geom_text(aes(y = pct + 1,    # nudge above top of bar\n                  label = paste0(pct, '%')),  # prettify\n              position = position_dodge(width = .9), \n              size = 4) +\n    labs(x = \"Self-Reported Health Status\",\n         y = \"Percentage of NHANES subjects\",\n         title = \"Self-Reported Health Status in NHANES subjects ages 21-64\") +\n    theme_bw()"},{"path":"visualizing-nhanes-data.html","id":"two-way-tables","chapter":"5 Visualizing NHANES Data","heading":"5.5.2 Two-Way Tables","text":"can create cross-classifications two categorical variables (example HealthGen Smoke100), adding row column marginal totals, compare subjects Sex, follows…like, can make look little polished knitr::kable function…, can get complete cross-tabulation, including (case) percentages people within two categories Smoke100 fall HealthGen category (percentages within row) like ., wanted column percentages, determine sex higher rate HealthGen status level, can get changing adorn_percentages describe results column level:","code":"\nnh_500cc %>%\n    tabyl(Smoke100, HealthGen) %>%\n    adorn_totals(c(\"row\", \"col\"))  Smoke100 Excellent Vgood Good Fair Poor Total\n       No        44   108  105   29    5   291\n      Yes         8    59   99   36    7   209\n    Total        52   167  204   65   12   500\nnh_500cc %>%\n    tabyl(Smoke100, HealthGen) %>%\n    adorn_totals(c(\"row\", \"col\")) %>%\n    knitr::kable()\nnh_500cc %>%\n    tabyl(Smoke100, HealthGen) %>%\n    adorn_totals(\"row\") %>%\n    adorn_percentages(\"row\") %>%\n    adorn_pct_formatting() %>%\n    adorn_ns() %>%\n    knitr::kable()\nnh_500cc %>%\n    tabyl(Sex, HealthGen) %>%\n    adorn_totals(\"col\") %>%\n    adorn_percentages(\"col\") %>%\n    adorn_pct_formatting() %>%\n    adorn_ns() %>%\n    knitr::kable()"},{"path":"visualizing-nhanes-data.html","id":"sbp-by-general-health-status","chapter":"5 Visualizing NHANES Data","heading":"5.5.3 SBP by General Health Status","text":"Let’s consider now relationship self-reported overall health systolic blood pressure.can see many people self-identify “Poor” health category.","code":"\nggplot(data = nh_500cc, aes(x = HealthGen, y = SBP, \n                            fill = HealthGen)) + \n    geom_boxplot() + \n    labs(title = \"SBP by Health Status, Overall Health for NHANES ages 21-64\",\n         y = \"Systolic Blood Pressure\", \n         x = \"Self-Reported Overall Health\") + \n    guides(fill = \"none\") \nnh_500cc %>%\n    group_by(HealthGen) %>%\n    summarise(count = n(), mean(SBP), median(SBP)) %>%\n    knitr::kable() "},{"path":"visualizing-nhanes-data.html","id":"sbp-by-physical-activity-and-general-health-status","chapter":"5 Visualizing NHANES Data","heading":"5.5.4 SBP by Physical Activity and General Health Status","text":"’ll build panel boxplots try understand relationships Systolic Blood Pressure, General Health Status Physical Activity. Note use coord_flip rotate graph 90 degrees, use labeller within facet_wrap include name (Physical Activity) variable value.","code":"\nggplot(data = nh_500cc, aes(x = HealthGen, y = SBP, fill = HealthGen)) + \n    geom_boxplot() + \n    labs(title = \"SBP by Health Status, Overall Health for NHANES ages 21-64\",\n         y = \"Systolic BP\", x = \"Self-Reported Overall Health\") + \n    guides(fill = \"none\") +\n    facet_wrap(~ PhysActive, labeller = \"label_both\") + \n    coord_flip()"},{"path":"visualizing-nhanes-data.html","id":"sbp-by-sleep-trouble-and-general-health-status","chapter":"5 Visualizing NHANES Data","heading":"5.5.5 SBP by Sleep Trouble and General Health Status","text":"’s plot faceted histograms, might used address similar questions related relationship Overall Health, Systolic Blood Pressure whether someone trouble sleeping.","code":"\nggplot(data = nh_500cc, aes(x = SBP, fill = HealthGen)) + \n    geom_histogram(color = \"white\", bins = 20) + \n    labs(title = \"SBP by Overall Health and Sleep Trouble for NHANES ages 21-64\",\n         x = \"Systolic BP\") + \n    guides(fill = \"none\") +\n    facet_grid(SleepTrouble ~ HealthGen, labeller = \"label_both\")"},{"path":"visualizing-nhanes-data.html","id":"conclusions","chapter":"5 Visualizing NHANES Data","heading":"5.6 Conclusions","text":"just small piece toolbox visualizations ’ll create class. Many additional tools way, main idea won’t change. Using ggplot2 package, can accomplish several critical tasks creating visualization, including:Identifying (labeling) axes titlesIdentifying type geom use, like point, bar histogramChanging fill, color, shape, size facilitate comparisonsBuilding “small multiples” plots facetingGood data visualizations make easy see data, ggplot2’s tools make relatively difficult make really bad graph.","code":""},{"path":"summarizing_quantities.html","id":"summarizing_quantities","chapter":"6 Summarizing Quantities","heading":"6 Summarizing Quantities","text":"numerical summaries might new applied appropriately quantitative variables. measures interest us relate :center distribution,spread distribution, andthe shape distribution.demonstrate key ideas Chapter, consider sample 750 adults ages 21-64 NHANES 2011-12 includes missing values. ’ll load nh_750 data frame information nh_adult750.Rds file created Section 4.2.","code":"\nnh_750 <- read_rds(\"data/nh_adult750.Rds\")"},{"path":"summarizing_quantities.html","id":"the-summary-function-for-quantitative-data","chapter":"6 Summarizing Quantities","heading":"6.1 The summary function for Quantitative data","text":"R provides small sampling numerical summaries summary function, instance.basic summary includes set five quantiles18, plus sample’s mean.Min. = minimum value variable, , example, youngest subject’s Age 21.1st Qu. = first quartile (25th percentile) variable - example, 25% subjects Age 30 younger.Median = median (50th percentile) - half subjects Age 40 younger.Mean = mean, usually one means average - sum Ages divided 750 40.8,3rd Qu. = third quartile (75th percentile) - 25% subjects Age 51 older.Max. = maximum value variable, oldest subject Age 64.summary also specifies number missing values variable. , missing 5 BMI values, example.","code":"\nnh_750 %>%\n  select(Age, BMI, SBP, DBP, Pulse) %>%\n  summary()      Age             BMI             SBP       \n Min.   :21.00   Min.   :16.70   Min.   : 83.0  \n 1st Qu.:30.00   1st Qu.:24.20   1st Qu.:108.0  \n Median :40.00   Median :27.90   Median :118.0  \n Mean   :40.82   Mean   :29.08   Mean   :118.8  \n 3rd Qu.:51.00   3rd Qu.:32.10   3rd Qu.:127.0  \n Max.   :64.00   Max.   :80.60   Max.   :209.0  \n                 NA's   :5       NA's   :33     \n      DBP             Pulse       \n Min.   :  0.00   Min.   : 40.00  \n 1st Qu.: 66.00   1st Qu.: 66.00  \n Median : 73.00   Median : 72.00  \n Mean   : 72.69   Mean   : 73.53  \n 3rd Qu.: 80.00   3rd Qu.: 80.00  \n Max.   :108.00   Max.   :124.00  \n NA's   :33       NA's   :32      "},{"path":"summarizing_quantities.html","id":"measuring-the-center-of-a-distribution","chapter":"6 Summarizing Quantities","heading":"6.2 Measuring the Center of a Distribution","text":"","code":""},{"path":"summarizing_quantities.html","id":"the-mean-and-the-median","chapter":"6 Summarizing Quantities","heading":"6.2.1 The Mean and The Median","text":"mean median commonly used measures center distribution quantitative variable. median generally useful value, relevant even data shape symmetric. might also collect sum observations, count number observations, usually symbolized n.variables without missing values, like Age, pretty straightforward., Mean just Sum (30616), divided number non-missing values Age (750), 40.8213333.Median middle value data sorted order. odd number values, sufficient. even number, case, take mean two middle values. sort list 500 Ages, wanted .data set figures don’t want output 10 observations table like .really want see data, can use View(nh_750) get spreadsheet-style presentation, use sort command…, find median, take mean middle two observations sorted data set. 250th 251st largest Ages.","code":"\nnh_750 %>%\n    summarise(n = n(), Mean = mean(Age), Median = median(Age), Sum = sum(Age))# A tibble: 1 x 4\n      n  Mean Median   Sum\n  <int> <dbl>  <dbl> <int>\n1   750  40.8     40 30616\nnh_750 %>% select(Age) %>% \n    arrange(Age)# A tibble: 750 x 1\n     Age\n   <int>\n 1    21\n 2    21\n 3    21\n 4    21\n 5    21\n 6    21\n 7    21\n 8    21\n 9    21\n10    21\n# ... with 740 more rows\nsort(nh_750$Age)  [1] 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21\n [19] 21 21 21 21 22 22 22 22 22 22 22 22 22 22 22 22 22 22\n [37] 22 22 22 22 22 22 22 22 23 23 23 23 23 23 23 23 23 23\n [55] 23 23 23 23 23 23 23 23 23 23 23 23 24 24 24 24 24 24\n [73] 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 25\n [91] 25 25 25 25 25 25 25 25 25 25 25 25 25 26 26 26 26 26\n[109] 26 26 26 26 26 26 26 26 26 27 27 27 27 27 27 27 27 27\n[127] 27 27 27 27 28 28 28 28 28 28 28 28 28 28 28 28 28 28\n[145] 28 28 28 28 28 28 29 29 29 29 29 29 29 29 29 29 29 29\n[163] 29 29 29 29 29 29 29 29 30 30 30 30 30 30 30 30 30 30\n[181] 30 30 30 30 30 30 30 30 30 30 30 30 30 30 31 31 31 31\n[199] 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 32 32 32\n[217] 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32\n[235] 32 32 32 32 32 32 32 33 33 33 33 33 33 33 33 33 33 33\n[253] 33 33 33 33 33 33 33 33 33 33 33 33 33 34 34 34 34 34\n[271] 34 34 34 34 34 34 34 34 34 35 35 35 35 35 35 35 35 35\n[289] 35 35 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n[307] 36 36 36 36 36 36 36 36 36 36 37 37 37 37 37 37 37 37\n[325] 37 37 37 37 37 37 37 37 37 37 38 38 38 38 38 38 38 38\n[343] 38 38 38 38 38 38 38 38 39 39 39 39 39 39 39 39 39 39\n[361] 39 39 39 39 39 39 39 39 39 40 40 40 40 40 40 40 40 40\n[379] 40 40 40 40 40 40 40 40 40 41 41 41 41 41 41 41 41 41\n[397] 41 41 41 41 42 42 42 42 42 42 42 42 42 42 42 42 42 42\n[415] 42 42 42 42 42 43 43 43 43 43 43 43 43 43 43 43 43 43\n[433] 43 43 43 43 43 44 44 44 44 44 44 44 44 44 44 44 44 44\n[451] 44 44 44 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45\n[469] 45 45 46 46 46 46 46 46 46 46 46 46 46 46 46 46 47 47\n[487] 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 48 48 48\n[505] 48 48 48 48 48 48 48 48 49 49 49 49 49 49 49 49 49 49\n[523] 49 49 49 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n[541] 50 50 50 50 50 50 50 50 50 50 50 51 51 51 51 51 51 51\n[559] 51 51 51 51 51 51 51 51 51 51 51 51 51 52 52 52 52 52\n[577] 52 52 52 52 52 52 52 53 53 53 53 53 53 53 53 53 53 53\n[595] 53 53 53 53 54 54 54 54 54 54 54 54 54 54 54 54 54 54\n[613] 54 54 55 55 55 55 55 55 55 55 55 55 56 56 56 56 56 56\n[631] 56 56 56 56 56 56 56 56 56 56 56 56 56 57 57 57 57 57\n[649] 57 57 57 57 58 58 58 58 58 58 58 58 58 58 58 58 58 58\n[667] 58 58 58 58 59 59 59 59 59 59 59 59 59 59 59 59 59 60\n[685] 60 60 60 60 60 60 60 60 60 60 60 60 61 61 61 61 61 61\n[703] 61 61 61 61 61 61 61 61 62 62 62 62 62 62 62 62 62 62\n[721] 62 62 62 63 63 63 63 63 63 63 63 63 63 63 63 63 64 64\n[739] 64 64 64 64 64 64 64 64 64 64 64 64\nsort(nh_750$Age)[250:251][1] 33 33"},{"path":"summarizing_quantities.html","id":"dealing-with-missingness","chapter":"6 Summarizing Quantities","heading":"6.2.2 Dealing with Missingness","text":"calculating mean, may tempted try something like …fails missing values Pulse data. can address either omitting data missing values run summarise() function, tell mean median summary functions remove missing values19., tell summary functions remove NA values.Chapter 8, discuss various assumptions can make missing data, importance imputation dealing modeling making inferences. now, limit descriptive summaries observed values, called complete case available case analyses.","code":"\nnh_750 %>%\n    summarise(mean(Pulse), median(Pulse))# A tibble: 1 x 2\n  `mean(Pulse)` `median(Pulse)`\n          <dbl>           <int>\n1            NA              NA\nnh_750 %>%\n    filter(complete.cases(Pulse)) %>%\n    summarise(count = n(), mean(Pulse), median(Pulse))# A tibble: 1 x 3\n  count `mean(Pulse)` `median(Pulse)`\n  <int>         <dbl>           <dbl>\n1   718          73.5              72\nnh_750 %>%\n    summarise(mean(Pulse, na.rm=TRUE), median(Pulse, na.rm=TRUE))# A tibble: 1 x 2\n  `mean(Pulse, na.rm = TRUE)` `median(Pulse, na.rm = TRUE)`\n                        <dbl>                         <dbl>\n1                        73.5                            72"},{"path":"summarizing_quantities.html","id":"the-mode-of-a-quantitative-variable","chapter":"6 Summarizing Quantities","heading":"6.2.3 The Mode of a Quantitative Variable","text":"One less common measure center quantitative variable’s distribution frequently observed value, referred mode. measure appropriate discrete variables, quantitative categorical. find mode, usually tabulate data, sort counts numbers observations.mode just common Age observed data.Note use three different “verbs” function - explanation strategy, visit Grolemund Wickham.20 group_by function useful. converts nh_750 data frame new grouped tibble operations performed groups. , means groups data Age counting observations, sorting groups (Ages) frequencies.alternative, modeest package’s mfv function calculates sample mode (frequent value).21","code":"\nnh_750 %>%\n    group_by(Age) %>%\n    summarise(count = n()) %>%\n    arrange(desc(count)) # A tibble: 44 x 2\n     Age count\n   <int> <int>\n 1    32    28\n 2    36    26\n 3    50    26\n 4    30    24\n 5    33    24\n 6    24    23\n 7    21    22\n 8    22    22\n 9    23    22\n10    28    20\n# ... with 34 more rows"},{"path":"summarizing_quantities.html","id":"measuring-the-spread-of-a-distribution","chapter":"6 Summarizing Quantities","heading":"6.3 Measuring the Spread of a Distribution","text":"Statistics variation, spread dispersion important fundamental concept statistics. Measures spread like inter-quartile range range (maximum - minimum) can help us understand compare data sets. values data close center, spread small. many values data scattered far away center, spread large.","code":""},{"path":"summarizing_quantities.html","id":"rangeandiqr","chapter":"6 Summarizing Quantities","heading":"6.3.1 The Range and the Interquartile Range (IQR)","text":"range quantitative variable sometimes interpreted difference maximum minimum, even though R presents actual minimum maximum values ask range…, variable missing values, can use…interesting useful statistic inter-quartile range, IQR, range middle half distribution, calculated subtracting 25th percentile value 75th percentile value.can calculate range IQR nicely summary information quantiles, course:","code":"\nnh_750 %>% \n    select(Age) %>% \n    range()[1] 21 64\nnh_750 %>% \n    select(BMI) %>% \n    range(., na.rm=TRUE)[1] 16.7 80.6\nnh_750 %>%\n    summarise(IQR(Age), quantile(Age, 0.25), quantile(Age, 0.75))# A tibble: 1 x 3\n  `IQR(Age)` `quantile(Age, 0.25)` `quantile(Age, 0.75)`\n       <dbl>                 <dbl>                 <dbl>\n1         21                    30                    51\nnh_750 %>%\n    select(Age, BMI, SBP, DBP, Pulse) %>%\n    summary()      Age             BMI             SBP       \n Min.   :21.00   Min.   :16.70   Min.   : 83.0  \n 1st Qu.:30.00   1st Qu.:24.20   1st Qu.:108.0  \n Median :40.00   Median :27.90   Median :118.0  \n Mean   :40.82   Mean   :29.08   Mean   :118.8  \n 3rd Qu.:51.00   3rd Qu.:32.10   3rd Qu.:127.0  \n Max.   :64.00   Max.   :80.60   Max.   :209.0  \n                 NA's   :5       NA's   :33     \n      DBP             Pulse       \n Min.   :  0.00   Min.   : 40.00  \n 1st Qu.: 66.00   1st Qu.: 66.00  \n Median : 73.00   Median : 72.00  \n Mean   : 72.69   Mean   : 73.53  \n 3rd Qu.: 80.00   3rd Qu.: 80.00  \n Max.   :108.00   Max.   :124.00  \n NA's   :33       NA's   :32      "},{"path":"summarizing_quantities.html","id":"the-variance-and-the-standard-deviation","chapter":"6 Summarizing Quantities","heading":"6.3.2 The Variance and the Standard Deviation","text":"IQR always reasonable summary spread, just median always reasonable summary center distribution. Yet, people inclined summarize batch data using two numbers: mean standard deviation. really sensible thing willing assume data follow Normal distribution: bell-shaped, symmetric distribution without substantial outliers.data (even approximately) follow Normal distribution. Summarizing median quartiles (25th 75th percentiles) much robust, explaining R’s emphasis .","code":""},{"path":"summarizing_quantities.html","id":"obtaining-the-variance-and-standard-deviation-in-r","chapter":"6 Summarizing Quantities","heading":"6.3.3 Obtaining the Variance and Standard Deviation in R","text":"variances quantitative variables nh_750 data. Note need include na.rm = TRUE deal missing values variables.standard deviations variables.","code":"\nnh_750 %>%\n    select(Age, BMI, SBP, DBP, Pulse) %>%\n    summarise_all(var, na.rm = TRUE)# A tibble: 1 x 5\n    Age   BMI   SBP   DBP Pulse\n  <dbl> <dbl> <dbl> <dbl> <dbl>\n1  157.  52.4  229.  128.  136.\nnh_750 %>%\n    select(Age, BMI, SBP, DBP, Pulse) %>%\n    summarise_all(sd, na.rm = TRUE)# A tibble: 1 x 5\n    Age   BMI   SBP   DBP Pulse\n  <dbl> <dbl> <dbl> <dbl> <dbl>\n1  12.5  7.24  15.1  11.3  11.6"},{"path":"summarizing_quantities.html","id":"defining-the-variance-and-standard-deviation","chapter":"6 Summarizing Quantities","heading":"6.3.4 Defining the Variance and Standard Deviation","text":"Bock, Velleman, De Veaux22 lots useful thoughts , lightly edited .thinking spread, might consider far data value mean. difference called deviation. just average deviations, positive negative differences always cancel , leaving average deviation zero, ’s helpful. Instead, square deviation obtain non-negative values, emphasize larger differences. add squared deviations find mean (almost), yields variance.\\[\n\\mbox{Variance} = s^2 = \\frac{\\Sigma (y - \\bar{y})^2}{n-1}\n\\]almost? mean squared deviations divided sum \\(n\\), instead divide \\(n-1\\) produces estimate true (population) variance unbiased23. ’re looking intuitive explanation, Stack Exchange link awaits attention.return original units measurement, take square root \\(s^2\\), instead work \\(s\\), standard deviation, also abbreviated SD.\\[\n\\mbox{Standard Deviation} = s = \\sqrt{\\frac{\\Sigma (y - \\bar{y})^2}{n-1}}\n\\]","code":""},{"path":"summarizing_quantities.html","id":"interpreting-the-sd-when-the-data-are-normally-distributed","chapter":"6 Summarizing Quantities","heading":"6.3.5 Interpreting the SD when the data are Normally distributed","text":"set measurements follow Normal distribution, interval:Mean \\(\\pm\\) Standard Deviation contains approximately 68% measurements;Mean \\(\\pm\\) 2(Standard Deviation) contains approximately 95% measurements;Mean \\(\\pm\\) 3(Standard Deviation) contains approximately (99.7%) measurements.often refer population process mean distribution \\(\\mu\\) standard deviation \\(\\sigma\\), leading Figure .\n(#fig:c6_Emp_Rule)Normal Distribution Empirical Rule\ndata approximately Normal distribution, Empirical Rule less helpful.","code":""},{"path":"summarizing_quantities.html","id":"chebyshevs-inequality-one-interpretation-of-the-standard-deviation","chapter":"6 Summarizing Quantities","heading":"6.3.6 Chebyshev’s Inequality: One Interpretation of the Standard Deviation","text":"Chebyshev’s Inequality tells us distribution, regardless relationship Normal distribution, 1/k2 distribution’s values can lie k standard deviations mean. implies, instance, distribution, least 75% values must lie within two standard deviations mean, least 89% must lie within three standard deviations mean., data sets follow Normal distribution. ’ll return notion soon. first, let’s try draw pictures let us get better understanding distribution data.","code":""},{"path":"summarizing_quantities.html","id":"measuring-the-shape-of-a-distribution","chapter":"6 Summarizing Quantities","heading":"6.4 Measuring the Shape of a Distribution","text":"considering shape distribution, one often interested three key points.number modes distribution, always assess plotting data.skewness, symmetry present, typically assess looking plot distribution data, required , summarize non-parametric measure skewness.kurtosis, heavy-tailedness (outlier-proneness) present, usually comparison Normal distribution. , something nearly inevitably assess graphically, measures.Normal distribution single mode, symmetric , naturally, neither heavy-tailed light-tailed compared Normal distribution (call mesokurtic).","code":""},{"path":"summarizing_quantities.html","id":"multimodal-vs.-unimodal-distributions","chapter":"6 Summarizing Quantities","heading":"6.4.1 Multimodal vs. Unimodal distributions","text":"unimodal distribution, level, straightforward. distribution single mode, “peak” distribution. distribution may skewed symmetric, light-tailed heavy-tailed. usually describe multimodal distributions like two right , multiple local maxima, even though just single global maximum peak.\n(#fig:c6_modality-fig)Unimodal Multimodal Sketches\nTruly multimodal distributions usually described way terms shape. unimodal distributions, skewness kurtosis become useful ideas.","code":""},{"path":"summarizing_quantities.html","id":"skew","chapter":"6 Summarizing Quantities","heading":"6.4.2 Skew","text":"Whether distribution approximately symmetric important consideration describing shape. Graphical assessments always useful setting, particularly unimodal data. favorite measure skew, skewness data single mode, :\\[\nskew_1 = \\frac{\\mbox{mean} - \\mbox{median}}{\\mbox{standard deviation}}\n\\]Symmetric distributions generally show values \\(skew_1\\) near zero. distribution actually symmetric, mean equal median.Distributions \\(skew_1\\) values 0.2 absolute value generally indicate meaningful skew.Positive skew (mean > median data unimodal) also referred right skew.Negative skew (mean < median data unimodal) referred left skew.\n(#fig:c6_negandposskew-fig)Negative (Left) Skew Positive (Right) Skew\n","code":""},{"path":"summarizing_quantities.html","id":"kurtosis","chapter":"6 Summarizing Quantities","heading":"6.4.3 Kurtosis","text":"unimodal distribution symmetric, often interested behavior tails distribution, compared Normal distribution mean standard deviation. High values kurtosis measures (several) indicate data extreme outliers, heavy-tailed.mesokurtic distribution similar tail behavior expect Normal distribution.leptokurtic distribution thinner, slender distribution, heavier tails ’d expect Normal distribution. One example t distribution.platykurtic distribution broader, flatter distribution, thinner tails ’d expect Normal distribution. One example uniform distribution.Graphical tools cases best way identify issues related kurtosis.","code":"\nset.seed(431)\nsims_kurt <- tibble(meso = rnorm(n = 300, mean = 0, sd = 1),\n                    lepto = rt(n = 300, df = 4),\n                    platy = runif(n = 300, min = -2, max = 2))\n\np1 <- ggplot(sims_kurt, aes(x = meso)) +\n  geom_histogram(aes(y = stat(density)), \n                 bins = 25, fill = \"royalblue\", col = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(sims_kurt$meso), \n                            sd = sd(sims_kurt$meso)),\n                col = \"red\") +\n  labs(title = \"Normal (mesokurtic)\")\n\np1a <- ggplot(sims_kurt, aes(x = meso, y = \"\")) +\n  geom_violin() +\n  geom_boxplot(fill = \"royalblue\", outlier.color = \"royalblue\", width = 0.3) +\n  labs(y = \"\", x = \"Normal (mesokurtic)\")\n\np2 <- ggplot(sims_kurt, aes(x = lepto)) +\n  geom_histogram(aes(y = stat(density)), \n                 bins = 25, fill = \"tomato\", col = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(sims_kurt$lepto), \n                            sd = sd(sims_kurt$lepto)),\n                col = \"royalblue\") +\n  labs(title = \"t (leptokurtic)\")\n\np2a <- ggplot(sims_kurt, aes(x = lepto, y = \"\")) +\n  geom_violin() +\n  geom_boxplot(fill = \"tomato\", outlier.color = \"tomato\", width = 0.3) +\n  labs(y = \"\", x = \"t (slender with heavy tails)\")\n\np3 <- ggplot(sims_kurt, aes(x = platy)) +\n  geom_histogram(aes(y = stat(density)), \n                 bins = 25, fill = \"yellow\", col = \"black\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(sims_kurt$platy), \n                            sd = sd(sims_kurt$platy)),\n                col = \"royalblue\", lwd = 1.5) +\n  xlim(-3, 3) +\n  labs(title = \"Uniform (platykurtic)\")\n\np3a <- ggplot(sims_kurt, aes(x = platy, y = \"\")) +\n  geom_violin() +\n  geom_boxplot(fill = \"yellow\", width = 0.3) + \n  xlim(-3, 3) +\n  labs(y = \"\", x = \"Uniform (broad with thin tails)\")\n\n\n(p1 + p2 + p3) / (p1a + p2a + p3a) + \n  plot_layout(heights = c(3, 1))"},{"path":"summarizing_quantities.html","id":"numerical-summaries-for-quantitative-variables","chapter":"6 Summarizing Quantities","heading":"6.5 Numerical Summaries for Quantitative Variables","text":"","code":""},{"path":"summarizing_quantities.html","id":"favstats-in-the-mosaic-package","chapter":"6 Summarizing Quantities","heading":"6.5.1 favstats in the mosaic package","text":"favstats function adds standard deviation, counts overall missing observations usual summary continuous variable. Let’s look systolic blood pressure, haven’t yet., course, duplicate results several summarise() pieces…somewhat unusual structure favstats (complete easy forget ~) actually helpful. allows look interesting grouping approaches, like :course, accomplish comparison dplyr commands, , favstats approach much offer.","code":"\nmosaic::favstats(~ SBP, data = nh_750) min  Q1 median  Q3 max     mean       sd   n missing\n  83 108    118 127 209 118.7908 15.14329 717      33\nnh_750 %>%\n    filter(complete.cases(SBP)) %>%\n    summarise(min = min(SBP), Q1 = quantile(SBP, 0.25), \n              median = median(SBP), Q3 = quantile(SBP, 0.75), \n              max = max(SBP),  mean = mean(SBP), \n              sd = sd(SBP), n = n(), miss = sum(is.na(SBP)))# A tibble: 1 x 9\n    min    Q1 median    Q3   max  mean    sd     n  miss\n  <int> <dbl>  <int> <dbl> <int> <dbl> <dbl> <int> <int>\n1    83   108    118   127   209  119.  15.1   717     0\nmosaic::favstats(SBP ~ Education, data = nh_750)       Education min     Q1 median     Q3 max     mean\n1      8th Grade  96 110.25  119.5 129.75 167 122.4565\n2 9 - 11th Grade  85 107.75  116.0 127.00 191 118.8026\n3    High School  84 111.50  120.5 129.00 209 121.0882\n4   Some College  85 108.00  117.0 126.00 186 118.6293\n5   College Grad  83 107.00  117.0 125.00 171 116.8326\n        sd   n missing\n1 16.34993  46       4\n2 15.79453  76       0\n3 16.52853 136       7\n4 14.32736 232       9\n5 14.41202 227      13\nnh_750 %>%\n    filter(complete.cases(SBP, Education)) %>%\n    group_by(Education) %>%\n    summarise(min = min(SBP), Q1 = quantile(SBP, 0.25), \n              median = median(SBP), Q3 = quantile(SBP, 0.75), \n              max = max(SBP), mean = mean(SBP), \n              sd = sd(SBP), n = n(), miss = sum(is.na(SBP)))# A tibble: 5 x 10\n  Education   min    Q1 median    Q3   max  mean    sd     n\n  <fct>     <int> <dbl>  <dbl> <dbl> <int> <dbl> <dbl> <int>\n1 8th Grade    96  110.   120.  130.   167  122.  16.3    46\n2 9 - 11th~    85  108.   116   127    191  119.  15.8    76\n3 High Sch~    84  112.   120.  129    209  121.  16.5   136\n4 Some Col~    85  108    117   126    186  119.  14.3   232\n5 College ~    83  107    117   125    171  117.  14.4   227\n# ... with 1 more variable: miss <int>"},{"path":"summarizing_quantities.html","id":"describe-in-the-psych-package","chapter":"6 Summarizing Quantities","heading":"6.5.2 describe in the psych package","text":"psych package detailed list numerical summaries quantitative variables lets us look group observations .additional statistics presented :trimmed = trimmed mean (default function, removes top bottom 10% data, computes mean remaining values - middle 80% full data set.)mad = median absolute deviation (median), can used manner similar standard deviation IQR measure spread.\ndata \\(Y_1, Y_2, ..., Y_n\\), mad defined \\(median(|Y_i - median(Y_i)|)\\).\nfind mad set numbers, find median, subtract median value find absolute value difference, find median absolute differences.\nnon-normal data skewed shape tails well approximated Normal, mad likely better (robust) estimate spread standard deviation.\ndata \\(Y_1, Y_2, ..., Y_n\\), mad defined \\(median(|Y_i - median(Y_i)|)\\).find mad set numbers, find median, subtract median value find absolute value difference, find median absolute differences.non-normal data skewed shape tails well approximated Normal, mad likely better (robust) estimate spread standard deviation.measure skew, refers much asymmetry present shape distribution. measure nonparametric skew measure usually prefer. [Wikipedia page skewness][https://en.wikipedia.org/wiki/Skewness] detailed.measure excess kurtosis, refers outlier-prone, heavy-tailed shape distribution , compared Normal distribution.se = standard error sample mean, equal sample sd divided square root sample size.","code":"\npsych::describe(nh_750 %>% select(Age, BMI, SBP, DBP, Pulse))      vars   n   mean    sd median trimmed   mad  min   max\nAge      1 750  40.82 12.54   40.0   40.53 14.83 21.0  64.0\nBMI      2 745  29.08  7.24   27.9   28.31  5.93 16.7  80.6\nSBP      3 717 118.79 15.14  118.0  117.88 13.34 83.0 209.0\nDBP      4 717  72.69 11.34   73.0   72.65 10.38  0.0 108.0\nPulse    5 718  73.53 11.65   72.0   73.11 11.86 40.0 124.0\n      range  skew kurtosis   se\nAge    43.0  0.16    -1.15 0.46\nBMI    63.9  1.72     6.16 0.27\nSBP   126.0  0.96     3.10 0.57\nDBP   108.0 -0.28     2.59 0.42\nPulse  84.0  0.48     0.73 0.43"},{"path":"summarizing_quantities.html","id":"the-hmisc-packages-version-of-describe","chapter":"6 Summarizing Quantities","heading":"6.5.3 The Hmisc package’s version of describe","text":"Hmisc package’s version describe distribution data presents three new ideas, addition comprehensive list quartiles (5th, 10th, 25th, 50th, 75th, 90th 95th shown) lowest highest observations. :distinct - number different values observed data.Info - measure “continuous” variable , related many “ties” data, Info taking higher value (closer maximum one) data continuous.Gmd - Gini mean difference - robust measure spread calculated mean absolute difference pairs observations. Larger values Gmd indicate spread-distributions. (Gini pronounced either “Genie” “Ginny.”)","code":"\nHmisc::describe(nh_750 %>% \n                  select(Age, BMI, SBP, DBP, Pulse))nh_750 %>% select(Age, BMI, SBP, DBP, Pulse) \n\n 5  Variables      750  Observations\n------------------------------------------------------------\nAge \n       n  missing distinct     Info     Mean      Gmd \n     750        0       44    0.999    40.82    14.46 \n     .05      .10      .25      .50      .75      .90 \n      22       24       30       40       51       59 \n     .95 \n      62 \n\nlowest : 21 22 23 24 25, highest: 60 61 62 63 64\n------------------------------------------------------------\nBMI \n       n  missing distinct     Info     Mean      Gmd \n     745        5      250        1    29.08    7.538 \n     .05      .10      .25      .50      .75      .90 \n   20.22    21.30    24.20    27.90    32.10    37.60 \n     .95 \n   41.28 \n\nlowest : 16.7 17.6 17.8 17.9 18.0, highest: 59.1 62.8 63.3 69.0 80.6\n------------------------------------------------------------\nSBP \n       n  missing distinct     Info     Mean      Gmd \n     717       33       81    0.999    118.8    16.36 \n     .05      .10      .25      .50      .75      .90 \n    98.0    102.0    108.0    118.0    127.0    137.0 \n     .95 \n   144.2 \n\nlowest :  83  84  85  86  89, highest: 171 179 186 191 209\n------------------------------------------------------------\nDBP \n       n  missing distinct     Info     Mean      Gmd \n     717       33       66    0.999    72.69    12.43 \n     .05      .10      .25      .50      .75      .90 \n      55       59       66       73       80       86 \n     .95 \n      91 \n\nlowest :   0  25  41  42  44, highest: 104 105 106 107 108\n------------------------------------------------------------\nPulse \n       n  missing distinct     Info     Mean      Gmd \n     718       32       37    0.997    73.53    12.95 \n     .05      .10      .25      .50      .75      .90 \n      56       60       66       72       80       88 \n     .95 \n      94 \n\nlowest :  40  44  46  48  50, highest: 108 112 114 118 124\n------------------------------------------------------------"},{"path":"summarizing_quantities.html","id":"other-options","chapter":"6 Summarizing Quantities","heading":"6.5.4 Other options","text":"package summarytools function called dfSummary like Dominic Comtois also published Recommendations Using summarytools R Markdown. Note isn’t really Word documents.DataExplorer can used automated exploratory data analyses (people also like skimr) visdat, well.df_stats function available mosaic package loaded allows run favstats multiple outcome variables simultaneously.","code":""},{"path":"summarizing-categories.html","id":"summarizing-categories","chapter":"7 Summarizing Categories","heading":"7 Summarizing Categories","text":"demonstrate key ideas Chapter, consider sample 750 adults ages 21-64 NHANES 2011-12 includes missing values. ’ll load nh_750 data frame information nh_adult750.Rds file created Section 4.2.Summarizing categorical variables numerically mostly building tables, calculating percentages proportions. ’ll save discussion modeling categorical data later. Recall nh_750 data set built Section 4.2 following categorical variables. number levels indicates number possible categories categorical variable.","code":"\nnh_750 <- read_rds(\"data/nh_adult750.Rds\")"},{"path":"summarizing-categories.html","id":"the-summary-function-for-categorical-data","chapter":"7 Summarizing Categories","heading":"7.1 The summary function for Categorical data","text":"R recognizes variable categorical, stores factor. variables get special treatment summary function, particular table available values (long aren’t many.)","code":"\nnh_750 %>%\n  select(Sex, Race, Education, PhysActive, Smoke100, \n         SleepTrouble, HealthGen, MaritalStatus) %>%\n  summary()     Sex            Race              Education  \n female:388   Asian   : 70   8th Grade     : 50  \n male  :362   Black   :128   9 - 11th Grade: 76  \n              Hispanic: 63   High School   :143  \n              Mexican : 80   Some College  :241  \n              White   :393   College Grad  :240  \n              Other   : 16                       \n PhysActive Smoke100  SleepTrouble     HealthGen  \n No :326    No :453   No :555      Excellent: 84  \n Yes:424    Yes:297   Yes:195      Vgood    :197  \n                                   Good     :252  \n                                   Fair     :104  \n                                   Poor     : 14  \n                                   NA's     : 99  \n      MaritalStatus\n Divorced    : 78  \n LivePartner : 70  \n Married     :388  \n NeverMarried:179  \n Separated   : 19  \n Widowed     : 16  "},{"path":"summarizing-categories.html","id":"tables-to-describe-one-categorical-variable","chapter":"7 Summarizing Categories","heading":"7.2 Tables to describe One Categorical Variable","text":"Suppose build table (using tabyl function janitor package) describe HealthGen distribution.Note missing (<NA>) values included valid_percent calculation, percent calculation. Note also use percentage formatting.want add total count, sometimes called marginal total?marital status, missing data sample?","code":"\nnh_750 %>%\n    tabyl(HealthGen) %>%\n    adorn_pct_formatting() HealthGen   n percent valid_percent\n Excellent  84   11.2%         12.9%\n     Vgood 197   26.3%         30.3%\n      Good 252   33.6%         38.7%\n      Fair 104   13.9%         16.0%\n      Poor  14    1.9%          2.2%\n      <NA>  99   13.2%             -\nnh_750 %>%\n    tabyl(HealthGen) %>%\n    adorn_totals() %>%\n    adorn_pct_formatting() HealthGen   n percent valid_percent\n Excellent  84   11.2%         12.9%\n     Vgood 197   26.3%         30.3%\n      Good 252   33.6%         38.7%\n      Fair 104   13.9%         16.0%\n      Poor  14    1.9%          2.2%\n      <NA>  99   13.2%             -\n     Total 750  100.0%        100.0%\nnh_750 %>%\n    tabyl(MaritalStatus) %>%\n    adorn_totals() %>%\n    adorn_pct_formatting() MaritalStatus   n percent\n      Divorced  78   10.4%\n   LivePartner  70    9.3%\n       Married 388   51.7%\n  NeverMarried 179   23.9%\n     Separated  19    2.5%\n       Widowed  16    2.1%\n         Total 750  100.0%"},{"path":"summarizing-categories.html","id":"constructing-tables-well","chapter":"7 Summarizing Categories","heading":"7.3 Constructing Tables Well","text":"prolific Howard Wainer responsible many interesting books visualization related issues, including Howard Wainer24 Howard Wainer.25 rules come Chapter 10 Howard Wainer.26Order rows columns way makes sense.Round, lot!different important","code":""},{"path":"summarizing-categories.html","id":"alabama-first","chapter":"7 Summarizing Categories","heading":"7.3.1 Alabama First!","text":"Tables useful ?2013 Percent Students grades 9-12 obeseor …rare event Alabama first best choice.","code":""},{"path":"summarizing-categories.html","id":"all-is-different-and-important","chapter":"7 Summarizing Categories","heading":"7.3.2 ALL is different and important","text":"Summaries rows columns provide measure typical usual. Sometimes sum helpful, times, consider presenting median summary. category, Wainer27 suggests, visually different individual entries set spatially apart.whole, ’s far easier fall good graph R (least ggplot2 skills) produce good table.","code":""},{"path":"summarizing-categories.html","id":"the-mode-of-a-categorical-variable","chapter":"7 Summarizing Categories","heading":"7.4 The Mode of a Categorical Variable","text":"common measure applied categorical variable identify mode, frequently observed value. find mode variables lots categories (summary may sufficient), usually tabulate data, sort counts numbers observations, discrete quantitative variables.","code":"\nnh_750 %>%\n    group_by(HealthGen) %>%\n    summarise(count = n()) %>%\n    arrange(desc(count)) # A tibble: 6 x 2\n  HealthGen count\n  <fct>     <int>\n1 Good        252\n2 Vgood       197\n3 Fair        104\n4 <NA>         99\n5 Excellent    84\n6 Poor         14"},{"path":"summarizing-categories.html","id":"describe-in-the-hmisc-package","chapter":"7 Summarizing Categories","heading":"7.5 describe in the Hmisc package","text":"","code":"\nHmisc::describe(nh_750 %>% \n                    select(Sex, Race, Education, PhysActive, \n                           Smoke100, SleepTrouble, \n                           HealthGen, MaritalStatus))nh_750 %>% select(Sex, Race, Education, PhysActive, Smoke100, SleepTrouble, HealthGen, MaritalStatus) \n\n 8  Variables      750  Observations\n------------------------------------------------------------\nSex \n       n  missing distinct \n     750        0        2 \n                        \nValue      female   male\nFrequency     388    362\nProportion  0.517  0.483\n------------------------------------------------------------\nRace \n       n  missing distinct \n     750        0        6 \n\nlowest : Asian    Black    Hispanic Mexican  White   \nhighest: Black    Hispanic Mexican  White    Other   \n                                                       \nValue         Asian    Black Hispanic  Mexican    White\nFrequency        70      128       63       80      393\nProportion    0.093    0.171    0.084    0.107    0.524\n                   \nValue         Other\nFrequency        16\nProportion    0.021\n------------------------------------------------------------\nEducation \n       n  missing distinct \n     750        0        5 \n\nlowest : 8th Grade      9 - 11th Grade High School    Some College   College Grad  \nhighest: 8th Grade      9 - 11th Grade High School    Some College   College Grad  \n                                                       \nValue           8th Grade 9 - 11th Grade    High School\nFrequency              50             76            143\nProportion          0.067          0.101          0.191\n                                        \nValue        Some College   College Grad\nFrequency             241            240\nProportion          0.321          0.320\n------------------------------------------------------------\nPhysActive \n       n  missing distinct \n     750        0        2 \n                      \nValue         No   Yes\nFrequency    326   424\nProportion 0.435 0.565\n------------------------------------------------------------\nSmoke100 \n       n  missing distinct \n     750        0        2 \n                      \nValue         No   Yes\nFrequency    453   297\nProportion 0.604 0.396\n------------------------------------------------------------\nSleepTrouble \n       n  missing distinct \n     750        0        2 \n                    \nValue        No  Yes\nFrequency   555  195\nProportion 0.74 0.26\n------------------------------------------------------------\nHealthGen \n       n  missing distinct \n     651       99        5 \n\nlowest : Excellent Vgood     Good      Fair      Poor     \nhighest: Excellent Vgood     Good      Fair      Poor     \n                                                  \nValue      Excellent     Vgood      Good      Fair\nFrequency         84       197       252       104\nProportion     0.129     0.303     0.387     0.160\n                    \nValue           Poor\nFrequency         14\nProportion     0.022\n------------------------------------------------------------\nMaritalStatus \n       n  missing distinct \n     750        0        6 \n\nlowest : Divorced     LivePartner  Married      NeverMarried Separated   \nhighest: LivePartner  Married      NeverMarried Separated    Widowed     \n                                                 \nValue          Divorced  LivePartner      Married\nFrequency            78           70          388\nProportion        0.104        0.093        0.517\n                                                 \nValue      NeverMarried    Separated      Widowed\nFrequency           179           19           16\nProportion        0.239        0.025        0.021\n------------------------------------------------------------"},{"path":"summarizing-categories.html","id":"cross-tabulations-of-two-variables","chapter":"7 Summarizing Categories","heading":"7.6 Cross-Tabulations of Two Variables","text":"common us want describe association one categorical variable another. instance, relationship Education SleepTrouble data?Note use adorn_totals get marginal counts, specify want row column totals. can add title columns …Often, ’ll want show percentages cross-tabulation like . get row percentages can directly see probability SleepTrouble = Yes level Education, can use:want compare distribution Education two levels SleepTrouble column percentages, can use following…want overall percentages cells table, total across combinations Education SleepTrouble 100%, can use:Another common approach include counts percentages cross-tabulation. Let’s look breakdown HealthGen MaritalStatus.wanted ignore missing HealthGen values? often, filter complete observations.working tabyls, see vignette janitor package. ’ll find complete list adorn functions, example.’s another approach, look cross-classification Race HealthGen:","code":"\nnh_750 %>%\n    tabyl(Education, SleepTrouble) %>%\n    adorn_totals(where = c(\"row\", \"col\"))       Education  No Yes Total\n      8th Grade  40  10    50\n 9 - 11th Grade  52  24    76\n    High School 102  41   143\n   Some College 173  68   241\n   College Grad 188  52   240\n          Total 555 195   750\nnh_750 %>%\n    tabyl(Education, SleepTrouble) %>%\n    adorn_totals(where = c(\"row\", \"col\")) %>%\n    adorn_title(placement = \"combined\") Education/SleepTrouble  No Yes Total\n              8th Grade  40  10    50\n         9 - 11th Grade  52  24    76\n            High School 102  41   143\n           Some College 173  68   241\n           College Grad 188  52   240\n                  Total 555 195   750\nnh_750 %>%\n    tabyl(Education, SleepTrouble) %>%\n    adorn_totals(where = \"row\") %>%\n    adorn_percentages(denominator = \"row\") %>%\n    adorn_pct_formatting() %>%\n    adorn_title(placement = \"combined\") Education/SleepTrouble    No   Yes\n              8th Grade 80.0% 20.0%\n         9 - 11th Grade 68.4% 31.6%\n            High School 71.3% 28.7%\n           Some College 71.8% 28.2%\n           College Grad 78.3% 21.7%\n                  Total 74.0% 26.0%\nnh_750 %>%\n    tabyl(Education, SleepTrouble) %>%\n    adorn_totals(where = \"col\") %>%\n    adorn_percentages(denominator = \"col\") %>%\n    adorn_pct_formatting() %>%\n    adorn_title(placement = \"combined\")  Education/SleepTrouble    No   Yes Total\n              8th Grade  7.2%  5.1%  6.7%\n         9 - 11th Grade  9.4% 12.3% 10.1%\n            High School 18.4% 21.0% 19.1%\n           Some College 31.2% 34.9% 32.1%\n           College Grad 33.9% 26.7% 32.0%\nnh_750 %>%\n    tabyl(Education, SleepTrouble) %>%\n    adorn_totals(where = c(\"row\", \"col\")) %>%\n    adorn_percentages(denominator = \"all\") %>%\n    adorn_pct_formatting() %>%\n    adorn_title(placement = \"combined\")  Education/SleepTrouble    No   Yes  Total\n              8th Grade  5.3%  1.3%   6.7%\n         9 - 11th Grade  6.9%  3.2%  10.1%\n            High School 13.6%  5.5%  19.1%\n           Some College 23.1%  9.1%  32.1%\n           College Grad 25.1%  6.9%  32.0%\n                  Total 74.0% 26.0% 100.0%\nnh_750 %>%\n    tabyl(MaritalStatus, HealthGen) %>%\n    adorn_totals(where = c(\"row\")) %>%\n    adorn_percentages(denominator = \"row\") %>%\n    adorn_pct_formatting() %>%\n    adorn_ns(position = \"front\") %>%\n    adorn_title(placement = \"combined\") %>%\n    knitr::kable()\nnh_750 %>%\n    filter(complete.cases(MaritalStatus, HealthGen)) %>%\n    tabyl(MaritalStatus, HealthGen) %>%\n    adorn_totals(where = c(\"row\")) %>%\n    adorn_percentages(denominator = \"row\") %>%\n    adorn_pct_formatting() %>%\n    adorn_ns(position = \"front\") %>%\n    adorn_title(placement = \"combined\") MaritalStatus/HealthGen  Excellent       Vgood        Good\n                Divorced  7 (10.1%)  19 (27.5%)  29 (42.0%)\n             LivePartner  4  (6.1%)  19 (28.8%)  25 (37.9%)\n                 Married 46 (14.2%) 101 (31.2%) 130 (40.1%)\n            NeverMarried 25 (15.6%)  52 (32.5%)  56 (35.0%)\n               Separated  2 (11.8%)   3 (17.6%)   4 (23.5%)\n                 Widowed  0  (0.0%)   3 (20.0%)   8 (53.3%)\n                   Total 84 (12.9%) 197 (30.3%) 252 (38.7%)\n        Fair       Poor\n  11 (15.9%)  3  (4.3%)\n  18 (27.3%)  0  (0.0%)\n  41 (12.7%)  6  (1.9%)\n  24 (15.0%)  3  (1.9%)\n   8 (47.1%)  0  (0.0%)\n   2 (13.3%)  2 (13.3%)\n 104 (16.0%) 14  (2.2%)\nxtabs(~ Race + HealthGen, data = nh_750)          HealthGen\nRace       Excellent Vgood Good Fair Poor\n  Asian           10    17   24    6    1\n  Black           15    28   40   24    4\n  Hispanic         4     9   24   13    2\n  Mexican          6    12   25   21    2\n  White           48   128  131   37    5\n  Other            1     3    8    3    0"},{"path":"summarizing-categories.html","id":"cross-classifying-three-categorical-variables","chapter":"7 Summarizing Categories","heading":"7.7 Cross-Classifying Three Categorical Variables","text":"Suppose interested Smoke100 relationship PhysActive SleepTrouble.result tabyl Smoke100 (rows) PhysActive (columns), split list SleepTrouble.several alternative approaches , although expect us stick tabyl work 431. alternatives include use xtabs function:can also build flat version table, follows:can dplyr functions table() function, well, example…","code":"\nnh_750 %>%\n    tabyl(Smoke100, PhysActive, SleepTrouble) %>%\n    adorn_title(placement = \"top\")$No\n          PhysActive    \n Smoke100         No Yes\n       No        137 219\n      Yes         93 106\n\n$Yes\n          PhysActive    \n Smoke100         No Yes\n       No         41  56\n      Yes         55  43\nxtabs(~ Smoke100 + PhysActive + SleepTrouble, data = nh_750), , SleepTrouble = No\n\n        PhysActive\nSmoke100  No Yes\n     No  137 219\n     Yes  93 106\n\n, , SleepTrouble = Yes\n\n        PhysActive\nSmoke100  No Yes\n     No   41  56\n     Yes  55  43\nftable(Smoke100 ~ PhysActive + SleepTrouble, data = nh_750)                        Smoke100  No Yes\nPhysActive SleepTrouble                 \nNo         No                    137  93\n           Yes                    41  55\nYes        No                    219 106\n           Yes                    56  43\nnh_750 %>%\n    select(Smoke100, PhysActive, SleepTrouble) %>%\n    table() , , SleepTrouble = No\n\n        PhysActive\nSmoke100  No Yes\n     No  137 219\n     Yes  93 106\n\n, , SleepTrouble = Yes\n\n        PhysActive\nSmoke100  No Yes\n     No   41  56\n     Yes  55  43"},{"path":"summarizing-categories.html","id":"gaining-control-over-tables-in-r-the-gt-package","chapter":"7 Summarizing Categories","heading":"7.8 Gaining Control over Tables in R: the gt package","text":"gt package, anyone can make wonderful-looking tables using R programming language. gt package allows start tibble data frame, use make detailed tables look professional, includes tools enable include titles subtitles, sorts labels, well footnotes source notes.’s fairly simple example cross-tabulation part nh_750 data built using tools gt package.gt package usage described detail https://gt.rstudio.com/.","code":"\nlibrary(gt)\n\ntemp_tbl <- nh_750 %>% filter(complete.cases(PhysActive, HealthGen)) %>%\n  tabyl(PhysActive, HealthGen) %>%\n  tibble() \n\ngt(temp_tbl) %>%\n  tab_header(title = md(\"**Cross-Tabulation from nh_750**\"),\n             subtitle = md(\"Physical Activity vs. Overall Health\"))"},{"path":"miss.html","id":"miss","chapter":"8 Missing Data and Single Imputation","heading":"8 Missing Data and Single Imputation","text":"Almost serious statistical analyses deal missing data. Data values missing indicated R, R, symbol NA.’ll focus tools naniar simputation packages work.","code":""},{"path":"miss.html","id":"a-simulated-example-with-15-subjects","chapter":"8 Missing Data and Single Imputation","heading":"8.1 A Simulated Example with 15 subjects","text":"following tiny data set called sbp_example, four variables set 15 subjects. addition subject id, :treatment subject received (, B C treatments),indicator (1 = yes, 0 = ) whether subject diabetes,subject’s systolic blood pressure baselinethe subject’s systolic blood pressure application treatment","code":"\n# create some temporary variables\nsubject <- 101:115\nx1 <- c(\"A\", \"B\", \"C\", \"A\", \"C\", \"A\", \"A\", NA, \"B\", \"C\", \"A\", \"B\", \"C\", \"A\", \"B\")\nx2 <- c(1, 0, 0, 1, NA, 1, 0, 1, NA, 1, 0, 0, 1, 1, NA)\nx3 <- c(120, 145, 150, NA, 155, NA, 135, NA, 115, 170, 150, 145, 140, 160, 135)\nx4 <- c(105, 135, 150, 120, 135, 115, 160, 150, 130, 155, 140, 140, 150, 135, 120)\n\nsbp_example <- \n  tibble(subject, treat = factor(x1), diabetes = x2, \n         sbp.before = x3, sbp.after = x4) \n\nrm(subject, x1, x2, x3, x4) # just cleaning up\n\nsbp_example# A tibble: 15 x 5\n   subject treat diabetes sbp.before sbp.after\n     <int> <fct>    <dbl>      <dbl>     <dbl>\n 1     101 A            1        120       105\n 2     102 B            0        145       135\n 3     103 C            0        150       150\n 4     104 A            1         NA       120\n 5     105 C           NA        155       135\n 6     106 A            1         NA       115\n 7     107 A            0        135       160\n 8     108 <NA>         1         NA       150\n 9     109 B           NA        115       130\n10     110 C            1        170       155\n11     111 A            0        150       140\n12     112 B            0        145       140\n13     113 C            1        140       150\n14     114 A            1        160       135\n15     115 B           NA        135       120"},{"path":"miss.html","id":"identifying-missingness-with-naniar-functions","chapter":"8 Missing Data and Single Imputation","heading":"8.2 Identifying missingness with naniar functions","text":"naniar package many useful functions.many missing values , overall?many variables missing values, overall?variables contain missing values?many missing values variable?missing one treat, 3 diabetes 3 sbp.values.Can plot missingness, variable?many cases (rows) missing values?many cases complete data, missing values?Can tabulate missingness case?cases missing values?can identify subjects missing data?nine subjects complete data, three subjects missing diabetes (), two subjects missing sbp.(), 1 subject missing treat sbp..","code":"\nn_miss(sbp_example)[1] 7\nn_var_miss(sbp_example)[1] 3\nmiss_var_which(sbp_example)[1] \"treat\"      \"diabetes\"   \"sbp.before\"\nmiss_var_summary(sbp_example)# A tibble: 5 x 3\n  variable   n_miss pct_miss\n  <chr>       <int>    <dbl>\n1 diabetes        3    20   \n2 sbp.before      3    20   \n3 treat           1     6.67\n4 subject         0     0   \n5 sbp.after       0     0   \ngg_miss_var(sbp_example)Warning: It is deprecated to specify `guide = FALSE` to\nremove a guide. Please use `guide = \"none\"` instead.\nn_case_miss(sbp_example)[1] 6\nn_case_complete(sbp_example)[1] 9\nmiss_case_table(sbp_example)# A tibble: 3 x 3\n  n_miss_in_case n_cases pct_cases\n           <int>   <int>     <dbl>\n1              0       9     60   \n2              1       5     33.3 \n3              2       1      6.67\nmiss_case_summary(sbp_example)# A tibble: 15 x 3\n    case n_miss pct_miss\n   <int>  <int>    <dbl>\n 1     8      2       40\n 2     4      1       20\n 3     5      1       20\n 4     6      1       20\n 5     9      1       20\n 6    15      1       20\n 7     1      0        0\n 8     2      0        0\n 9     3      0        0\n10     7      0        0\n11    10      0        0\n12    11      0        0\n13    12      0        0\n14    13      0        0\n15    14      0        0\nsbp_example %>% filter(!complete.cases(.))# A tibble: 6 x 5\n  subject treat diabetes sbp.before sbp.after\n    <int> <fct>    <dbl>      <dbl>     <dbl>\n1     104 A            1         NA       120\n2     105 C           NA        155       135\n3     106 A            1         NA       115\n4     108 <NA>         1         NA       150\n5     109 B           NA        115       130\n6     115 B           NA        135       120"},{"path":"miss.html","id":"missing-data-mechanisms","chapter":"8 Missing Data and Single Imputation","heading":"8.3 Missing-data mechanisms","text":"source description mechanisms Chapter 25 Andrew Gelman Jennifer Hill,28 chapter available link.MCAR = Missingness completely random. variable missing completely random probability missingness units, example, subject, decide whether collect diabetes status rolling die refusing answer “6” shows . data missing completely random, throwing cases missing data bias inferences.Missingness depends observed predictors. general assumption, called missing random MAR, probability variable missing depends available information. , willing assume probability nonresponse diabetes depends , fully recorded variables data. often reasonable model process logistic regression, outcome variable equals 1 observed cases 0 missing. outcome variable missing random, acceptable exclude missing cases (, treat NA), long regression controls variables affect probability missingness.Missingness depends unobserved predictors. Missingness longer “random” depends information recorded information also predicts missing values. particular treatment causes discomfort, patient likely drop study. missingness random (unless “discomfort” measured observed patients). missingness random, must explicitly modeled, else must accept bias inferences.Missingness depends missing value . Finally, particularly difficult situation arises probability missingness depends (potentially missing) variable . example, suppose people higher earnings less likely reveal .Essentially, situations 3 4 referred collectively non-random missingness, cause trouble us 1 2.","code":""},{"path":"miss.html","id":"options-for-dealing-with-missingness","chapter":"8 Missing Data and Single Imputation","heading":"8.4 Options for Dealing with Missingness","text":"several available methods dealing missing data MCAR MAR, basically boil :Complete Case (Available Case) analysesSingle ImputationMultiple Imputation","code":""},{"path":"miss.html","id":"complete-case-and-available-case-analyses","chapter":"8 Missing Data and Single Imputation","heading":"8.5 Complete Case (and Available Case) analyses","text":"Complete Case analyses, rows containing NA values omitted data analyses commence. default approach many statistical software packages, may introduce unpredictable bias fail include useful, often hard-won information.complete case analysis can appropriate number missing observations large, missing pattern either MCAR (missing completely random) MAR (missing random.)Two problems arise complete-case analysis:\nunits missing values differ systematically completely observed cases, bias complete-case analysis.\nmany variables included model, may complete cases, data discarded sake straightforward analysis.\nunits missing values differ systematically completely observed cases, bias complete-case analysis.many variables included model, may complete cases, data discarded sake straightforward analysis.related approach available-case analysis different aspects problem studied different subsets data, perhaps identified basis missing .","code":""},{"path":"miss.html","id":"single-imputation","chapter":"8 Missing Data and Single Imputation","heading":"8.6 Single Imputation","text":"single imputation analyses, NA values estimated/replaced one time one particular data value purpose obtaining complete samples, expense creating potential bias eventual conclusions obtaining slightly less accurate estimates available missing values data.single imputation can just replacement mean median (quantity) mode (categorical variable.) However, approach, though easy understand, underestimates variance ignores relationship missing values variables.Single imputation can also done using variety models try capture information NA values available variables within data set.simputation package can help us execute single imputations using wide variety techniques, within pipe approach used tidyverse. Another approach used past mice package, can also perform single imputations.","code":""},{"path":"miss.html","id":"multiple-imputation","chapter":"8 Missing Data and Single Imputation","heading":"8.7 Multiple Imputation","text":"Multiple imputation, NA values repeatedly estimated/replaced multiple data values, purpose obtaining mode complete samples capturing details variation inherent fact data missingness, obtain accurate estimates possible single imputation.’ll postpone discussion multiple imputation later semester.","code":""},{"path":"miss.html","id":"building-a-complete-case-analysis","chapter":"8 Missing Data and Single Imputation","heading":"8.8 Building a Complete Case Analysis","text":"can drop missing values data set drop_na na.omit filtering complete.cases. approaches produces result - new data set 9 rows (dropping six subjects NA values) 5 columns.","code":"\ncc.1 <- na.omit(sbp_example)\ncc.2 <- sbp_example %>% drop_na\ncc.3 <- sbp_example %>% filter(complete.cases(.))"},{"path":"miss.html","id":"single-imputation-with-the-mean-or-mode","chapter":"8 Missing Data and Single Imputation","heading":"8.9 Single Imputation with the Mean or Mode","text":"straightforward approach single imputation impute single summary variable, mean, median mode., suppose decide imputesbp.mean (143.3) among non-missing values,diabetes common value, 1, andtreat common value, mode ()","code":"\nmosaic::favstats(~ sbp.before, data = sbp_example) min  Q1 median     Q3 max     mean       sd  n missing\n 115 135    145 151.25 170 143.3333 15.71527 12       3\nsbp_example %>% tabyl(diabetes, treat) %>%\n  adorn_totals(where = c(\"row\", \"col\")) diabetes A B C NA_ Total\n        0 2 2 1   0     5\n        1 4 0 2   1     7\n     <NA> 0 2 1   0     3\n    Total 6 4 4   1    15\nsi.1 <- sbp_example %>%\n    replace_na(list(sbp.before = 143.33,\n                    diabetes = 1,\n                    treat = \"A\"))\nsi.1# A tibble: 15 x 5\n   subject treat diabetes sbp.before sbp.after\n     <int> <fct>    <dbl>      <dbl>     <dbl>\n 1     101 A            1       120        105\n 2     102 B            0       145        135\n 3     103 C            0       150        150\n 4     104 A            1       143.       120\n 5     105 C            1       155        135\n 6     106 A            1       143.       115\n 7     107 A            0       135        160\n 8     108 A            1       143.       150\n 9     109 B            1       115        130\n10     110 C            1       170        155\n11     111 A            0       150        140\n12     112 B            0       145        140\n13     113 C            1       140        150\n14     114 A            1       160        135\n15     115 B            1       135        120"},{"path":"miss.html","id":"doing-single-imputation-with-simputation","chapter":"8 Missing Data and Single Imputation","heading":"8.10 Doing Single Imputation with simputation","text":"Single imputation potentially appropriate method missingness can assumed either completely random (MCAR) dependent observed predictors (MAR). ’ll use simputation package accomplish .simputation vignette available https://cran.r-project.org/web/packages/simputation/vignettes/intro.htmlThe simputation reference manual available https://cran.r-project.org/web/packages/simputation/simputation.pdfSuppose wanted use:robust linear model predict sbp.missing values, basis sbp.diabetes status, anda predictive mean matching approach (, unlike robust linear model, ensure values diabetes ’ve seen imputed) predict diabetes status, basis sbp., anda decision tree approach predict treat status, using variables dataDetails many available methods simputation provided manual. include:impute_cart uses Classification Regression Tree approach numerical categorical data. also impute_rf command uses Random Forests imputation.impute_pmm one several “hot deck” options imputation, one predictive mean matching, can used numeric data (). Missing values first imputed using predictive model. Next, predictions replaced observed values nearest prediction. imputation options group include random hot deck, sequential hot deck k-nearest neighbor imputation.impute_rlm one several regression imputation methods, including linear models, robust linear models (use called M-estimation impute numerical variables) lasso/elastic net/ridge regression models.simputation package can also EM-based multivariate imputation, multivariate random forest imputation, several approaches.","code":"\nset.seed(50001)\n\nimp.2 <- sbp_example %>%\n    impute_rlm(sbp.before ~ sbp.after + diabetes) %>%\n    impute_pmm(diabetes ~ sbp.after) %>%\n    impute_cart(treat ~ .)\n\nimp.2# A tibble: 15 x 5\n   subject treat diabetes sbp.before sbp.after\n *   <int> <fct>    <dbl>      <dbl>     <dbl>\n 1     101 A            1       120        105\n 2     102 B            0       145        135\n 3     103 C            0       150        150\n 4     104 A            1       139.       120\n 5     105 C            1       155        135\n 6     106 A            1       136.       115\n 7     107 A            0       135        160\n 8     108 A            1       155.       150\n 9     109 B            1       115        130\n10     110 C            1       170        155\n11     111 A            0       150        140\n12     112 B            0       145        140\n13     113 C            1       140        150\n14     114 A            1       160        135\n15     115 B            1       135        120"},{"path":"NYFS-Study.html","id":"NYFS-Study","chapter":"9 National Youth Fitness Survey","heading":"9 National Youth Fitness Survey","text":"nnyfs.csv nnyfs.Rds data files built Professor Love using data 2012 National Youth Fitness Survey.NHANES National Youth Fitness Survey (NNYFS) conducted 2012 collect data physical activity fitness levels order provide evaluation health fitness children U.S. ages 3 15. NNYFS collected data physical activity fitness levels youth interviews fitness tests.nnyfs data file (either .csv .Rds), ’m providing modest fraction available information. NNYFS (including information ’m using) available https://wwwn.cdc.gov/nchs/nhanes/search/nnyfs12.aspx.data elements ’m using fall four main groups, components:DemographicsDietaryExamination andQuestionnaireWhat merge elements available components NHANES National Youth Fitness Survey, reformulated (cases simplified) variables, restricted sample kids completed elements four components.","code":""},{"path":"NYFS-Study.html","id":"the-variables-included-in-nnyfs","chapter":"9 National Youth Fitness Survey","heading":"9.1 The Variables included in nnyfs","text":"section tells data come , briefly describe collected.","code":""},{"path":"NYFS-Study.html","id":"from-the-nnyfs-demographic-component","chapter":"9 National Youth Fitness Survey","heading":"9.1.1 From the NNYFS Demographic Component","text":"come Y_DEMO file.","code":""},{"path":"NYFS-Study.html","id":"from-the-nnyfs-dietary-component","chapter":"9 National Youth Fitness Survey","heading":"9.1.2 From the NNYFS Dietary Component","text":"Y_DR1TOT file, number variables related child’s diet, following summaries mostly describing consumption “yesterday” dietary recall questionnaire.","code":""},{"path":"NYFS-Study.html","id":"from-the-nnyfs-examination-component","chapter":"9 National Youth Fitness Survey","heading":"9.1.3 From the NNYFS Examination Component","text":"Y_BMX file Body Measures:Y_PLX file Plank test results:","code":""},{"path":"NYFS-Study.html","id":"from-the-nnyfs-questionnaire-component","chapter":"9 National Youth Fitness Survey","heading":"9.1.4 From the NNYFS Questionnaire Component","text":"Y_PAQ file Physical Activity questions:Y_DBQ file Diet Behavior Nutrition questions:Y_HIQ file Health Insurance questions:Y_HUQ file Access Care questions:Y_MCQ file Medical Conditions questions:Y_RXQ_RX file Prescription Medication questions:","code":""},{"path":"NYFS-Study.html","id":"looking-over-a-few-variables","chapter":"9 National Youth Fitness Survey","heading":"9.2 Looking over A Few Variables","text":"Now, ’ll take look nnyfs data, ’ve made available comma-separated version (nnyfs.csv), prefer, well R data set (nnyfs.Rds) loads bit faster. loading file, let’s get handle size contents. R Project notes, data contained separate data subdirectory.1518 rows (subjects) 45 columns (variables), mean 1518 kids nnyfs data frame, 45 pieces information subject.\n, , exactly?Tibbles modern reimagining main way people stored data R, called data frame. Tibbles developed keep time proven effective, throwing . can learn something structure tibble functions str glimpse.lot variables . Let’s run first little detail.","code":"\nnnyfs <- readRDS(\"data/nnyfs.Rds\")\n\n## size of the tibble\ndim(nnyfs)[1] 1518   45\nnnyfs # this is a tibble, has some nice features in a print-out like this# A tibble: 1,518 x 45\n    SEQN sex    age_child race_eth       educ_child language\n   <dbl> <fct>      <dbl> <fct>               <dbl> <fct>   \n 1 71917 Female        15 3_Black Non-H~          9 English \n 2 71918 Female         8 3_Black Non-H~          2 English \n 3 71919 Female        14 2_White Non-H~          8 English \n 4 71920 Female        15 2_White Non-H~          8 English \n 5 71921 Male           3 2_White Non-H~         NA English \n 6 71922 Male          12 1_Hispanic              6 English \n 7 71923 Male          12 2_White Non-H~          5 English \n 8 71924 Female         8 4_Other Race/~          2 English \n 9 71925 Male           7 1_Hispanic              0 English \n10 71926 Male           8 3_Black Non-H~          2 English \n# ... with 1,508 more rows, and 39 more variables:\n#   sampling_wt <dbl>, income_pov <dbl>, age_adult <dbl>,\n#   educ_adult <fct>, respondent <fct>, salt_used <fct>,\n#   energy <dbl>, protein <dbl>, sugar <dbl>, fat <dbl>,\n#   diet_yesterday <fct>, water <dbl>, plank_time <dbl>,\n#   height <dbl>, weight <dbl>, bmi <dbl>, bmi_cat <fct>,\n#   arm_length <dbl>, waist <dbl>, arm_circ <dbl>, ...\nstr(nnyfs)tibble [1,518 x 45] (S3: tbl_df/tbl/data.frame)\n $ SEQN                : num [1:1518] 71917 71918 71919 71920 71921 ...\n $ sex                 : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 2 2 2 1 2 2 ...\n $ age_child           : num [1:1518] 15 8 14 15 3 12 12 8 7 8 ...\n $ race_eth            : Factor w/ 4 levels \"1_Hispanic\",\"2_White Non-Hispanic\",..: 3 3 2 2 2 1 2 4 1 3 ...\n $ educ_child          : num [1:1518] 9 2 8 8 NA 6 5 2 0 2 ...\n $ language            : Factor w/ 2 levels \"English\",\"Spanish\": 1 1 1 1 1 1 1 1 1 1 ...\n $ sampling_wt         : num [1:1518] 28299 15127 29977 80652 55592 ...\n $ income_pov          : num [1:1518] 0.21 5 5 0.87 4.34 5 5 2.74 0.46 1.57 ...\n $ age_adult           : num [1:1518] 46 46 42 53 31 42 39 31 45 56 ...\n $ educ_adult          : Factor w/ 5 levels \"1_Less than 9th Grade\",..: 2 3 5 3 3 4 2 3 2 3 ...\n $ respondent          : Factor w/ 3 levels \"Child\",\"Mom\",..: 1 2 1 1 2 1 1 1 2 1 ...\n $ salt_used           : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 2 2 1 2 ...\n $ energy              : num [1:1518] 2844 1725 2304 1114 1655 ...\n $ protein             : num [1:1518] 169.1 55.2 199.3 14 50.6 ...\n $ sugar               : num [1:1518] 128.2 118.7 81.4 119.2 90.3 ...\n $ fat                 : num [1:1518] 127.9 63.7 86.1 36 53.3 ...\n $ diet_yesterday      : Factor w/ 3 levels \"1_Much more than usual\",..: 2 2 2 2 2 2 1 2 2 3 ...\n $ water               : num [1:1518] 607 178 503 859 148 ...\n $ plank_time          : num [1:1518] NA 45 121 45 11 107 127 44 184 58 ...\n $ height              : num [1:1518] NA 131.6 172 167.1 90.2 ...\n $ weight              : num [1:1518] NA 38.6 58.7 92.5 12.4 66.4 56.7 22.2 20.9 28.3 ...\n $ bmi                 : num [1:1518] NA 22.3 19.8 33.1 15.2 25.9 22.5 14.4 15.9 17 ...\n $ bmi_cat             : Factor w/ 4 levels \"1_Underweight\",..: NA 4 2 4 2 4 3 2 2 2 ...\n $ arm_length          : num [1:1518] NA 27.7 38.4 35.9 18.3 34.2 33 26.5 24.2 26 ...\n $ waist               : num [1:1518] NA 71.9 79.4 96.4 46.8 90 72.3 56.1 54.5 59.7 ...\n $ arm_circ            : num [1:1518] NA 25.4 26 37.9 15.1 29.5 27.9 17.6 17.7 19.9 ...\n $ calf_circ           : num [1:1518] NA 32.3 35.3 46.8 19.4 36.9 36.8 24 24.3 27.3 ...\n $ calf_skinfold       : num [1:1518] NA 22 18.4 NA 8.4 22 18.3 7 7.2 8.2 ...\n $ triceps_skinfold    : num [1:1518] NA 19.9 15 20.6 8.6 22.8 20.5 12.9 6.9 8.8 ...\n $ subscapular_skinfold: num [1:1518] NA 17.4 9.8 22.8 5.7 24.4 12.6 6.8 4.8 6.1 ...\n $ active_days         : num [1:1518] 3 5 3 3 7 2 5 3 7 7 ...\n $ tv_hours            : num [1:1518] 2 2 1 3 2 3 0 4 2 2 ...\n $ computer_hours      : num [1:1518] 1 2 3 3 0 1 0 3 1 1 ...\n $ physical_last_week  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 2 2 2 2 2 ...\n $ enjoy_recess        : Factor w/ 5 levels \"1_Strongly Agree\",..: 1 1 3 2 NA 2 2 NA 1 1 ...\n $ meals_out           : num [1:1518] 0 2 3 2 1 1 2 1 0 2 ...\n $ insured             : Factor w/ 2 levels \"Has Insurance\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ phys_health         : Factor w/ 5 levels \"1_Excellent\",..: 1 3 1 3 1 1 3 1 2 1 ...\n $ access_to_care      : Factor w/ 2 levels \"Has Usual Care Source\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ care_source         : Factor w/ 6 levels \"Clinic or Health Center\",..: 1 2 2 2 2 2 2 2 2 2 ...\n $ asthma_ever         : Factor w/ 2 levels \"History of Asthma\",..: 2 1 2 1 2 2 2 2 2 2 ...\n $ asthma_now          : Factor w/ 2 levels \"Asthma Now\",\"No Asthma Now\": 2 1 2 1 2 2 2 2 2 2 ...\n $ med_use             : Factor w/ 2 levels \"Had Medication\",..: 2 1 2 1 2 2 2 2 2 2 ...\n $ med_count           : num [1:1518] 0 1 0 2 0 0 0 0 0 0 ...\n $ insurance           : Factor w/ 10 levels \"Medicaid\",\"Medicare\",..: 8 8 5 8 5 5 5 5 8 1 ..."},{"path":"NYFS-Study.html","id":"seqn","chapter":"9 National Youth Fitness Survey","heading":"9.2.1 SEQN","text":"first variable, SEQN just (numerical) identifying code attributable given subject survey. nominal data, little interest line. occasions, case, ID numbers sequential, sense subject 71919 included data base subject 71918, fact isn’t particularly interesting , protocol remained unchanged throughout study.","code":""},{"path":"NYFS-Study.html","id":"sex","chapter":"9 National Youth Fitness Survey","heading":"9.2.2 sex","text":"second variable, sex, listed factor variable (R uses factor character refer categorical, especially non-numeric information). , can see , two levels, Female Male.","code":"\nnnyfs %>%\n  tabyl(sex) %>%\n  adorn_totals() %>%\n  adorn_pct_formatting()    sex    n percent\n Female  760   50.1%\n   Male  758   49.9%\n  Total 1518  100.0%"},{"path":"NYFS-Study.html","id":"age_child","chapter":"9 National Youth Fitness Survey","heading":"9.2.3 age_child","text":"third variable, age_child, age child time screening study, measured years. Note age continuous concept, measure used (number full years alive) common discrete approach measurement. Age, course, meaningful zero point, can thought ratio variable; child 6 half old one 12. can tabulate observed values, since dozen .time initial screening, children 3 15 years age, things look reasonable. Since meaningful quantitative variable, may interested descriptive summary.six numbers provide nice, incomplete, look ages.Min. = minimum, youngest age examination 3 years old.1st Qu. = first quartile (25th percentile) ages 6. means 25 percent subjects age 6 less.Median = second quartile (50th percentile) ages 9. often used describe center data. Half subjects age 9 less.3rd Qu. = third quartile (75th percentile) ages 12Max. = maximum, oldest age examination 15 years.get standard deviation count missing non-missing observations favstats mosaic package.","code":"\nnnyfs %>% tabyl(age_child) %>%\n  adorn_pct_formatting() age_child   n percent\n         3 110    7.2%\n         4 112    7.4%\n         5 114    7.5%\n         6 129    8.5%\n         7 123    8.1%\n         8 112    7.4%\n         9  99    6.5%\n        10 124    8.2%\n        11 111    7.3%\n        12 137    9.0%\n        13 119    7.8%\n        14 130    8.6%\n        15  98    6.5%\nnnyfs %>% select(age_child) %>% \n  summary()   age_child     \n Min.   : 3.000  \n 1st Qu.: 6.000  \n Median : 9.000  \n Mean   : 9.033  \n 3rd Qu.:12.000  \n Max.   :15.000  \nmosaic::favstats(~ age_child, data = nnyfs) %>%\n  kable(digits = 1)"},{"path":"NYFS-Study.html","id":"race_eth","chapter":"9 National Youth Fitness Survey","heading":"9.2.4 race_eth","text":"fourth variable data set race_eth, multi-categorical variable describing child’s race ethnicity.now, get idea looking whether numerical summaries children’s ages varies race/ethnicity…","code":"\nnnyfs %>% tabyl(race_eth) %>% \n  adorn_pct_formatting() %>%\n  knitr::kable()\nmosaic::favstats(age_child ~ race_eth, data = nnyfs)                race_eth min   Q1 median Q3 max     mean\n1             1_Hispanic   3 5.25    9.0 12  15 8.793333\n2   2_White Non-Hispanic   3 6.00    9.0 12  15 9.137705\n3   3_Black Non-Hispanic   3 6.00    9.0 12  15 9.038462\n4 4_Other Race/Ethnicity   3 7.00    9.5 12  15 9.383333\n        sd   n missing\n1 3.733846 450       0\n2 3.804421 610       0\n3 3.576423 338       0\n4 3.427970 120       0"},{"path":"NYFS-Study.html","id":"income_pov","chapter":"9 National Youth Fitness Survey","heading":"9.2.5 income_pov","text":"Skipping bit, let’s look family income multiple poverty level. ’s summary.see missing data . Let’s ignore moment concentrate interpreting results children actual data. start picture.histogram shows us values truncated 5, children whose actual family income 5 times poverty line listed 5. also see message reminding us data missing variable.relationship income_pov race_eth data?deserves picture. Let’s try boxplot.","code":"\nnnyfs %>% select(income_pov) %>% summary()   income_pov   \n Min.   :0.000  \n 1st Qu.:0.870  \n Median :1.740  \n Mean   :2.242  \n 3rd Qu.:3.520  \n Max.   :5.000  \n NA's   :89     \nggplot(nnyfs, aes(x = income_pov)) +\n  geom_histogram(bins = 30, fill = \"white\", col = \"blue\")Warning: Removed 89 rows containing non-finite values\n(stat_bin).\nmosaic::favstats(income_pov ~ race_eth, data = nnyfs) %>%\n  kable(digits = 1)\nggplot(nnyfs, aes(x = race_eth, y = income_pov)) +\n  geom_boxplot()Warning: Removed 89 rows containing non-finite values\n(stat_boxplot)."},{"path":"NYFS-Study.html","id":"bmi","chapter":"9 National Youth Fitness Survey","heading":"9.2.6 bmi","text":"Moving body measurement data, bmi body-mass index child. BMI person’s weight kilograms divided height meters squared. Symbolically, BMI = weight kg / (height m)2. continuous concept, measured many decimal places like, meaningful zero point, ’s ratio variable.table BMI values great idea, data? hint R represents variable num numeric depiction data structure, implies R decimal values stored. , ’ll use head() function tail() function show first last values prove long table bmi values.","code":"\nnnyfs %>% select(bmi) %>% summary()      bmi       \n Min.   :11.90  \n 1st Qu.:15.90  \n Median :18.10  \n Mean   :19.63  \n 3rd Qu.:21.90  \n Max.   :48.30  \n NA's   :4      \nnnyfs %>% tabyl(bmi) %>% \n  adorn_pct_formatting() %>% \n  head()  bmi n percent valid_percent\n 11.9 1    0.1%          0.1%\n 12.6 1    0.1%          0.1%\n 12.7 1    0.1%          0.1%\n 12.9 1    0.1%          0.1%\n 13.0 2    0.1%          0.1%\n 13.1 1    0.1%          0.1%\nnnyfs %>% tabyl(bmi) %>% \n  adorn_pct_formatting() %>% \n  tail()  bmi n percent valid_percent\n 42.8 1    0.1%          0.1%\n 43.0 1    0.1%          0.1%\n 46.9 1    0.1%          0.1%\n 48.2 1    0.1%          0.1%\n 48.3 1    0.1%          0.1%\n   NA 4    0.3%             -"},{"path":"NYFS-Study.html","id":"bmi_cat","chapter":"9 National Youth Fitness Survey","heading":"9.2.7 bmi_cat","text":"Next ’ll look bmi_cat information. four-category ordinal variable, divides sample according BMI four groups. BMI categories use sex-specific 2000 BMI--age (months) growth charts prepared Centers Disease Control US. can get breakdown table variable’s values.terms percentiles age sex growth charts, meanings categories :Underweight (BMI < 5th percentile)Normal weight (BMI 5th < 85th percentile)Overweight (BMI 85th < 95th percentile)Obese (BMI \\(\\geq\\) 95th percentile)Note ’ve used labels bmi_cat variable include number start table results sorted rational way. R sorts tables alphabetically, general. ’ll use forcats package work categorical variables store factors eventually, now, ’ll keep things relatively simple.Note bmi_cat data don’t completely separate raw bmi data, calculation percentiles requires different tables combination age sex.","code":"\nnnyfs %>% tabyl(bmi_cat) %>% adorn_pct_formatting()       bmi_cat   n percent valid_percent\n 1_Underweight  41    2.7%          2.7%\n      2_Normal 920   60.6%         60.8%\n  3_Overweight 258   17.0%         17.0%\n       4_Obese 295   19.4%         19.5%\n          <NA>   4    0.3%             -\nmosaic::favstats(bmi ~ bmi_cat, data = nnyfs) %>%\n  kable(digits = 1)"},{"path":"NYFS-Study.html","id":"waist","chapter":"9 National Youth Fitness Survey","heading":"9.2.8 waist","text":"Let’s also look briefly waist, circumference child’s waist, centimeters. , numeric variable, perhaps ’ll stick simple summary, rather obtaining table observed values.’s histogram waist circumference data.","code":"\nmosaic::favstats(~ waist, data = nnyfs)   min   Q1 median   Q3   max     mean       sd    n missing\n 42.5 55.6   64.8 76.6 144.7 67.70536 15.19809 1512       6\nggplot(nnyfs, aes(x = waist)) +\n  geom_histogram(bins = 25, fill = \"tomato\", color = \"cyan\")Warning: Removed 6 rows containing non-finite values\n(stat_bin)."},{"path":"NYFS-Study.html","id":"triceps_skinfold","chapter":"9 National Youth Fitness Survey","heading":"9.2.9 triceps_skinfold","text":"last variable ’ll look now triceps_skinfold, measured millimeters. one several common locations used assessment body fat using skinfold calipers, frequent part growth assessments children. , numeric variable according R.’s histogram triceps skinfold data, fill color flipped saw plot waist circumference data moment ago.OK. ’ve seen variables, ’ll move now look seriously data.","code":"\nmosaic::favstats(~ triceps_skinfold, data = nnyfs) min  Q1 median Q3  max     mean       sd    n missing\n   4 9.1   12.4 18 38.8 14.35725 6.758825 1497      21\nggplot(nnyfs, aes(x = triceps_skinfold)) +\n  geom_histogram(bins = 25, fill = \"cyan\", color = \"tomato\")Warning: Removed 21 rows containing non-finite values\n(stat_bin)."},{"path":"NYFS-Study.html","id":"additional-numeric-summaries","chapter":"9 National Youth Fitness Survey","heading":"9.3 Additional Numeric Summaries","text":"","code":""},{"path":"NYFS-Study.html","id":"the-five-number-summary-quantiles-and-iqr","chapter":"9 National Youth Fitness Survey","heading":"9.3.1 The Five Number Summary, Quantiles and IQR","text":"five number summary famous used form box plot - ’s minimum, 25th percentile, median, 75th percentile maximum. numerical integer variables, summary function produces five number summary, plus mean, count missing values (NA’s).alternative, can use $ notation indicate variable wish study inside data set, can use fivenum function get five numbers used developing box plot. ’ll focus little number kilocalories consumed child, according dietary recall questionnaire. ’s energy variable.mentioned 6.3.1, inter-quartile range, IQR, sometimes used competitor standard deviation. ’s difference 75th percentile 25th percentile. 25th percentile, median, 75th percentile referred quartiles data set, , together, split data quarters.can obtain quantiles (percentiles) like - , ’m asking 1st 99th:","code":"\nnnyfs %>% \n  select(waist, energy, sugar) %>%\n  summary()     waist            energy         sugar       \n Min.   : 42.50   Min.   : 257   Min.   :  1.00  \n 1st Qu.: 55.60   1st Qu.:1368   1st Qu.: 82.66  \n Median : 64.80   Median :1794   Median :116.92  \n Mean   : 67.71   Mean   :1877   Mean   :124.32  \n 3rd Qu.: 76.60   3rd Qu.:2306   3rd Qu.:157.05  \n Max.   :144.70   Max.   :5265   Max.   :405.49  \n NA's   :6                                       \nfivenum(nnyfs$energy)[1]  257.0 1367.0 1794.5 2306.0 5265.0\nIQR(nnyfs$energy)[1] 938.5\nquantile(nnyfs$energy, probs=c(0.01, 0.99))     1%     99% \n 566.85 4051.75 "},{"path":"NYFS-Study.html","id":"additional-summaries-from-favstats","chapter":"9 National Youth Fitness Survey","heading":"9.4 Additional Summaries from favstats","text":"’re focusing single variable, favstats function mosaic package can helpful. Rather calling entire mosaic library , ’ll just specify function within library.adds three useful results base summary - standard deviation, sample size number missing observations.","code":"\nmosaic::favstats(~ energy, data = nnyfs) min     Q1 median   Q3  max     mean       sd    n missing\n 257 1367.5 1794.5 2306 5265 1877.157 722.3537 1518       0"},{"path":"NYFS-Study.html","id":"the-histogram","chapter":"9 National Youth Fitness Survey","heading":"9.5 The Histogram","text":"Obtaining basic histogram , example, energy (kilocalories consumed) nnyfs data pretty straightforward.","code":"\nggplot(data = nnyfs, aes(x = energy)) +\n    geom_histogram(binwidth = 100, col = \"white\")"},{"path":"NYFS-Study.html","id":"freedman-diaconis-rule-to-select-bin-width","chapter":"9 National Youth Fitness Survey","heading":"9.5.1 Freedman-Diaconis Rule to select bin width","text":"like, can suggest particular number cells histogram, instead accepting defaults. case, \\(n\\) = 1518 observations. Freedman-Diaconis rule can helpful . rule suggests set bin-width \\[\nh = \\frac{2*IQR}{n^{1/3}}\n\\]number bins equal range data set (maximum - minimum) divided \\(h\\).energy data nnyfs tibble, haveIQR 938.5, \\(n\\) = 1518 range = 5008Thus, Freedman-Diaconis rule, optimal binwidth \\(h\\) 163.3203676, , realistically, 163.number bins 30.6636586, , realistically 31., ’ll draw graph , using Freedman-Diaconis rule identify number bins, also play around bit fill color bars.nice start, means finished graph.Let’s improve axis labels, add title, fill bars distinctive blue use black outline around bar. ’ll just use 25 bars, like looks case, optimizing number bins rarely important.","code":"\nbw <- 2 * IQR(nnyfs$energy) / length(nnyfs$energy)^(1/3)\nggplot(data = nnyfs, aes(x = energy)) +\n    geom_histogram(binwidth=bw, color = \"white\", fill = \"black\")\nggplot(data = nnyfs, aes(x = energy)) +\n    geom_histogram(bins=25, color = \"black\", fill = \"dodgerblue\") + \n    labs(title = \"Histogram of Body-Mass Index Results in the nnyfs data\",\n         x = \"Energy Consumed (kcal)\", y = \"# of Subjects\")"},{"path":"NYFS-Study.html","id":"a-note-on-colors","chapter":"9 National Youth Fitness Survey","heading":"9.5.2 A Note on Colors","text":"simplest way specify color name, enclosed parentheses. favorite list R colors http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf. pinch, can usually find googling Colors R. can also type colors() R console obtain list names 657 colors.using colors make comparisons, may interested using scale nice properties. viridis package vignette describes four color scales (viridis, magma, plasma inferno) designed colorful, robust colorblindness gray scale printing, perceptually uniform, means (package authors describe ) values close similar-appearing colors values far away different-appearing colors, consistently across range values. can apply colors special functions within ggplot.’s comparison several histograms, looking energy consumed function whether yesterday typical terms food consumption.don’t really need legend , perhaps restrict plot participants responded diet_yesterday question, put title better axis labels?","code":"\nggplot(data = nnyfs, aes(x = energy, fill = diet_yesterday)) +\n  geom_histogram(bins = 20, col = \"white\") +\n  scale_fill_viridis_d() +\n  facet_wrap(~ diet_yesterday)\nnnyfs %>% filter(complete.cases(energy, diet_yesterday)) %>%\n  ggplot(data = ., aes(x = energy, fill = diet_yesterday)) +\n  geom_histogram(bins = 20, col = \"white\") +\n  scale_fill_viridis_d() +\n  guides(fill = \"none\") +\n  facet_wrap(~ diet_yesterday) +\n  labs(x = \"Energy consumed, in kcal\",\n       title = \"Energy Consumption and How Typical Was Yesterday's Eating\",\n       subtitle = \"NHANES National Youth Fitness Survey, no survey weighting\")"},{"path":"NYFS-Study.html","id":"the-frequency-polygon","chapter":"9 National Youth Fitness Survey","heading":"9.6 The Frequency Polygon","text":"’ve seen, can also plot distribution single continuous variable using freqpoly geom. can also add rug plot, places small vertical line horizontal axis everywhere observation appears data.","code":"\nggplot(data = nnyfs, aes(x = energy)) +\n    geom_freqpoly(binwidth = 150, color = \"dodgerblue\") + \n    geom_rug(color = \"red\") +\n    labs(title = \"Frequency Polygon of nnyfs Energy data\",\n         x = \"Energy (kcal)\", y = \"# of Patients\")"},{"path":"NYFS-Study.html","id":"plotting-the-probability-density-function","chapter":"9 National Youth Fitness Survey","heading":"9.7 Plotting the Probability Density Function","text":"can also produce density function, effect smoothing bumps histogram frequency polygon, also changing plotted y-axis., ’s density function?probability density function function continuous variable, x, represents probability x falling within given range. Specifically, integral interval (,b) density function gives probability value x within (,b).’re interested exploring notion density functions continuous (discrete) random variables, nice elementary material available Khan Academy.","code":"\nggplot(data = nnyfs, aes(x = energy)) +\n    geom_density(kernel = \"gaussian\", color = \"dodgerblue\") + \n    labs(title = \"Density of nnyfs Energy data\",\n         x = \"Energy (kcal)\", y = \"Probability Density function\")"},{"path":"NYFS-Study.html","id":"the-boxplot","chapter":"9 National Youth Fitness Survey","heading":"9.8 The Boxplot","text":"Sometimes, ’s helpful picture five-number summary data way get general sense distribution. One approach boxplot, sometimes called box--whisker plot.","code":""},{"path":"NYFS-Study.html","id":"drawing-a-boxplot-for-one-variable-in-ggplot2","chapter":"9 National Youth Fitness Survey","heading":"9.8.1 Drawing a Boxplot for One Variable in ggplot2","text":"ggplot2 library easily handles comparison boxplots multiple distributions, ’ll see moment. However, building boxplot single distribution requires little trickiness.","code":"\nggplot(nnyfs, aes(x = 1, y = energy)) + \n    geom_boxplot(fill = \"deepskyblue\") + \n    coord_flip() + \n    labs(title = \"Boxplot of Energy for kids in the NNYFS\",\n         y = \"Energy (kcal)\",\n         x = \"\") +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank())"},{"path":"NYFS-Study.html","id":"about-the-boxplot","chapter":"9 National Youth Fitness Survey","heading":"9.8.2 About the Boxplot","text":"boxplot another John Tukey invention.R draws box (yellow) edges box fall 25th 75th percentiles data, thick line inside box falls median (50th percentile).whiskers extend largest smallest values classified plot candidate outliers.outlier unusual point, far center distribution.Note ’ve used horizontal option show boxplot direction. comparison boxplots, ’ll see , oriented vertically.boxplot’s whiskers drawn first third quartiles (.e. 25th 75th percentiles) extreme points data meet standard ``candidate outliers.’’ outlier simply point far away center data - may due number reasons, generally indicates need investigation.software, including R, uses standard proposed Tukey describes ``candidate outlier’’ point upper fence lower fence. definitions fences based inter-quartile range (IQR).IQR = 75th percentile - 25th percentile, upper fence 75th percentile + 1.5 IQR, lower fence 25th percentile - 1.5 IQR.energy data,upper fence located 2306 + 1.5(938.5) = 3713.75the lower fence located 1367 - 1.5(938.5) = -40.75In case, see points identified outliers low part distribution, quite identified way high side. tends identify 5% data candidate outlier, data follow Normal distribution.plot indicating clearly asymmetry (skew) data, specifically right skew.standard R uses indicate outliers points 1.5 inter-quartile ranges away edges box.horizontal orientation ’ve chosen clarifies relationship direction skew plot. plot like , multiple outliers right side indicative long right tail distribution, hence, positive right skew - mean larger median. indications skew include one side box substantially wider , one side whiskers substantially longer . skew later.","code":""},{"path":"NYFS-Study.html","id":"a-simple-comparison-boxplot","chapter":"9 National Youth Fitness Survey","heading":"9.9 A Simple Comparison Boxplot","text":"Boxplots often used comparison. can build boxplots using ggplot2, well, ’ll discuss detail later. now, ’s boxplot built compare energy results subject’s race/ethnicity.Let’s look comparison observed energy levels across five categories phys_health variable, now making use viridis color scheme.graph, ’s bad, want improve ?Let’s turn boxes horizontal direction, get rid perhaps unnecessary phys_health labels.","code":"\nggplot(nnyfs, aes(x = factor(race_eth), y = energy, fill=factor(race_eth))) +\n  geom_boxplot() + \n  guides(fill = \"none\") +\n  labs(y = \"Energy consumed (kcal)\", x = \"Race/Ethnicity\")\nggplot(nnyfs, aes(x = factor(phys_health), y = energy, fill = factor(phys_health))) +\n  geom_boxplot() + \n  scale_fill_viridis_d() + \n  labs(title = \"Energy by Self-Reported Physical Health, in nnyfs data\")\nggplot(nnyfs, aes(x = factor(phys_health), y = energy, fill = factor(phys_health))) +\n    geom_boxplot() + \n    scale_fill_viridis_d() + \n    coord_flip() + \n    guides(fill = \"none\") +\n    labs(title = \"Energy Consumed by Self-Reported Physical Health\", \n         subtitle = \"NHANES National Youth Fitness Survey, unweighted\", \n         x = \"\")"},{"path":"NYFS-Study.html","id":"using-describe-in-the-psych-library","chapter":"9 National Youth Fitness Survey","heading":"9.10 Using describe in the psych library","text":"additional numerical summaries, one option consider using describe function psych library.package provides, order, following…n = sample sizemean = sample meansd = sample standard deviationmedian = median, 50th percentiletrimmed = mean middle 80% datamad = median absolute deviationmin = minimum value samplemax = maximum value samplerange = max - minskew = skewness measure, described (indicates degree asymmetry)kurtosis = kurtosis measure, described (indicates heaviness tails, degree outlier-proneness)se = standard error sample mean = sd / square root sample size, useful inference","code":"\npsych::describe(nnyfs$energy)   vars    n    mean     sd median trimmed    mad min  max\nX1    1 1518 1877.16 722.35 1794.5  1827.1 678.29 257 5265\n   range skew kurtosis    se\nX1  5008  0.8     1.13 18.54"},{"path":"NYFS-Study.html","id":"the-trimmed-mean","chapter":"9 National Youth Fitness Survey","heading":"9.10.1 The Trimmed Mean","text":"trimmed mean trim value R indicates proportion observations trimmed end outcome distribution mean calculated. trimmed value provided psych::describe package describes particular package calls 20% trimmed mean (bottom top 10% energy values removed taking mean - ’s mean middle 80% data.) might call 10% trimmed mean settings, ’s just .","code":"\nmean(nnyfs$energy, trim=.1) [1] 1827.1"},{"path":"NYFS-Study.html","id":"the-median-absolute-deviation","chapter":"9 National Youth Fitness Survey","heading":"9.10.2 The Median Absolute Deviation","text":"alternative IQR fancier, bit robust, median absolute deviation, , large sample sizes, data follow Normal distribution, (expectation) equal standard deviation. MAD median absolute deviations median, multiplied constant (1.4826) yield asymptotically normal consistency.","code":"\nmad(nnyfs$energy)[1] 678.2895"},{"path":"NYFS-Study.html","id":"assessing-skew","chapter":"9 National Youth Fitness Survey","heading":"9.11 Assessing Skew","text":"relatively common idea assess skewness, several measures available. Many models assume Normal distribution, , among things, data symmetric around mean.Skewness measures asymmetry distribution, left skew (mean < median) indicated negative skewness values, right skew (mean > median) indicated positive values. skew value near zero data follow symmetric distribution.","code":""},{"path":"NYFS-Study.html","id":"non-parametric-skewness","chapter":"9 National Youth Fitness Survey","heading":"9.11.1 Non-parametric Skewness","text":"simpler measure skew, sometimes called nonparametric skew closely related Pearson’s notion median skewness, falls -1 +1 distribution. just difference mean median, divided standard deviation.Values greater +0.2 sometimes taken indicate fairly substantial right skew, values -0.2 indicate fairly substantial left skew.Wikipedia page skewness, material derived, provides definitions several skewness measures.","code":"\n(mean(nnyfs$energy) - median(nnyfs$energy))/sd(nnyfs$energy)[1] 0.114427"},{"path":"NYFS-Study.html","id":"assessing-kurtosis-heavy-tailedness","chapter":"9 National Youth Fitness Survey","heading":"9.12 Assessing Kurtosis (Heavy-Tailedness)","text":"Another measure distribution’s shape can found psych library kurtosis. Kurtosis indicator whether distribution heavy-tailed light-tailed compared Normal distribution. Positive kurtosis means variance due outliers - unusual points far away mean relative might expect Normally distributed data set standard deviation.Normal distribution kurtosis value near 0, distribution similar tail behavior expect Normal said mesokurticHigher kurtosis values (meaningfully higher 0) indicate , compared Normal distribution, observed variance result extreme outliers (.e. heavy tails) opposed result modest sized deviations mean. heavy-tailed, outlier prone, distributions sometimes called leptokurtic.Kurtosis values meaningfully lower 0 indicate light-tailed data, fewer outliers ’d expect Normal distribution. distributions sometimes referred platykurtic, include distributions without outliers, like Uniform distribution.’s table:Note kurtosi() function strangely named, part psych package.","code":"\npsych::kurtosi(nnyfs$energy)[1] 1.130539"},{"path":"NYFS-Study.html","id":"the-standard-error-of-the-sample-mean","chapter":"9 National Youth Fitness Survey","heading":"9.12.1 The Standard Error of the Sample Mean","text":"standard error sample mean, standard deviation divided square root sample size:","code":"\nsd(nnyfs$energy)/sqrt(length(nnyfs$energy))[1] 18.54018"},{"path":"NYFS-Study.html","id":"the-describe-function-in-the-hmisc-package","chapter":"9 National Youth Fitness Survey","heading":"9.13 The describe function in the Hmisc package","text":"Hmisc package lots useful functions. ’s named main developer, Frank Harrell. describe function Hmisc knows enough separate numerical categorical variables, give separate (detailed) summaries .categorical variable, provides counts total observations (n), number missing values, number unique categories, along counts percentages falling category.numerical variable, provides:counts total observations (n), number missing values, number unique valuesan Info value data, indicates continuous variable (score 1 generally indicative completely continuous variable ties, scores near 0 indicate lots ties, unique values)sample MeanGini’s mean difference, robust measure spread, larger values indicating greater dispersion data. defined mean absolute difference pairs observations.many sample percentiles (quantiles) data, specifically (5, 10, 25, 50, 75, 90, 95, 99)either complete table observed values, counts percentages (modest number unique values), ora table five smallest five largest values data set, useful range checkingMore Info value Hmisc::describe available ","code":"\nnnyfs %>% \n  select(waist, energy, bmi) %>%\n  Hmisc::describe(). \n\n 3  Variables      1518  Observations\n------------------------------------------------------------\nwaist \n       n  missing distinct     Info     Mean      Gmd \n    1512        6      510        1    67.71     16.6 \n     .05      .10      .25      .50      .75      .90 \n   49.40    51.40    55.60    64.80    76.60    88.70 \n     .95 \n   96.84 \n\nlowest :  42.5  43.4  44.1  44.4  44.5\nhighest: 125.8 126.0 127.0 132.3 144.7\n------------------------------------------------------------\nenergy \n       n  missing distinct     Info     Mean      Gmd \n    1518        0     1137        1     1877    796.1 \n     .05      .10      .25      .50      .75      .90 \n     849     1047     1368     1794     2306     2795 \n     .95 \n    3195 \n\nlowest :  257  260  326  349  392, highest: 4382 4529 5085 5215 5265\n------------------------------------------------------------\nbmi \n       n  missing distinct     Info     Mean      Gmd \n    1514        4      225        1    19.63    5.269 \n     .05      .10      .25      .50      .75      .90 \n   14.30    14.90    15.90    18.10    21.90    26.27 \n     .95 \n   30.20 \n\nlowest : 11.9 12.6 12.7 12.9 13.0, highest: 42.8 43.0 46.9 48.2 48.3\n------------------------------------------------------------"},{"path":"NYFS-Study.html","id":"summarizing-data-within-subgroups","chapter":"9 National Youth Fitness Survey","heading":"9.14 Summarizing data within subgroups","text":"Suppose want understand subjects whose diet involved consuming much usual yesterday compare consumer usual amount, consumed much less usual, terms energy consumed, well protein. might start looking medians means.Perhaps restrict people missing diet_yesterday category, look now sugar water consumption.looks like children “Much usual” category consumed energy, protein, sugar water children two categories. Let’s draw picture .can see considerable overlap distributions, regardless ’re measuring.","code":"\nnnyfs %>%\n    group_by(diet_yesterday) %>%\n    select(diet_yesterday, energy, protein) %>%\n    summarise_all(list(median = median, mean = mean))# A tibble: 4 x 5\n  diet_yesterday    energy_median protein_median energy_mean\n  <fct>                     <dbl>          <dbl>       <dbl>\n1 1_Much more than~          2098           69.4       2150.\n2 2_Usual                    1794           61.3       1858.\n3 3_Much less than~          1643           53.9       1779.\n4 <NA>                       4348          155.        4348 \n# ... with 1 more variable: protein_mean <dbl>\nnnyfs %>%\n    filter(complete.cases(diet_yesterday)) %>%\n    group_by(diet_yesterday) %>%\n    select(diet_yesterday, energy, protein, sugar, water) %>%\n    summarise_all(list(median))# A tibble: 3 x 5\n  diet_yesterday         energy protein sugar water\n  <fct>                   <dbl>   <dbl> <dbl> <dbl>\n1 1_Much more than usual   2098    69.4  137.  500 \n2 2_Usual                  1794    61.3  114.  385.\n3 3_Much less than usual   1643    53.9  115.  311.\ntemp_dat <- nnyfs %>%\n    filter(complete.cases(diet_yesterday)) %>%\n    mutate(diet_yesterday = fct_recode(diet_yesterday,\n        \"Much more\" = \"1_Much more than usual\",\n        \"Usual diet\" = \"2_Usual\",\n        \"Much less\" = \"3_Much less than usual\"))\n\np1 <- ggplot(temp_dat, aes(x = diet_yesterday, y = energy)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = diet_yesterday), width = 0.2) +\n    theme_light() + \n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Energy Comparison\")\n\np2 <- ggplot(temp_dat, aes(x = diet_yesterday, y = protein)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = diet_yesterday), width = 0.2) +\n    theme_light() + \n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Protein Comparison\")\n\np3 <- ggplot(temp_dat, aes(x = diet_yesterday, y = sugar)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = diet_yesterday), width = 0.2) +\n    theme_light() + \n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Sugar Comparison\")\n\np4 <- ggplot(temp_dat, aes(x = diet_yesterday, y = water)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = diet_yesterday), width = 0.2) +\n    theme_light() + \n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Water Comparison\")\n\np1 + p2 + p3 + p4"},{"path":"NYFS-Study.html","id":"another-example","chapter":"9 National Youth Fitness Survey","heading":"9.15 Another Example","text":"Suppose now ask different question. kids larger categories BMI larger waist circumferences?Oops. Looks like need filter cases complete data BMI category waist circumference order get meaningful results. add count, ., use something like favstats mosaic package, automatically accounts missing data, omits calculating summary statistics within group.patients heavier groups generally higher waist circumferences, standard deviations suggest may meaningful overlap. Let’s draw picture, case comparison boxplot accompanying violin plot.data transformation dplyr cheat sheet found Help menu RStudio great resource. , course, details, visit Grolemund Wickham.29","code":"\nnnyfs %>%\n    group_by(bmi_cat) %>%\n    summarise(mean = mean(waist), sd = sd(waist), \n              median = median(waist), \n              skew_1 = round((mean(waist) - median(waist)) / \n                                 sd(waist),2))# A tibble: 5 x 5\n  bmi_cat        mean    sd median skew_1\n  <fct>         <dbl> <dbl>  <dbl>  <dbl>\n1 1_Underweight  55.2  7.58   54.5   0.09\n2 2_Normal       NA   NA      NA    NA   \n3 3_Overweight   72.3 11.9    74    -0.14\n4 4_Obese        NA   NA      NA    NA   \n5 <NA>           NA   NA      NA    NA   \nnnyfs %>%\n    filter(complete.cases(bmi_cat, waist)) %>%\n    group_by(bmi_cat) %>%\n    summarise(count = n(), mean = mean(waist), \n              sd = sd(waist), median = median(waist), \n       skew_1 = \n         round((mean(waist) - median(waist)) / sd(waist),2))# A tibble: 4 x 6\n  bmi_cat       count  mean    sd median skew_1\n  <fct>         <int> <dbl> <dbl>  <dbl>  <dbl>\n1 1_Underweight    41  55.2  7.58   54.5   0.09\n2 2_Normal        917  61.2  9.35   59.5   0.19\n3 3_Overweight    258  72.3 11.9    74    -0.14\n4 4_Obese         294  85.6 17.1    86.8  -0.07\nmosaic::favstats(waist ~ bmi_cat, data = nnyfs) %>%\n    kable(digits = 1)\nnnyfs %>%\n    filter(complete.cases(bmi_cat, waist)) %>%\n    ggplot(., aes(x = bmi_cat, y = waist)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = bmi_cat), width = 0.2) +\n    theme_light() + \n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Waist Circumference by BMI Category\")"},{"path":"NYFS-Study.html","id":"boxplots-to-relate-an-outcome-to-a-categorical-predictor","chapter":"9 National Youth Fitness Survey","heading":"9.16 Boxplots to Relate an Outcome to a Categorical Predictor","text":"Boxplots much useful comparing samples data. instance, consider comparison boxplot describing triceps skinfold results across four levels BMI category., probably want omit missing values (bmi_cat triceps_skinfold) also eliminate repetitive legend (guides) right.always, boxplot shows five-number summary (minimum, 25th percentile, median, 75th percentile maximum) addition highlighting candidate outliers.","code":"\nggplot(nnyfs, aes(x = bmi_cat, y = triceps_skinfold,\n                  fill = bmi_cat)) +\n    geom_boxplot() +\n    scale_fill_viridis_d() +\n    theme_light()Warning: Removed 21 rows containing non-finite values\n(stat_boxplot).\nnnyfs %>% \n    filter(complete.cases(bmi_cat, triceps_skinfold)) %>%\n    ggplot(., aes(x = bmi_cat, y = triceps_skinfold,\n                  fill = bmi_cat)) +\n    geom_boxplot() +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    theme_light() +\n    labs(x = \"BMI Category\", y = \"Triceps Skinfold in mm\",\n         title = \"Triceps Skinfold increases with BMI category\",\n         subtitle = \"NNYFS children\")"},{"path":"NYFS-Study.html","id":"augmenting-the-boxplot-with-the-sample-mean","chapter":"9 National Youth Fitness Survey","heading":"9.16.1 Augmenting the Boxplot with the Sample Mean","text":"Often, want augment plot, perhaps adding little diamond show sample mean within category, highlight skew (terms whether mean meaningfully different median.)","code":"\nnnyfs %>% \n    filter(complete.cases(bmi_cat, triceps_skinfold)) %>%\n    ggplot(., aes(x = bmi_cat, y = triceps_skinfold,\n                  fill = bmi_cat)) +\n    geom_boxplot() +\n    stat_summary(fun=\"mean\", geom=\"point\", \n                 shape=23, size=3, fill=\"white\") +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    theme_light() +\n    labs(x = \"BMI Category\", y = \"Triceps Skinfold in mm\",\n         title = \"Triceps Skinfold increases with BMI category\",\n         subtitle = \"NNYFS children\")"},{"path":"NYFS-Study.html","id":"building-a-violin-plot","chapter":"9 National Youth Fitness Survey","heading":"9.17 Building a Violin Plot","text":"number plots compare distributions data sets. interesting one called violin plot. violin plot kernel density estimate, mirrored form symmetrical shape.Traditionally, plots shown overlaid boxplots white dot median, like example, now looking waist circumference .","code":"\nnnyfs %>%\n    filter(complete.cases(triceps_skinfold, bmi_cat)) %>%\n    ggplot(., aes(x=bmi_cat, y=triceps_skinfold, \n                  fill = bmi_cat)) + \n    geom_violin(trim=FALSE) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Triceps Skinfold by BMI Category\")\nnnyfs %>%\n    filter(complete.cases(waist, bmi_cat)) %>%\n    ggplot(., aes(x = bmi_cat, y = waist, \n                  fill = bmi_cat)) + \n    geom_violin(trim=FALSE) +\n    geom_boxplot(width=.1, outlier.colour=NA, \n                 color = c(rep(\"white\",2), rep(\"black\",2))) +\n    stat_summary(fun=median, geom=\"point\", \n                 fill=\"white\", shape=21, size=3) + \n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Waist Circumference by BMI Category\")"},{"path":"NYFS-Study.html","id":"adding-notches-to-a-boxplot","chapter":"9 National Youth Fitness Survey","heading":"9.17.1 Adding Notches to a Boxplot","text":"Notches used boxplots help visually assess whether medians distributions across various groups actually differ statistically detectable extent. Think confidence regions around medians. notches overlap, situation, provides evidence medians populations represented samples may different.overlap notches four categories, might reasonably conclude true median triceps skinfold values across four categories statistically significantly different.example notches overlap, consider comparison plank times BMI category.overlap notches (instance Underweight Normal) suggests median plank times population interest don’t necessarily differ meaningful way BMI category, perhaps Obese group may shorter time.data somewhat right skewed. logarithmic transformation plot help us see patterns clearly?","code":"\nnnyfs %>% \n    filter(complete.cases(bmi_cat, triceps_skinfold)) %>%\n    ggplot(., aes(x = bmi_cat, y = triceps_skinfold)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = bmi_cat), width = 0.3, notch = TRUE) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    theme_light() +\n    labs(x = \"BMI Category\", y = \"Triceps Skinfold in mm\",\n         title = \"Triceps Skinfold increases with BMI category\",\n         subtitle = \"NNYFS children\")\nnnyfs %>% \n    filter(complete.cases(bmi_cat, plank_time)) %>%\n    ggplot(., aes(x=bmi_cat, y=plank_time)) +\n    geom_violin(aes(fill = bmi_cat)) +\n    geom_boxplot(width = 0.3, notch=TRUE) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") + \n    theme_light() +\n    labs(title = \"Plank Times by BMI category\", \n         x = \"\", y = \"Plank Time (in seconds)\")\nnnyfs %>% \n    filter(complete.cases(bmi_cat, plank_time)) %>%\n    ggplot(., aes(x=bmi_cat, y = log(plank_time))) +\n    geom_violin() +\n    geom_boxplot(aes(fill = bmi_cat), width = 0.3, notch=TRUE) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") + \n    theme_light() +\n    labs(title = \"log(Plank Times) by BMI category\", \n         x = \"\", y = \"Natural Log of Plank Time\")"},{"path":"NYFS-Study.html","id":"using-multiple-histograms-to-make-comparisons","chapter":"9 National Youth Fitness Survey","heading":"9.18 Using Multiple Histograms to Make Comparisons","text":"can make array histograms describe multiple groups data, using ggplot2 notion faceting plot.","code":"\nnnyfs %>% \n    filter(complete.cases(triceps_skinfold, bmi_cat)) %>%\n    ggplot(., aes(x=triceps_skinfold, fill = bmi_cat)) +\n    geom_histogram(binwidth = 2, color = \"black\") + \n    facet_wrap(~ bmi_cat) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Triceps Skinfold by BMI Category\")"},{"path":"NYFS-Study.html","id":"using-multiple-density-plots-to-make-comparisons","chapter":"9 National Youth Fitness Survey","heading":"9.19 Using Multiple Density Plots to Make Comparisons","text":", can make series density plots describe multiple groups data., can plot densities top semi-transparent fills.really works better comparing two groups, like females males.","code":"\nnnyfs %>% \n    filter(complete.cases(triceps_skinfold, bmi_cat)) %>%\n    ggplot(., aes(x=triceps_skinfold, fill = bmi_cat)) +\n    geom_density(color = \"black\") + \n    facet_wrap(~ bmi_cat) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Triceps Skinfold by BMI Category\")\nnnyfs %>% \n    filter(complete.cases(triceps_skinfold, bmi_cat)) %>%\n    ggplot(., aes(x=triceps_skinfold, fill = bmi_cat)) +\n    geom_density(alpha=0.3) + \n    scale_fill_viridis_d() + \n    labs(title = \"Triceps Skinfold by BMI Category\")\nnnyfs %>% \n    filter(complete.cases(triceps_skinfold, sex)) %>%\n    ggplot(., aes(x=triceps_skinfold, fill = sex)) +\n    geom_density(alpha=0.5) + \n    labs(title = \"Triceps Skinfold by Sex\")"},{"path":"NYFS-Study.html","id":"a-ridgeline-plot","chapter":"9 National Youth Fitness Survey","heading":"9.20 A Ridgeline Plot","text":"people don’t like violin plots - example, see https://simplystatistics.org/2017/07/13/-joy----violin-plots/. alternative plot available part ggridges package. shows distribution several groups simultaneously, especially lots subgroup categories, called ridgeline plot.’s ridgeline plot triceps skinfolds. ’ll start sorting subgroups median value outcome (triceps skinfold) case, though turns matter. ’ll also add color.one last example, ’ll look age BMI category, sorting BMI subgroups median matters, ’ll try alternate color scheme, theme specially designed ridgeline plot.","code":"\nnnyfs %>% \n    filter(complete.cases(waist, bmi_cat)) %>%\n    ggplot(., aes(x = waist, y = bmi_cat, height = ..density..)) +\n    ggridges::geom_density_ridges(scale = 0.85) + \n    theme_light() +\n    labs(title = \"Ridgeline Plot of Waist Circumference by BMI category (nnyfs)\",\n         x = \"Waist Circumference\", y = \"BMI Category\")Picking joint bandwidth of 3.47\nnnyfs %>%\n    filter(complete.cases(bmi_cat, triceps_skinfold)) %>%\n    mutate(bmi_cat = fct_reorder(bmi_cat,\n                                 triceps_skinfold, \n                                 .fun = median)) %>%\n    ggplot(., aes(x = triceps_skinfold, y = bmi_cat, \n                  fill = bmi_cat, height = ..density..)) +\n    ggridges::geom_density_ridges(scale = 0.85) + \n    scale_fill_viridis_d(option = \"magma\") +\n    guides(fill = \"none\") +\n    labs(title = \"Ridgeline Plot of Triceps Skinfold by BMI Category (nnyfs)\",\n         x = \"Triceps Skinfold\", y = \"BMI Category\") +\n    theme_light()Picking joint bandwidth of 1.37\nnnyfs %>%\n    filter(complete.cases(bmi_cat, age_child)) %>%\n    mutate(bmi_cat = reorder(bmi_cat, age_child, median)) %>%\n    ggplot(aes(x = age_child, y = bmi_cat, fill = bmi_cat, height = ..density..)) +\n    ggridges::geom_density_ridges(scale = 0.85) + \n    scale_fill_brewer(palette = \"YlOrRd\") +\n    guides(fill = \"none\") +\n    labs(title = \"Ridgeline Plot of Age at Exam by BMI category (nnyfs)\",\n         x = \"Age of Child at Exam\", y = \"BMI Category\") +\n    ggridges::theme_ridges()Picking joint bandwidth of 1.15"},{"path":"NYFS-Study.html","id":"what-summaries-to-report","chapter":"9 National Youth Fitness Survey","heading":"9.21 What Summaries to Report","text":"usually helpful focus shape, center spread distribution. Bock, Velleman DeVeaux provide useful advice:data skewed, report median IQR (three middle quantiles). may want include mean standard deviation, point mean median differ. fact mean median agree sign distribution may skewed. histogram help make point.data symmetric, report mean standard deviation, possibly median IQR well.clear outliers reporting mean standard deviation, report outliers present outliers removed. differences may revealing. median IQR likely seriously affected outliers.","code":""},{"path":"assessing-normality.html","id":"assessing-normality","chapter":"10 Assessing Normality","heading":"10 Assessing Normality","text":"Data well approximated Normal distribution shape data’s distribution good match Normal distribution mean standard deviation equal sample statistics.data symmetrically distributed single peak, located sample meanthe spread distribution well characterized Normal distribution standard deviation equal sample standard deviationthe data show outlying values (number candidate outliers, size distance outliers center distribution) similar predicted Normal model.several tools assessing Normality single batch data, including:histogram superimposed Normal distributionhistogram variants (like boxplot) provide information center, spread shape distributionthe Empirical Rule interpretation standard deviationa specialized normal Q-Q plot (also called normal probability plot normal quantile-quantile plot) designed reveal differences sample distribution might expect normal distribution similar number values mean standard deviation","code":""},{"path":"assessing-normality.html","id":"empirical-rule-interpretation-of-the-standard-deviation","chapter":"10 Assessing Normality","heading":"10.1 Empirical Rule Interpretation of the Standard Deviation","text":"set measurements follows Normal distribution, interval:Mean \\(\\pm\\) Standard Deviation contains approximately 68% measurements;Mean \\(\\pm\\) 2(Standard Deviation) contains approximately 95% measurements;Mean \\(\\pm\\) 3(Standard Deviation) contains approximately (99.7%) measurements., data sets follow Normal distribution. occasionally think transforming re-expressing data obtain results better approximated Normal distribution, part standard deviation can meaningful.energy data studying, summary statistics…mean 1877 standard deviation 722, data really Normally distributed, ’d expect see:68% data range (1155, 2600). fact, 1085 1518 energy values range, 71.5%.95% data range (432, 3322). fact, 1450 1518 energy values range, 95.5%.99.7% data range (-290, 4044). fact, 1502 1518 energy values range, 98.9%., based Empirical Rule approximation, energy data seem well approximated Normal distribution?","code":"\nnnyfs <- read_rds(\"data/nnyfs.Rds\")\nmosaic::favstats(nnyfs$energy) min     Q1 median   Q3  max     mean       sd    n missing\n 257 1367.5 1794.5 2306 5265 1877.157 722.3537 1518       0"},{"path":"assessing-normality.html","id":"describing-outlying-values-with-z-scores","chapter":"10 Assessing Normality","heading":"10.2 Describing Outlying Values with Z Scores","text":"maximum energy consumption value 5265. One way gauge extreme (much outlier ) uses observation’s Z score, number standard deviations away mean observation falls., maximum value, 5265 4.69 standard deviations mean, thus Z score 4.7.negative Z score indicate point mean, positive Z score indicates, ’ve seen, point mean. minimum body-mass index, 257 2.24 standard deviations mean, Z score -2.2.Recall Empirical Rule suggests variable follows Normal distribution, approximately 95% observations falling inside Z score (-2, 2), 99.74% falling inside Z score range (-3, 3).","code":""},{"path":"assessing-normality.html","id":"fences-and-z-scores","chapter":"10 Assessing Normality","heading":"10.2.1 Fences and Z Scores","text":"Note relationship fences (Tukey’s approach identifying points fall within whiskers boxplot, compared candidate outliers) Z scores.upper inner fence case falls 3713.75, indicates Z score 2.5, lower inner fence falls -40.25, indicates Z score -2.7. neither unusual inevitable inner fences fall Z scores near -2.0 +2.0.","code":""},{"path":"assessing-normality.html","id":"comparing-a-histogram-to-a-normal-distribution","chapter":"10 Assessing Normality","heading":"10.3 Comparing a Histogram to a Normal Distribution","text":"time, want understand whether data well approximated Normal distribution, use graph aid decision.One option build histogram Normal density function (mean standard deviation data) superimposed. one way help visualize deviations data might expected Normal distribution.seem though Normal model (shown blue density curve) effective approximation observed distribution shown bars histogram?’ll return shortly questions:Normal distribution model fit data well? andIf data aren’t Normal, want use Normal model anyway, ?","code":"\nres <- mosaic::favstats(~ energy, data = nnyfs)\nbin_w <- 50 # specify binwidth\n\nggplot(nnyfs, aes(x=energy)) +\n    geom_histogram(aes(y = ..density..), binwidth = bin_w, \n                   fill = \"papayawhip\", color = \"seagreen\") +\n    stat_function(fun = dnorm, \n                  args = list(mean = res$mean, sd = res$sd), \n                  lwd = 1.5, col = \"blue\") +\n    geom_text(aes(label = paste(\"Mean\", round(res$mean,1), \n                                \", SD\", round(res$sd,1))),\n              x = 4000, y = 0.0006, \n              color=\"blue\", fontface = \"italic\") + \n    labs(title = \"nnyfs energy values with Normal Density Superimposed\", \n         x = \"Energy (kcal)\", y = \"Probability Density Function\")"},{"path":"assessing-normality.html","id":"histogram-of-energy-with-normal-model-with-counts","chapter":"10 Assessing Normality","heading":"10.3.1 Histogram of energy with Normal model (with Counts)","text":"first, ’ll demonstrate approach building histogram counts (rather probability density) superimposing Normal model.","code":"\nres <- mosaic::favstats(~ energy, data = nnyfs)\nbin_w <- 50 # specify binwidth\n\nggplot(nnyfs, aes(x = energy)) +\n  geom_histogram(binwidth = bin_w, \n                 fill = \"papayawhip\", \n                 col = \"navy\") +\n  theme_bw() +\n  stat_function(\n    fun = function(x) dnorm(x, mean = res$mean, \n                            sd = res$sd) * res$n * bin_w,\n    col = \"blue\", size = 2) +\n    geom_text(aes(label = paste(\"Mean\", round(res$mean,1), \n                                \", SD\", round(res$sd,1))),\n              x = 4000, y = 50, \n              color=\"blue\", fontface = \"italic\") + \n    labs(title = \"Histogram of energy, with Normal Model\", \n         x = \"Energy consumed (kcal)\", y = \"# of subjects\")"},{"path":"assessing-normality.html","id":"does-a-normal-model-work-well-for-the-waist-circumference","chapter":"10 Assessing Normality","heading":"10.4 Does a Normal model work well for the waist circumference?","text":"Now, suppose instead look waist data, remembering filter data complete cases plotting. data appear follow Normal distribution?mean 67.71 standard deviation 15.2 waist data really Normally distributed, ’d expect see:68% data range (52.51, 82.9). fact, 1076 1512 Age values range, 71.2%.68% data range (52.51, 82.9). fact, 1076 1512 Age values range, 71.2%.95% data range (37.31, 98.1). fact, 1443 1512 Age values range, 95.4%.95% data range (37.31, 98.1). fact, 1443 1512 Age values range, 95.4%.99.7% data range (22.11, 113.3). fact, 1500 1512 Age values range, 99.2%.99.7% data range (22.11, 113.3). fact, 1500 1512 Age values range, 99.2%.Normal approximation work waist circumference, according Empirical Rule?","code":"\nres <- mosaic::favstats(~ waist, data = nnyfs)\nbin_w <- 5 # specify binwidth\n\nnnyfs %>% filter(complete.cases(waist)) %>%\n    ggplot(., aes(x = waist)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"antiquewhite\", \n                   col = \"navy\") +\n    theme_bw() +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", size = 2) +\n    geom_text(aes(label = paste(\"Mean\", round(res$mean,1), \n                                \", SD\", round(res$sd,1))),\n              x = 100, y = 200, \n              color=\"darkred\", fontface = \"italic\") + \n    labs(title = \"Histogram of waist, with Normal Model\", \n         x = \"Waist Circumference (cm)\", y = \"# of subjects\")\nmosaic::favstats(~ waist, data = nnyfs)  min   Q1 median   Q3   max     mean       sd    n missing\n 42.5 55.6   64.8 76.6 144.7 67.70536 15.19809 1512       6"},{"path":"assessing-normality.html","id":"the-normal-q-q-plot","chapter":"10 Assessing Normality","heading":"10.5 The Normal Q-Q Plot","text":"normal probability plot (normal quantile-quantile plot) energy results nnyfs data, developed using ggplot2 shown . case, picture 1518 energy consumption assessments. idea normal Q-Q plot plots observed sample values (vertical axis) , horizontal, expected theoretical quantiles observed standard normal distribution (Normal distribution mean 0 standard deviation 1) number observations.Normal Q-Q plot follow straight line data (approximately) Normally distributed. data different shape, plot reflect .","code":"\nggplot(nnyfs, aes(sample = energy)) +\n    geom_qq() + geom_qq_line(col = \"red\") +\n    theme_light() +\n    labs(title = \"Normal Q-Q plot for energy data\")"},{"path":"assessing-normality.html","id":"interpreting-the-normal-q-q-plot","chapter":"10 Assessing Normality","heading":"10.6 Interpreting the Normal Q-Q Plot","text":"purpose Normal Q-Q plot help point distinctions Normal distribution. Normal distribution symmetric certain expectations regarding tails. Normal Q-Q plot can help us identify data well approximated Normal distribution, , :skew (including distinguishing right skew left skew)behavior tails (heavy-tailed [outliers expected] light-tailed)","code":""},{"path":"assessing-normality.html","id":"data-from-a-normal-distribution-shows-up-as-a-straight-line-in-a-normal-q-q-plot","chapter":"10 Assessing Normality","heading":"10.6.1 Data from a Normal distribution shows up as a straight line in a Normal Q-Q plot","text":"’ll demonstrate looks can obtain Normal Q-Q plot simulations. First, example Normal Q-Q plot, associated histogram, sample 200 observations simulated Normal distribution.simulated data appear well-modeled Normal distribution, points Normal Q-Q plot follow diagonal reference line. particular,substantial curve (’d see data skewed)particularly surprising behavior (curves away line) either tail, ’s obvious problem outliers","code":"\nset.seed(123431) # so the results can be replicated\n                                          \n# simulate 200 observations from a Normal(20, 5) distribution and place them \n# in the d variable within the temp.1 data frame\ntemp.1 <- data.frame(d = rnorm(200, mean = 20, sd = 5)) \n                                          \n# left plot - basic Normal Q-Q plot of simulated data\np1 <- ggplot(temp.1, aes(sample = d)) +\n    geom_qq() + geom_qq_line(col = \"red\") +\n    theme_light() +\n    labs(y = \"Ordered Simulated Sample Data\")\n\n# right plot - histogram with superimposed normal distribution\nres <- mosaic::favstats(~ d, data = temp.1)\nbin_w <- 2 # specify binwidth\n\np2 <- ggplot(temp.1, aes(x = d)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"papayawhip\", \n                   col = \"seagreen\") +\n    theme_bw() +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"blue\", size = 1.5) +\n    geom_text(aes(label = paste(\"Mean\", round(res$mean,1), \n                                \", SD\", round(res$sd,1))),\n              x = 25, y = 35, \n              color=\"blue\", fontface = \"italic\") + \n    labs(x = \"Simulated Sample Data\", y = \"\")\n\np1 + p2 + \n  plot_annotation(title = \"200 observations from a simulated Normal distribution\") \n# uses patchwork package to combine plots"},{"path":"assessing-normality.html","id":"skew-is-indicated-by-monotonic-curves-in-the-normal-q-q-plot","chapter":"10 Assessing Normality","heading":"10.6.2 Skew is indicated by monotonic curves in the Normal Q-Q plot","text":"Data come skewed distribution appear curve away straight line Q-Q plot.Note bends away straight line sample. non-Normality may easier see histogram.","code":"\nset.seed(123431) # so the results can be replicated\n\n# simulate 200 observations from a beta(5, 2) distribution into the e1 variable\n# simulate 200 observations from a beta(1, 5) distribution into the e2 variable\ntemp.2 <- data.frame(e1 = rbeta(200, 5, 2), e2 = rbeta(200, 1, 5)) \n\np1 <- ggplot(temp.2, aes(sample = e1)) +\n    geom_qq(col = \"orchid\") + geom_qq_line(col = \"blue\") +\n    theme_light() +\n    labs(y = \"Ordered Sample e1\",\n         title = \"Beta(5, 2) sample: Left Skewed\")\n\np2 <- ggplot(temp.2, aes(sample = e2)) +\n    geom_qq(col = \"darkorange\") + geom_qq_line(col = \"blue\") +\n    theme_light() +\n    labs(y = \"Ordered Sample e2\",\n         title = \"Beta(1, 5) sample: Right Skewed\")\n\np1 + p2 + plot_annotation(title = \"200 observations from simulated Beta distributions\")\nres1 <- mosaic::favstats(~ e1, data = temp.2)\nbin_w1 <- 0.025 # specify binwidth\n\np1 <- ggplot(temp.2, aes(x = e1)) +\n    geom_histogram(binwidth = bin_w1, \n                   fill = \"orchid\", \n                   col = \"black\") +\n    theme_bw() +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res1$mean, \n                                sd = res1$sd) * \n            res1$n * bin_w1,\n        col = \"blue\", size = 1.5) +\nlabs(x = \"Sample e1\", y = \"\",\n     title = \"Beta(5,2) sample: Left Skew\")\n\nres2 <- mosaic::favstats(~ e2, data = temp.2)\nbin_w2 <- 0.025 # specify binwidth\n\np2 <- ggplot(temp.2, aes(x = e2)) +\n    geom_histogram(binwidth = bin_w2, \n                   fill = \"darkorange\", \n                   col = \"black\") +\n    theme_bw() +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res2$mean, \n                                sd = res2$sd) * \n            res2$n * bin_w2,\n        col = \"blue\", size = 1.5) +\nlabs(x = \"Sample e1\", y = \"\",\n     title = \"Beta(1,5) sample: Right Skew\")\n\np1 + p2 + plot_annotation(caption = \"Histograms with Normal curve superimposed\")"},{"path":"assessing-normality.html","id":"direction-of-skew","chapter":"10 Assessing Normality","heading":"10.6.3 Direction of Skew","text":"pairs plots, see basic result.left plot (data e1) shows left skew, longer tail left hand side clustered data right end distribution.right plot (data e2) shows right skew, longer tail right hand side, mean larger median, clustered data left end distribution.","code":""},{"path":"assessing-normality.html","id":"outlier-proneness-is-indicated-by-s-shaped-curves-in-a-normal-q-q-plot","chapter":"10 Assessing Normality","heading":"10.6.4 Outlier-proneness is indicated by “s-shaped” curves in a Normal Q-Q plot","text":"Heavy-tailed symmetric distributions indicated reverse “S”-shapes, shown left .Light-tailed symmetric distributions indicated “S” shapes plot, shown right ., can also visualize simulations histograms, although ’re less helpful understanding tail behavior skew.Instead, boxplots (augmented violin plots) can helpful thinking light-tailed vs. heavy-tailed distributions.","code":"\nset.seed(4311) # so the results can be replicated\n\n# sample 200 observations from each of two probability distributions\ntemp.3 <- data.frame(s1 = rcauchy(200, location=10, scale = 1),\n                     s2 = runif(200, -30, 30)) \n\np1 <- ggplot(temp.3, aes(sample = s1)) +\n    geom_qq(col = \"slateblue\") + geom_qq_line(col = \"black\") +\n    theme_light() +\n    labs(y = \"Ordered Sample s1\",\n         title = \"Heavy-Tailed Symmetric Sample s1\")\n\np2 <- ggplot(temp.3, aes(sample = s2)) +\n    geom_qq(col = \"dodgerblue\") + geom_qq_line(col = \"black\") +\n    theme_light() +\n    labs(y = \"Ordered Sample s2\",\n         title = \"Light-Tailed Symmetric Sample s2\")\n\np1 + p2 + plot_annotation(title = \"200 observations from simulated distributions\")\nres1 <- mosaic::favstats(~ s1, data = temp.3)\nbin_w1 <- 20 # specify binwidth\n\np1 <- ggplot(temp.3, aes(x = s1)) +\n    geom_histogram(binwidth = bin_w1, \n                   fill = \"slateblue\", \n                   col = \"white\") +\n    theme_bw() +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res1$mean, \n                                sd = res1$sd) * \n            res1$n * bin_w1,\n        col = \"blue\") +\nlabs(x = \"Sample s1\", y = \"\",\n     title = \"Cauchy sample: Heavy Tails\")\n\nres2 <- mosaic::favstats(~ s2, data = temp.3)\nbin_w2 <- 2 # specify binwidth\n\np2 <- ggplot(temp.3, aes(x = s2)) +\n    geom_histogram(binwidth = bin_w2, \n                   fill = \"dodgerblue\", \n                   col = \"white\") +\n    theme_bw() +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res2$mean, \n                                sd = res2$sd) * \n            res2$n * bin_w2,\n        col = \"blue\") +\nlabs(x = \"Sample s2\", y = \"\",\n     title = \"Uniform sample: Light Tails\")\n\np1 + p2 + plot_annotation(caption = \"Histograms with Normal curve superimposed\")\np1 <- ggplot(temp.3, aes(x = \"s1\", y = s1)) +\n    geom_violin(col = \"slateblue\") + \n    geom_boxplot(fill = \"slateblue\", width = 0.2) +\n    theme_light() +\n    coord_flip() +\n    labs(y = \"Ordered Sample s1\", x = \"\",\n         title = \"Heavy-Tailed Symmetric Sample s1\")\n\np2 <- ggplot(temp.3, aes(x = \"s2\", y = s2)) +\n    geom_violin(col = \"dodgerblue\") + \n    geom_boxplot(fill = \"dodgerblue\", width = 0.2) +\n    theme_light() +\n    coord_flip() +\n    labs(y = \"Ordered Sample s2\", x = \"\",\n         title = \"Light-Tailed Symmetric Sample s2\")\n\np1 + p2 + plot_annotation(title = \"200 observations from simulated distributions\")\nrm(temp.1, temp.2, temp.3, p1, p2, res, res1, res2, bin_w, bin_w1, bin_w2) # cleaning up"},{"path":"assessing-normality.html","id":"can-a-normal-distribution-fit-the-nnyfs-energy-data-well","chapter":"10 Assessing Normality","heading":"10.7 Can a Normal Distribution Fit the nnyfs energy data Well?","text":"energy data ’ve studying shows meaningful signs right skew.Skewness indicated curve Normal Q-Q plot. Curving away line tails suggests right skew, histogram.plotted original energy values (positive) instead plotted square roots energy values?Compare two plots - left describes distribution original energy data NNYFS data frame, right plot shows distribution square root values.left plot shows substantial right positive skewThe right plot shows ’s much less skew square root taken.conclusion Normal model far better fit square root energy values raw energy values.effect taking square root may clearer histograms , Normal models superimposed.confronted variable Normally distributed wish Normally distributed, sometimes useful consider whether working transformation data yield helpful result, square root instance.rest Chapter provides guidance choosing class power transformations can reduce impact non-Normality unimodal data.confronted variable Normally distributed wish Normally distributed, sometimes useful consider whether working transformation data yield helpful result.Many statistical methods, including t tests analyses variance, assume Normal distributions.’ll discuss using R assess range called Box-Cox power transformations, via plots, mainly.","code":"\np1 <- ggplot(nnyfs, aes(sample = energy)) +\n    geom_qq(col = \"coral\", size = 2) + \n    geom_qq_line(col = \"blue\") +\n    theme_light() +\n    labs(title = \"Energy Consumed\",\n         y = \"Sorted Energy data\")\n\nres <- mosaic::favstats(~ energy, data = nnyfs)\nbin_w <- 250 # specify binwidth\n\np2 <- ggplot(nnyfs, aes(x = energy)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"coral\", \n                   col = \"white\") +\n    theme_bw() +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"blue\", size = 1.5) +\nlabs(x = \"Energy (kcal consumed)\", y = \"\",\n     title = \"Energy Consumed\")\n\n\n\np1 + p2\np1 <- ggplot(nnyfs, aes(sample = energy)) +\n    geom_qq(col = \"coral\", size = 2) + \n    geom_qq_line(col = \"blue\") +\n    theme_light() +\n    labs(title = \"Energy Consumed\",\n         y = \"Sorted Energy data\")\n\np2 <- ggplot(nnyfs, aes(sample = sqrt(energy))) +\n    geom_qq(col = \"darkcyan\", size = 2) + \n    geom_qq_line(col = \"blue\") +\n    theme_light() +\n    labs(title = \"Square Root of Energy\",\n         y = \"Sorted Square Root of Energy\")\n\np1 + p2\nres <- mosaic::favstats(~ energy, data = nnyfs)\nbin_w <- 250 # specify binwidth\n\np1 <- ggplot(nnyfs, aes(x = energy)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"coral\", \n                   col = \"white\") +\n    theme_bw() +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"black\", size = 1.5) +\nlabs(x = \"Energy (kcal consumed)\", y = \"\",\n     title = \"Energy Consumed\")\n\n\nres2 <- mosaic::favstats(~ sqrt(energy), data = nnyfs)\nbin_w2 <- 5 # specify binwidth\n\np2 <- ggplot(nnyfs, aes(x = sqrt(energy))) +\n    geom_histogram(binwidth = bin_w2, \n                   fill = \"darkcyan\", \n                   col = \"white\") +\n    theme_bw() +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res2$mean, \n                                sd = res2$sd) * \n            res2$n * bin_w2,\n        col = \"black\", size = 1.5) +\nlabs(x = \"Square Root of Energy\", y = \"\",\n     title = \"Square Root of Energy\")\n\n\np1 + p2 + plot_annotation(title = \"Comparing energy to sqrt(energy)\")\nrm(p1, p2, bin_w, bin_w2, res, res2) # cleanup"},{"path":"assessing-normality.html","id":"the-ladder-of-power-transformations","chapter":"10 Assessing Normality","heading":"10.8 The Ladder of Power Transformations","text":"key notion re-expression single variable obtain distribution better approximated Normal re-expression outcome simple regression model ladder power transformations, applies unimodal data.","code":""},{"path":"assessing-normality.html","id":"using-the-ladder","chapter":"10 Assessing Normality","heading":"10.9 Using the Ladder","text":"move away identity function (power = 1) change shape general direction.instance, try logarithm, seems like much change, might try square root instead.Note ladder (like many things due John Tukey) uses logarithm “power zero” transformation rather constant, x0 actually .variable x can take negative values, might take different approach. x count something zero, often simply add 1 x transformation.ladder power transformations particularly helpful confronted data shows skew.handle right skew (mean exceeds median) usually apply powers 1.handle left skew (median exceeds mean) usually apply powers greater 1.common transformations square (power 2), square root (power 1/2), logarithm (power 0) inverse (power -1), usually restrict options practical work.","code":""},{"path":"assessing-normality.html","id":"protein-consumption-in-the-nnyfs-data","chapter":"10 Assessing Normality","heading":"10.10 Protein Consumption in the NNYFS data","text":"protein consumption (grams) results NNYFS data.key point see several signs meaningful right skew, ’ll want consider transformation might make Normal model plausible.","code":"\nmosaic::favstats(~ protein, data = nnyfs)  min    Q1 median     Q3    max     mean       sd    n\n 4.18 45.33 61.255 82.565 241.84 66.90148 30.96319 1518\n missing\n       0\np1 <- ggplot(nnyfs, aes(x = \"Protein\", y = protein)) +\n    geom_violin() +\n    geom_boxplot(width = 0.2, fill = \"salmon\", \n                 outlier.color = \"red\") +\n    \n    labs(title = \"NNYFS Protein consumption\",\n         x = \"\", y = \"Protein Consumption (g)\")\n\np2 <- ggplot(nnyfs, aes(sample = protein)) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    \n    labs(title = \"NNYFS Protein Consumption\",\n         y = \"Protein Consumption (g)\")\n\np1 + p2"},{"path":"assessing-normality.html","id":"using-patchwork-to-compose-plots","chapter":"10 Assessing Normality","heading":"10.10.1 Using patchwork to compose plots","text":"mentioned previously, feel slickest approach composing series plots placed together available patchwork package. ’s another example., patchwork package repository https://patchwork.data-imaginist.com/index.html lots nice examples work .","code":"\nres <- mosaic::favstats(~ protein, data = nnyfs)\nbin_w <- 5 # specify binwidth\n\np1 <- ggplot(nnyfs, aes(x = protein)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"salmon\", \n                   col = \"white\") +\n    \n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", size = 2) +\n    labs(title = \"Histogram with Normal fit\", \n         x = \"Protein Consumption (g)\", y = \"# of subjects\")\n\n\np2 <- ggplot(nnyfs, aes(sample = protein)) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    \n    labs(title = \"Normal Q-Q plot\",\n         y = \"Protein Consumption (g)\")\n\np3 <- ggplot(nnyfs, aes(x = \"\", y = protein)) +\n    geom_violin() +\n    geom_boxplot(width = 0.2, fill = \"salmon\", \n                 outlier.color = \"red\") +\n    \n    coord_flip() +\n    labs(title = \"Boxplot with Violin\",\n         x = \"\", y = \"Protein Consumption (g)\")\n\np1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) +\n    plot_annotation(title = \"NNYFS Protein Consumption\")"},{"path":"assessing-normality.html","id":"can-we-transform-the-protein-data","chapter":"10 Assessing Normality","heading":"10.11 Can we transform the protein data?","text":"’ve seen, protein data right skewed, values strictly positive. want use tools Normal distribution describe data, might try taking step “” ladder power 1 (raw data) lower powers.","code":""},{"path":"assessing-normality.html","id":"the-square-root","chapter":"10 Assessing Normality","heading":"10.11.1 The Square Root","text":"square root applied protein data help alleviate right skew?looks like symmetric distribution, certainly, although still outliers right side distribution. take another step ladder?","code":"\nres <- mosaic::favstats(~ sqrt(protein), data = nnyfs)\nbin_w <- 1 # specify binwidth\n\np1 <- ggplot(nnyfs, aes(x = sqrt(protein))) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"salmon\", \n                   col = \"white\") +\n    \n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", size = 2) +\n    labs(title = \"Histogram with Normal fit\", \n         x = \"Square Root of Protein Consumption (g)\", y = \"# of subjects\")\n\n\np2 <- ggplot(nnyfs, aes(sample = sqrt(protein))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    \n    labs(title = \"Normal Q-Q plot\",\n         y = \"Square Root of Protein Consumption (g)\")\n\np3 <- ggplot(nnyfs, aes(x = \"\", y = sqrt(protein))) +\n    geom_violin() +\n    geom_boxplot(width = 0.2, fill = \"salmon\", \n                 outlier.color = \"red\") +\n    \n    coord_flip() +\n    labs(title = \"Boxplot with Violin\",\n         x = \"\", y = \"Square Root of Protein Consumption (g)\")\n\np1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) +\n    plot_annotation(title = \"Square Root of NNYFS Protein Consumption\")"},{"path":"assessing-normality.html","id":"the-logarithm","chapter":"10 Assessing Normality","heading":"10.11.2 The Logarithm","text":"might also try logarithm energy circumference data. can use either natural logarithm (log, R) base-10 logarithm (log10, R) - either impact skew.Now, looks like may gone far direction. looks like square root sensible choice try improve fit Normal model protein consumption data.","code":"\nres <- mosaic::favstats(~ log(protein), data = nnyfs)\nbin_w <- 0.5 # specify binwidth\n\np1 <- ggplot(nnyfs, aes(x = log(protein))) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"salmon\", \n                   col = \"white\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", size = 2) +\n    labs(title = \"Histogram with Normal fit\", \n         x = \"Log of Protein Consumption (g)\", y = \"# of subjects\")\n\n\np2 <- ggplot(nnyfs, aes(sample = log(protein))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Normal Q-Q plot\",\n         y = \"Log of Protein Consumption (g)\")\n\np3 <- ggplot(nnyfs, aes(x = \"\", y = log(protein))) +\n    geom_violin() +\n    geom_boxplot(width = 0.2, fill = \"salmon\", \n                 outlier.color = \"red\") +\n    coord_flip() +\n    labs(title = \"Boxplot with Violin\",\n         x = \"\", y = \"Log of Protein Consumption (g)\")\n\np1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) +\n    plot_annotation(title = \"Logarithm of NNYFS Protein Consumption\")"},{"path":"assessing-normality.html","id":"this-course-uses-natural-logarithms-unless-otherwise-specified","chapter":"10 Assessing Normality","heading":"10.11.3 This course uses Natural Logarithms, unless otherwise specified","text":"course, assume use natural logarithms unless specify otherwise. Following R’s convention, use log natural logarithms.","code":""},{"path":"assessing-normality.html","id":"what-if-we-considered-all-9-available-transformations","chapter":"10 Assessing Normality","heading":"10.12 What if we considered all 9 available transformations?","text":"square root still appears best choice transformation , even consider 8 transformation raw data.","code":"\np1 <- ggplot(nnyfs, aes(sample = protein^3)) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Cube (power 3)\",\n         y = \"Protein, Cubed\")\n\np2 <- ggplot(nnyfs, aes(sample = protein^2)) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Square (power 2)\",\n         y = \"Protein, Squared\")\n\np3 <- ggplot(nnyfs, aes(sample = protein)) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Original Data\",\n         y = \"Protein (g)\")\n\n\np4 <- ggplot(nnyfs, aes(sample = sqrt(protein))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"sqrt (power 0.5)\",\n         y = \"Square Root of Protein\")\n\np5 <- ggplot(nnyfs, aes(sample = log(protein))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"log (power 0)\",\n         y = \"Natural Log of Protein\")\n\np6 <- ggplot(nnyfs, aes(sample = protein^(-0.5))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"1/sqrt (power -0.5)\",\n         y = \"1/Square Root(Protein)\")\n\n\np7 <- ggplot(nnyfs, aes(sample = 1/protein)) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Inverse (power -1)\",\n         y = \"1/Protein\")\n\np8 <- ggplot(nnyfs, aes(sample = 1/(protein^2))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"1/Square (power -2)\",\n         y = \"1 /(Protein squared)\")\n\np9 <- ggplot(nnyfs, aes(sample = 1/(protein^3))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"1/Cube (power -3)\",\n         y = \"1/(Protein cubed)\")\n\n\np1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 +\n    plot_layout(nrow = 3) +\n    plot_annotation(title = \"Transformations of NNYFS Protein Consumption\")"},{"path":"assessing-normality.html","id":"a-simulated-data-set","chapter":"10 Assessing Normality","heading":"10.13 A Simulated Data Set","text":"’d like transform data better approximate Normal distribution, start? transformation suggest?Given left skew data, looks like step ladder warranted, perhaps looking square data?Looks like best modest improvement. cubing data, instead?newly transformed (cube ) data appears symmetric, although somewhat light-tailed. Perhaps Normal model appropriate now, although standard deviation likely overstate variation see data due light tails. , wouldn’t thrilled using cube practical work, hard interpret, look like reasonable choice .","code":"\nset.seed(431); \ndata2 <- \n    data_frame(sample2 = 100*rbeta(n = 125, shape1 = 5, shape2 = 2))Warning: `data_frame()` was deprecated in tibble 1.1.0.\nPlease use `tibble()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\nres <- mosaic::favstats(~ sample2, data = data2)\nbin_w <- 4 # specify binwidth\n\np1 <- ggplot(data2, aes(x = sample2)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"royalblue\", \n                   col = \"white\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", size = 2) +\n    labs(title = \"Histogram with Normal fit\", \n         x = \"Simulated Data\", y = \"# of subjects\")\n\n\np2 <- ggplot(data2, aes(sample = sample2)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Normal Q-Q plot\",\n         y = \"Simulated Data\")\n\np3 <- ggplot(data2, aes(x = \"\", y = sample2)) +\n    geom_violin() +\n    geom_boxplot(width = 0.3, fill = \"royalblue\", \n                 outlier.color = \"royalblue\") +\n    coord_flip() +\n    labs(title = \"Boxplot with Violin\",\n         x = \"\", y = \"Simulated Data\")\n\np1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) +\n    plot_annotation(title = \"Simulated Data\")\nres <- mosaic::favstats(~ sample2^2, data = data2)\nbin_w <- 600 # specify binwidth\n\np1 <- ggplot(data2, aes(x = sample2^2)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"royalblue\", \n                   col = \"white\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", size = 2) +\n    labs(title = \"Histogram with Normal fit\", \n         x = \"Squared Simulated Data\", y = \"# of subjects\")\n\n\np2 <- ggplot(data2, aes(sample = sample2^2)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Normal Q-Q plot\",\n         y = \"Squared Simulated Data\")\n\np3 <- ggplot(data2, aes(x = \"\", y = sample2^2)) +\n    geom_violin() +\n    geom_boxplot(width = 0.3, fill = \"royalblue\", \n                 outlier.color = \"royalblue\") +\n    coord_flip() +\n    labs(title = \"Boxplot with Violin\",\n         x = \"\", y = \"Squared Simulated Data\")\n\np1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) +\n    plot_annotation(title = \"Squared Simulated Data\")\nres <- mosaic::favstats(~ sample2^3, data = data2)\nbin_w <- 100000 # specify binwidth\n\np1 <- ggplot(data2, aes(x = sample2^3)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"royalblue\", \n                   col = \"white\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", size = 2) +\n    labs(title = \"Histogram with Normal fit\", \n         x = \"Cubed Simulated Data\", y = \"# of subjects\")\n\n\np2 <- ggplot(data2, aes(sample = sample2^3)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Normal Q-Q plot\",\n         y = \"Cubed Simulated Data\")\n\np3 <- ggplot(data2, aes(x = \"\", y = sample2^3)) +\n    geom_violin() +\n    geom_boxplot(width = 0.3, fill = \"royalblue\", \n                 outlier.color = \"royalblue\") +\n    coord_flip() +\n    labs(title = \"Boxplot with Violin\",\n         x = \"\", y = \"Cubed Simulated Data\")\n\np1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) +\n    plot_annotation(title = \"Cubed Simulated Data\")"},{"path":"assessing-normality.html","id":"what-if-we-considered-all-9-available-transformations-1","chapter":"10 Assessing Normality","heading":"10.14 What if we considered all 9 available transformations?","text":", either cube square looks like best choice , terms creating symmetric (albeit light-tailed) distribution.","code":"\np1 <- ggplot(data2, aes(sample = sample2^3)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Cube (power 3)\")\n\np2 <- ggplot(data2, aes(sample = sample2^2)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Square (power 2)\")\n\np3 <- ggplot(data2, aes(sample = sample2)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Original Data\")\n\np4 <- ggplot(data2, aes(sample = sqrt(sample2))) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"sqrt (power 0.5)\")\n\np5 <- ggplot(data2, aes(sample = log(sample2))) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"log (power 0)\")\n\np6 <- ggplot(data2, aes(sample = sample2^(0.5))) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"1/sqrt (power -0.5)\")\n\np7 <- ggplot(data2, aes(sample = 1/sample2)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Inverse (power -1)\")\n\np8 <- ggplot(data2, aes(sample = 1/(sample2^2))) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"1/Square (power -2)\")\n\np9 <- ggplot(data2, aes(sample = 1/(sample2^3))) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"1/Cube (power -3)\")\n\np1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 +\n    plot_layout(nrow = 3) +\n    plot_annotation(title = \"Transformations of Simulated Sample\")"},{"path":"straight-line-models.html","id":"straight-line-models","chapter":"11 Straight Line Models","heading":"11 Straight Line Models","text":"","code":""},{"path":"straight-line-models.html","id":"assessing-a-scatterplot","chapter":"11 Straight Line Models","heading":"11.1 Assessing A Scatterplot","text":"Let’s consider relationship protein fat consumption children nnyfs data.’ll begin investigation, always , drawing relevant picture. association two quantitative variables, scatterplot usually right start. subject nnyfs data represented one points . plot, ’ve also used geom_smooth add straight line regression model, ’ll discuss later., ’ve arbitrarily decided place fat vertical axis, protein horizontal. Fitting prediction model scatterplot require predict fat basis protein.case, pattern appears :direct, positive, values \\(x\\) variable (protein) increase, values \\(y\\) variable (fat). Essentially, appears subjects consumed protein also consumed fat, don’t know cause effect .fairly linear points cluster around appears pattern well-fitted straight line.moderately strong range values fat associated particular value protein fairly tight. know someone’s protein consumption, meaningfully improve ability predict fat consumption, among subjects data.see unusual outlier values, away general pattern subjects shown data.","code":"\nnnyfs <- read_rds(\"data/nnyfs.Rds\")\nggplot(data = nnyfs, aes(x = protein, y = fat)) +\n    geom_point(shape = 1, size = 2, col = \"sienna\") +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, col = \"steelblue\") +\n    labs(title = \"Protein vs. Fat consumption\",\n         subtitle = \"Children in the NNYFS data, with fitted straight line regression model\",\n         x = \"Protein (in g)\", y = \"Fat (in g)\")"},{"path":"straight-line-models.html","id":"highlighting-an-unusual-point","chapter":"11 Straight Line Models","heading":"11.1.1 Highlighting an unusual point","text":"Consider subject protein consumption close 200 g, whose fat consumption 100 g. ’s well prediction linear model example. can identify subject person protein > 190 fat < 100 BMI > 35 waist.circ < 70. ’ll create subset nnyfs data containing point meets standard, add red point label plot.subject hardly unusual point data set, one unusual ones, terms vertical distance regression line. can identify subject printing (part ) tibble created.Now, seem like straight line model describe protein-fat relationship well?","code":"\n# identify outlier and place it in data frame s1 \ns1 <- filter(nnyfs, protein > 190 & fat < 100)\n\nggplot(data = nnyfs, aes(x = protein, y = fat)) +\n    geom_point(shape = 1, size = 2, col = \"sienna\") +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x, col = \"steelblue\") +\n    geom_point(data = s1, size = 2, col = \"red\") +\n    geom_text(data = s1, label = s1$SEQN, \n              vjust = -1, col = \"red\") +\n    labs(title = \"Protein vs. Fat consumption in NNYFS\",\n         subtitle = \"with regression line, and an outlier labeled by SEQN\",\n         x = \"Protein (in g)\", y = \"Fat (in g)\")\ns1 %>% select(SEQN, sex, race_eth, age_child, protein, fat) %>% kable()"},{"path":"straight-line-models.html","id":"adding-a-scatterplot-smooth-using-loess","chapter":"11 Straight Line Models","heading":"11.1.2 Adding a Scatterplot Smooth using loess","text":"Next, ’ll use loess procedure fit smooth curve data, attempts capture general pattern.“loess” smooth curve fairly close straight line fit, indicating perhaps linear regression model might fit data well.loess smooth method fitting local polynomial regression model R uses default smooth scatterplots fewer 1000 observations. Think loess way fitting curve data tracking (point x) points within neighborhood point x, emphasis given points near x. can adjusted tweaking two specific parameters, particular:span parameter (defaults 0.75) also called \\(\\alpha\\) literature, controls degree smoothing (essentially, large neighborhood ), anda degree parameter (defaults 2) specifies degree polynomial used. Normally, either 1 2 - complex functions rarely needed simple scatterplot smoothing.addition curve, smoothing procedures can also provide confidence intervals around main fitted line. Consider following plot, adjusts span also adds confidence intervals.reducing size span, plot right shows somewhat less “smooth” function plot left.","code":"\nggplot(data = nnyfs, aes(x = protein, y = fat)) +\n    geom_point(shape = 1, size = 2, col = \"sienna\") +\n    geom_smooth(method = \"loess\", se = FALSE, formula = y ~ x, col = \"red\") +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x, col = \"steelblue\") +\n    labs(title = \"Protein vs. Fat consumption in NNYFS\",\n         subtitle = \"with loess smooth (red) and linear fit (blue)\",\n         x = \"Protein (in g)\", y = \"Fat (in g)\")\np1 <- ggplot(data = nnyfs, aes(x = protein, y = fat)) +\n    geom_point(shape = 1, size = 2, col = \"sienna\") +\n    geom_smooth(method = \"loess\", span = 0.75, se = TRUE, \n                col = \"red\", formula = y ~ x) +\n    labs(title = \"loess smooth (span = 0.75)\",\n         x = \"Protein (in g)\", y = \"Fat (in g)\")\n\np2 <- ggplot(data = nnyfs, aes(x = protein, y = fat)) +\n    geom_point(shape = 1, size = 2, col = \"sienna\") +\n    geom_smooth(method = \"loess\", span = 0.2, se = TRUE, \n                col = \"red\", formula = y ~ x) +\n    labs(title = \"loess smooth (span = 0.2)\",\n         x = \"Protein (in g)\", y = \"Fat (in g)\")\n\np1 + p2 + \n    plot_annotation(title = \"Impact of adjusting loess smooth span: NNYFS\")"},{"path":"straight-line-models.html","id":"what-line-does-r-fit","chapter":"11 Straight Line Models","heading":"11.1.3 What Line Does R Fit?","text":"Returning linear regression model, can , mathematically, characterize line? straight line, model equation requires us specify two parameters: slope intercept (sometimes called y-intercept.)identify equation R used fit line (using method least squares), use lm commandSo fitted line specified \\[\n\\mbox{fat} = 18.8945 + 0.7251 \\mbox{ protein }\n\\]detailed summary fitted linear regression model also available.way ’ll usually summarize estimated coefficients linear model use broom package’s tidy function put coefficient estimates tibble.can also summarize quality fit linear model using broom package’s glance function. now, ’ll focus attention just one many summaries available linear model glance: R-squared value.’ll spend lot time working regression summaries course.now, suffice understand following:outcome variable model fat, predictor variable protein.straight line model data fitted least squares fat = 18.9 + 0.725 proteinThe slope protein positive, indicates protein increases, expect fat also increase. Specifically, expect every additional gram protein consumed, fat consumption 0.725 gram larger.multiple R-squared (squared correlation coefficient) 0.445, implies 44.5% variation fat explained using linear model protein.also implies Pearson correlation fat protein square root 0.445, 0.667. Pearson correlation soon., plan use simple (least squares) linear regression model describe fat consumption function protein consumption NNYFS data, look like least squares (linear regression) model effective choice?","code":"\nlm(fat ~ protein, data = nnyfs)\nCall:\nlm(formula = fat ~ protein, data = nnyfs)\n\nCoefficients:\n(Intercept)      protein  \n    18.8945       0.7251  \nsummary(lm(fat ~ protein, data = nnyfs))\nCall:\nlm(formula = fat ~ protein, data = nnyfs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-77.798 -14.841  -2.449  13.601 110.597 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  18.8945     1.5330   12.32   <2e-16 ***\nprotein       0.7251     0.0208   34.87   <2e-16 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.08 on 1516 degrees of freedom\nMultiple R-squared:  0.4451,    Adjusted R-squared:  0.4447 \nF-statistic:  1216 on 1 and 1516 DF,  p-value: < 2.2e-16\ntidy(lm(fat ~ protein, data = nnyfs),\n            conf.int = TRUE, conf.level = 0.95) %>%\n    kable(digits = 3)\nglance(lm(fat ~ protein, data = nnyfs)) %>% select(r.squared) %>%\n    kable(digits = 3)"},{"path":"straight-line-models.html","id":"correlation-coefficients","chapter":"11 Straight Line Models","heading":"11.2 Correlation Coefficients","text":"Two different correlation measures worth immediate attention.one often used called Pearson correlation coefficient, symbolized letter r sometimes Greek letter rho (\\(\\rho\\)).Another tool Spearman rank correlation coefficient, also occasionally symbolized \\(\\rho\\).nnyfs data, Pearson correlation fat protein can found using cor() function.Note correlation variable 1, correlation fat protein regardless whether enter fat first protein first.","code":"\nnnyfs %$% cor(fat, protein)[1] 0.6671209"},{"path":"straight-line-models.html","id":"the-pearson-correlation-coefficient","chapter":"11 Straight Line Models","heading":"11.3 The Pearson Correlation Coefficient","text":"Suppose \\(n\\) observations two variables, called X Y. Pearson correlation coefficient assesses well relationship X Y can described using linear function.Pearson correlation dimension-free.falls -1 +1, extremes corresponding situations points scatterplot fall exactly straight line negative positive slopes, respectively.Pearson correlation zero corresponds situation linear association.Unlike estimated slope regression line, sample correlation coefficient symmetric X Y, depend labeling one (Y) response variable, one (X) predictor.Suppose \\(n\\) observations two variables, called \\(X\\) \\(Y\\), \\(\\bar{X}\\) sample mean \\(X\\) \\(s_x\\) standard deviation \\(X\\). Pearson correlation coefficient \\(r_{XY}\\) :\\[\nr_{XY} = \\frac{1}{n-1} \\sum\\limits_{=1}^n (\\frac{x_i - \\bar{x}}{s_x}) (\\frac{y_i - \\bar{y}}{s_y}) \n\\]","code":""},{"path":"straight-line-models.html","id":"studying-correlation-through-6-examples","chapter":"11 Straight Line Models","heading":"11.4 Studying Correlation through 6 Examples","text":"correx1 data file contains six different sets (x,y) points, identified set variable.","code":"\ncorrex1 <- read_csv(\"data/correx1.csv\") Rows: 277 Columns: 3-- Column specification ------------------------------------\nDelimiter: \",\"\nchr (1): set\ndbl (2): x, y\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\nsummary(correx1)     set                  x                y         \n Length:277         Min.   : 5.897   Min.   : 7.308  \n Class :character   1st Qu.:29.487   1st Qu.:30.385  \n Mode  :character   Median :46.154   Median :46.923  \n                    Mean   :46.529   Mean   :49.061  \n                    3rd Qu.:63.333   3rd Qu.:68.077  \n                    Max.   :98.205   Max.   :95.385  "},{"path":"straight-line-models.html","id":"data-set-alex","chapter":"11 Straight Line Models","heading":"11.4.1 Data Set Alex","text":"Let’s start working Alex data set.","code":"\nggplot(filter(correx1, set == \"Alex\"), aes(x = x, y = y)) + \n    geom_point() +\n    labs(title = \"correx1: Data Set Alex\")\nggplot(filter(correx1, set == \"Alex\"), aes(x = x, y = y)) + \n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, col = \"blue\") +\n    labs(title = \"correx1: Alex, with loess smooth\")\nsetA <- filter(correx1, set == \"Alex\")\n\nggplot(setA, aes(x = x, y = y)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    labs(title = \"correx1: Alex, with Fitted Linear Model\") +\n    annotate(\"text\", x = 75, y = 75, col = \"purple\", size = 6,\n             label = paste(\"Pearson r = \", signif(cor(setA$x, setA$y),3))) +\n    annotate(\"text\", x = 50, y = 15,  col = \"red\", size = 5,\n             label = paste(\"y = \", signif(coef(lm(setA$y ~ setA$x))[1],3), \n                           signif(coef(lm(setA$y ~ setA$x))[2],2), \"x\"))"},{"path":"straight-line-models.html","id":"data-set-bonnie","chapter":"11 Straight Line Models","heading":"11.4.2 Data Set Bonnie","text":"","code":"\nsetB <- dplyr::filter(correx1, set == \"Bonnie\")\n\nggplot(setB, aes(x = x, y = y)) + \n    geom_point() +\n    labs(title = \"correx1: Data Set Bonnie\")\nggplot(setB, aes(x = x, y = y)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    labs(title = \"correx1: Bonnie, with Fitted Linear Model\") +\n    annotate(\"text\", x = 25, y = 65, col = \"purple\", size = 6,\n             label = paste(\"Pearson r = \", signif(cor(setB$x, setB$y),2))) +\n    annotate(\"text\", x = 50, y = 15,  col = \"red\", size = 5,\n             label = paste(\"y = \", signif(coef(lm(setB$y ~ setB$x))[1],3), \n                           \" + \",\n                           signif(coef(lm(setB$y ~ setB$x))[2],2), \"x\"))"},{"path":"straight-line-models.html","id":"correlations-for-all-six-data-sets-in-the-correx1-example","chapter":"11 Straight Line Models","heading":"11.4.3 Correlations for All Six Data Sets in the Correx1 Example","text":"Let’s look Pearson correlations associated six data sets contained correx1 example.","code":"\ntab1 <- correx1 %>%\n    group_by(set) %>%\n    summarise(\"Pearson r\" = round(cor(x, y, use=\"complete\"),2))\n\nknitr::kable(tab1)"},{"path":"straight-line-models.html","id":"data-set-colin","chapter":"11 Straight Line Models","heading":"11.4.4 Data Set Colin","text":"looks like picture Colin similar (terms scatter) picture Bonnie, except Colin negative slope, rather positive one Bonnie . plays ?Uh, oh. looks like point Colin top right twisting otherwise straight regression model extremely strong negative correlation. ’s better way look outliers examine scatterplot.","code":"\nsetBC <- filter(correx1, set == \"Bonnie\" | set == \"Colin\")\n\nggplot(setBC, aes(x = x, y = y)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    facet_wrap(~ set)"},{"path":"straight-line-models.html","id":"draw-the-picture","chapter":"11 Straight Line Models","heading":"11.4.5 Draw the Picture!","text":"’ve seen Danielle, Earl Fiona show Pearson correlations essentially zero. However, three data sets look different scatterplot.learn correlation zero, tend assume picture like Danielle data set. Danielle real data, might well think x little use predicting y.data looked like Earl? Earl data set, x incredibly helpful predicting y, can’t use straight line model - instead, need non-linear modeling approach.’ll recall Fiona data set also Pearson correlation zero. , picture rather interesting., remember, draw appropriate scatterplot whenever make use correlation coefficient.","code":"\nggplot(correx1, aes(x = x, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x) +\n    facet_wrap(~ set)\nrm(setA, setB, setBC, tab1)"},{"path":"straight-line-models.html","id":"estimating-correlation-from-scatterplots","chapter":"11 Straight Line Models","heading":"11.5 Estimating Correlation from Scatterplots","text":"correx2 data set designed help calibrate bit terms estimating correlation scatterplot. 11 data sets buried within correx2 example, labeled Pearson correlation coefficients, ranging r = 0.01 r = 0.999Here plot 11 data sets, showing increase correlation 0.01 (Set 01) 0.999 (Set 999).Note R allow fit straight line model relationships, matter appropriate might .Note 10 points used data sets. ’s always possible lurking subgroup data within scatterplot follows strong linear relationship. ’s important (difficult) go searching thing without strong foundation logic, theory prior empirical evidence.","code":"\ncorrex2 <- read_csv(\"data/correx2.csv\")Rows: 582 Columns: 4-- Column specification ------------------------------------\nDelimiter: \",\"\nchr (1): set\ndbl (3): x, y, group\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\ncorrex2 %>%\n    group_by(set) %>%\n    summarise(cor = round(cor(x, y, use=\"complete\"),3))# A tibble: 11 x 2\n   set       cor\n   <chr>   <dbl>\n 1 Set 01  0.01 \n 2 Set 10  0.102\n 3 Set 20  0.202\n 4 Set 30  0.301\n 5 Set 40  0.403\n 6 Set 50  0.499\n 7 Set 60  0.603\n 8 Set 70  0.702\n 9 Set 80  0.799\n10 Set 90  0.902\n11 Set 999 0.999\nggplot(correx2, aes(x = x, y = y)) +\n    geom_point() + \n    facet_wrap(~ set) +\n    labs(title = \"Pearson Correlations from 0.01 to 0.999\")\nggplot(correx2, aes(x = x, y = y)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    facet_wrap(~ set) +\n    labs(title = \"R will fit a straight line to anything.\")\nggplot(correx2, aes(x = x, y = y)) +\n    geom_point() + \n    geom_smooth(col = \"blue\") +\n    facet_wrap(~ set) +\n    labs(title = \"Even if a loess smooth suggests non-linearity.\")\nggplot(correx2, aes(x = x, y = y, color = factor(group))) +\n    geom_point() + \n    guides(color = \"none\") +\n    facet_wrap(~ set) +\n    labs(title = \"Note: The same 10 points (in red) are in each plot.\")"},{"path":"straight-line-models.html","id":"the-spearman-rank-correlation","chapter":"11 Straight Line Models","heading":"11.6 The Spearman Rank Correlation","text":"Spearman rank correlation coefficient rank-based measure statistical dependence assesses well relationship X Y can described using monotone function even relationship linear.monotone function preserves order, , Y must either strictly increasing X increases, strictly decreasing X increases.Spearman correlation 1.0 indicates simply X increases, Y always increases.Like Pearson correlation, Spearman correlation dimension-free, falls -1 +1.positive Spearman correlation corresponds increasing (necessarily linear) association X Y, negative Spearman correlation corresponds decreasing (necessarily linear) association.","code":""},{"path":"straight-line-models.html","id":"spearman-formula","chapter":"11 Straight Line Models","heading":"11.6.1 Spearman Formula","text":"calculate Spearman rank correlation, take ranks X Y data, apply usual Pearson correlation. find ranks, sort X Y ascending order, number 1 (smallest) n (largest). event tie, assign average rank tied subjects.","code":""},{"path":"straight-line-models.html","id":"comparing-pearson-and-spearman-correlations","chapter":"11 Straight Line Models","heading":"11.6.2 Comparing Pearson and Spearman Correlations","text":"Let’s look nnyfs data .Spearman Pearson correlations especially different case.","code":"\nnnyfs %$% cor(fat, protein)[1] 0.6671209\nnnyfs %$% cor(fat, protein, method = \"spearman\")[1] 0.6577489"},{"path":"straight-line-models.html","id":"spearman-vs.-pearson-example-1","chapter":"11 Straight Line Models","heading":"11.6.3 Spearman vs. Pearson Example 1","text":"next plots describe relationships anticipate Pearson Spearman correlations might differ conclusions.Example 1 shows function Pearson correlation 0.925 (strong perfect linear relation), Spearman correlation 1 relationship monotone, even though perfectly linear., positive Spearman correlation corresponds increasing (necessarily linear) association x y.","code":"\nspear1 <- read_csv(\"data/spear1.csv\")Rows: 22 Columns: 2-- Column specification ------------------------------------\nDelimiter: \",\"\ndbl (2): x, y\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\nspear2 <- read_csv(\"data/spear2.csv\")Rows: 90 Columns: 2-- Column specification ------------------------------------\nDelimiter: \",\"\ndbl (2): x, y\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\nspear3 <- read_csv(\"data/spear3.csv\")Rows: 55 Columns: 2-- Column specification ------------------------------------\nDelimiter: \",\"\ndbl (2): x, y\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\nspear4 <- read_csv(\"data/spear4.csv\")Rows: 15 Columns: 2-- Column specification ------------------------------------\nDelimiter: \",\"\ndbl (2): x, y\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n# these are just toy examples with\n# two columns per data set and no row numbering\nggplot(spear1, aes(x = x, y = y)) + \n    geom_point() +\n    labs(title = \"Spearman vs. Pearson, Example 1\") +\n    annotate(\"text\", x = -10, y = 20, \n             label = paste(\"Pearson r = \", \n                 signif(cor(spear1$x, spear1$y),2),\n                 \", Spearman r = \", \n                 signif(cor(spear1$x, spear1$y, method = \"spearman\"),2)))"},{"path":"straight-line-models.html","id":"spearman-vs.-pearson-example-2","chapter":"11 Straight Line Models","heading":"11.6.4 Spearman vs. Pearson Example 2","text":"Example 2 shows negative Spearman correlation corresponds decreasing (, , necessarily linear) association x y.","code":"\nggplot(spear2, aes(x = x, y = y)) + \n    geom_point(col = \"purple\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = FALSE) +\n    labs(title = \"Spearman vs. Pearson, Example 2\") +\n    annotate(\"text\", x = 10, y = 20, col = \"purple\",\n             label = paste(\"Pearson r = \", \n                    signif(cor(spear2$x, spear2$y),2),\n                    \", Spearman r = \", \n                    signif(cor(spear2$x, spear2$y, method = \"spearman\"),2)))"},{"path":"straight-line-models.html","id":"spearman-vs.-pearson-example-3","chapter":"11 Straight Line Models","heading":"11.6.5 Spearman vs. Pearson Example 3","text":"Spearman correlation less sensitive Pearson correlation strong outliers unusual either X Y axis, . Spearman rank coefficient limits outlier value rank.Example 3, instance, Spearman correlation reacts much less outliers around X = 12 Pearson correlation.","code":"\nggplot(spear3, aes(x = x, y = y)) + \n    geom_point(col = \"blue\") +\n    labs(title = \"Spearman vs. Pearson, Example 3\") +\n    annotate(\"text\", x = 5, y = -15, col = \"blue\",\n             label = paste(\"Pearson r = \", \n                signif(cor(spear3$x, spear3$y),2),\n                \", Spearman r = \", \n                signif(cor(spear3$x, spear3$y, method = \"spearman\"),2)))"},{"path":"straight-line-models.html","id":"spearman-vs.-pearson-example-4","chapter":"11 Straight Line Models","heading":"11.6.6 Spearman vs. Pearson Example 4","text":"use Spearman correlation substitute looking data. non-monotone data like see Example 4, neither Spearman Pearson correlation alone provides much guidance, just (essentially) telling thing, doesn’t mean ’re telling helpful.","code":"\nggplot(spear4, aes(x = x, y = y)) +\n    geom_point(col = \"purple\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = FALSE) +\n    labs(title = \"Spearman vs. Pearson, Example 4\") +\n    annotate(\"text\", x = 10, y = 20, col = \"purple\",\n             label = paste(\"Pearson r = \", \n                 signif(cor(spear4$x, spear4$y),2),\n                 \", Spearman r = \", \n                 signif(cor(spear4$x, spear4$y, method = \"spearman\"),2)))"},{"path":"linearizing-transformations.html","id":"linearizing-transformations","chapter":"12 Linearizing Transformations","heading":"12 Linearizing Transformations","text":"","code":""},{"path":"linearizing-transformations.html","id":"linearize-the-association-between-quantitative-variables","chapter":"12 Linearizing Transformations","heading":"12.1 “Linearize” The Association between Quantitative Variables","text":"Confronted scatterplot describing monotone association two quantitative variables, may decide data well approximated straight line, thus, least squares regression may sufficiently useful. circumstances, least two options, mutually exclusive:Let data may, summarize scatterplot using tools like loess curves, polynomial functions, cubic splines model relationship.Consider re-expressing data (often start re-expressions outcome data [Y variable]) using transformation transformed data may modeled effectively using straight line.","code":""},{"path":"linearizing-transformations.html","id":"the-box-cox-plot","chapter":"12 Linearizing Transformations","heading":"12.2 The Box-Cox Plot","text":", Tukey’s ladder power transformations can guide exploration.Box-Cox plot, boxCox function car package, sifts ladder options suggest transformation (Y) best linearize outcome-predictor(s) relationship.","code":""},{"path":"linearizing-transformations.html","id":"a-few-caveats","chapter":"12 Linearizing Transformations","heading":"12.2.1 A Few Caveats","text":"methods work well monotone data, smooth function Y either strictly increasing, strictly decreasing, X increases.transformations require data positive. can rescale Y data adding constant every observation data set without changing shape.can use natural logarithm (log R), base 10 logarithm (log10) even sometimes base 2 logarithm (log2) good effect Tukey’s ladder. affect association’s shape way, ’ll stick log (base e).re-expressions don’t lead easily interpretable results. many things make sense original units also make sense inverse square roots. times won’t care, often, .primary interest making predictions, ’ll generally interested getting good predictions back original scale, can back-transform point interval estimates accomplish .","code":""},{"path":"linearizing-transformations.html","id":"a-simulated-example","chapter":"12 Linearizing Transformations","heading":"12.3 A Simulated Example","text":"simulated data produces curved scatterplot, now use Box-Cox plot lead choice appropriate power transformation Y order “linearize” association Y X.Box-Cox plot peaks value \\(\\lambda\\) = 0.44, pretty close \\(\\lambda\\) = 0.5. Now, 0.44 isn’t Tukey’s ladder, 0.5 .use \\(\\lambda\\) = 0.5, Tukey’s ladder power transformations, suggests look relationship square root Y X, shown next.eye, think square root plot better matches linear fit.","code":"\nset.seed(999); x.rand <- rbeta(80, 2, 5) * 20 + 3\nset.seed(1000); y.rand <- abs(50 + 0.75*x.rand^(3) - 0.65*x.rand + rnorm(80, 0, 200))\nscatter1 <- data_frame(x = x.rand, y = y.rand) Warning: `data_frame()` was deprecated in tibble 1.1.0.\nPlease use `tibble()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\nrm(x.rand, y.rand)\n\nggplot(scatter1, aes(x = x, y = y)) +\n    geom_point(shape = 1, size = 3) +\n    ## add loess smooth\n    geom_smooth(method = \"loess\", se = FALSE, \n                col = \"dodgerblue\", formula = y ~ x) +\n    ## then add linear fit\n    geom_smooth(method = \"lm\", se = FALSE, \n                col = \"red\", formula = y ~ x, linetype = \"dashed\") +\n    labs(title = \"Simulated scatter1 example: Y vs. X\")\nlibrary(car)\nboxCox(scatter1$y ~ scatter1$x) \npowerTransform(scatter1$y ~ scatter1$x)Estimated transformation parameter \n       Y1 \n0.4368753 \np1 <- ggplot(scatter1, aes(x = x, y = y)) +\n    geom_point(size = 2) +\n    geom_smooth(method = \"loess\", se = FALSE, \n                formula = y ~ x, col = \"dodgerblue\") +\n    geom_smooth(method = \"lm\", se = FALSE, \n                formula = y ~ x, col = \"red\", linetype = \"dashed\") +\n    labs(title = \"scatter1: Y vs. X\")\n\np2 <- ggplot(scatter1, aes(x = x, y = sqrt(y))) +\n    geom_point(size = 2) +\n    geom_smooth(method = \"loess\", se = FALSE, \n                formula = y ~ x, col = \"dodgerblue\") +\n    geom_smooth(method = \"lm\", se = FALSE, \n                formula = y ~ x, col = \"red\", linetype = \"dashed\") +\n    labs(title = \"scatter1: Square Root of Y vs. X\")\n\np1 + p2"},{"path":"linearizing-transformations.html","id":"checking-on-a-transformation-or-re-expression","chapter":"12 Linearizing Transformations","heading":"12.4 Checking on a Transformation or Re-Expression","text":"can three things check transformation.can calculate correlation original re-expressed associations.can use testTransform function car library R perform statistical test comparing optimal choice power (\\(\\lambda\\) = 0.44) various transformations.can go ahead fit regression models using approach compare plots studentized residuals fitted values data see re-expression reduces curve residual plot, well.Option 3 far important practice, ’s one ’ll focus going forward, ’ll demonstrate three .","code":""},{"path":"linearizing-transformations.html","id":"checking-the-correlation-coefficients","chapter":"12 Linearizing Transformations","heading":"12.4.1 Checking the Correlation Coefficients","text":", calculate correlation original re-expressed associations.Pearson correlation little stronger transformation. ’d expect.","code":"\ncor(scatter1$y, scatter1$x)[1] 0.891198\ncor(sqrt(scatter1$y), scatter1$x)[1] 0.9144307"},{"path":"linearizing-transformations.html","id":"using-the-testtransform-function","chapter":"12 Linearizing Transformations","heading":"12.4.2 Using the testTransform function","text":", use testTransform function (also car package) compare optimal choice determined powerTransform function (\\(\\lambda\\) = 0.44) \\(\\lambda\\) = 0 (logarithm), 0.5 (square root) 1 (transformation).looks like square root (\\(\\lambda\\) = 0.5) three options significantly worse log-likelihood criterion applied optimal choice.’s ’s one p value larger usual standard statistical significance, 0.05.","code":"\ntestTransform(powerTransform(scatter1$y ~ scatter1$x), 0)                           LRT df      pval\nLR test, lambda = (0) 46.17947  1 1.079e-11\ntestTransform(powerTransform(scatter1$y ~ scatter1$x), 0.5)                             LRT df    pval\nLR test, lambda = (0.5) 1.024888  1 0.31136\ntestTransform(powerTransform(scatter1$y ~ scatter1$x), 1)                           LRT df       pval\nLR test, lambda = (1) 63.75953  1 1.4433e-15"},{"path":"linearizing-transformations.html","id":"comparing-the-residual-plots","chapter":"12 Linearizing Transformations","heading":"12.4.3 Comparing the Residual Plots","text":"can fit regression models, obtain plots residuals fitted values, compare see one less indication curve residuals.’re looking plot absence curve, among things, want see “fuzzy football” shapes.compared original residual plot, square root version, modest improvement regard. look bit less curved, bit like random cluster points, ’s nice. Usually, can little better real data, shown next example NNYFS data introduced Chapter 9.","code":"\nmodel.orig <- lm(scatter1$y ~ scatter1$x)\nmodel.sqrt <- lm(sqrt(scatter1$y) ~ scatter1$x)\n\np1 <- augment(model.orig) %>%\n    ggplot(., aes(x = scatter1$x, y = .resid)) + \n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = FALSE) +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, col = \"red\") +\n    labs(title = \"Y vs X Residual Plot\")\n\np2 <- augment(model.sqrt) %>%\n    ggplot(., aes(x = scatter1$x, y = .resid)) + \n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = FALSE) +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, col = \"red\") +\n    labs(title = \"Square Root Model Residuals\")\n\np1 + p2"},{"path":"linearizing-transformations.html","id":"an-example-from-the-nnyfs-data","chapter":"12 Linearizing Transformations","heading":"12.5 An Example from the NNYFS data","text":"Using subjects nnyfs data complete data two variables interest, let’s look relationship arm circumference (outcome, shown Y axis) arm length (predictor, shown X axis.)","code":"\nnnyfs <- read_rds(\"data/nnyfs.Rds\")\nnnyfs_c <- nnyfs %>% \n    filter(complete.cases(arm_circ, arm_length)) %>%\n    select(SEQN, arm_circ, arm_length)"},{"path":"linearizing-transformations.html","id":"pearson-correlation-and-scatterplot","chapter":"12 Linearizing Transformations","heading":"12.5.1 Pearson correlation and scatterplot","text":"Pearson correlation two variables. Note use %$% pipe magrittr package help tell cor function data process.’s resulting scatterplot.Pearson correlation still quite strong, note loess smooth (shown blue) bends straight line model (shown red) low high end arm length.Note also use alpha = 0.2 show points greater transparency shown normally (default setting transparency alpha = 1.)","code":"\nnnyfs_c %$% cor(arm_length, arm_circ)[1] 0.8120242\nggplot(nnyfs_c, aes(x = arm_length, y = arm_circ)) +\n    geom_point(alpha = 0.2) +\n    geom_smooth(method = \"loess\", formula = y ~ x, \n                se = FALSE, color = \"blue\") +\n    geom_smooth(method = \"lm\", formula = y ~ x, \n                se = FALSE, color = \"red\")"},{"path":"linearizing-transformations.html","id":"plotting-the-residuals","chapter":"12 Linearizing Transformations","heading":"12.5.2 Plotting the Residuals","text":"Now, let’s build plot residuals straight line model plotted arm length. can obtain residuals using augment() function broom package.OK. residuals now stored .resid variable. can create residual plot, follows.","code":"\nm1 <- lm(arm_circ ~ arm_length, data = nnyfs_c)\n\nnnyfs_c_aug1 <- augment(m1, data = nnyfs_c)\n\nnnyfs_c_aug1# A tibble: 1,511 x 9\n    SEQN arm_circ arm_length .fitted .resid     .hat .sigma\n   <dbl>    <dbl>      <dbl>   <dbl>  <dbl>    <dbl>  <dbl>\n 1 71918     25.4       27.7    21.9  3.51  0.000695   3.21\n 2 71919     26         38.4    30.4 -4.38  0.00253    3.21\n 3 71920     37.9       35.9    28.4  9.50  0.00167    3.20\n 4 71921     15.1       18.3    14.4  0.669 0.00304    3.21\n 5 71922     29.5       34.2    27.0  2.45  0.00124    3.21\n 6 71923     27.9       33      26.1  1.80  0.00100    3.21\n 7 71924     17.6       26.5    20.9 -3.34  0.000788   3.21\n 8 71925     17.7       24.2    19.1 -1.41  0.00113    3.21\n 9 71926     19.9       26      20.5 -0.642 0.000844   3.21\n10 71927     17.3       20      15.8  1.52  0.00234    3.21\n# ... with 1,501 more rows, and 2 more variables:\n#   .cooksd <dbl>, .std.resid <dbl>\nggplot(nnyfs_c_aug1, aes(x = arm_length, y = .resid)) +\n    geom_point(alpha = 0.2) +\n    geom_smooth(method = \"loess\", col = \"purple\",\n                formula = y ~ x, se = FALSE) +\n    labs(title = \"Residuals show a curve.\")"},{"path":"linearizing-transformations.html","id":"using-the-box-cox-approach-to-identify-a-transformation","chapter":"12 Linearizing Transformations","heading":"12.5.3 Using the Box-Cox approach to identify a transformation","text":"suggests transform arm_circ data taking inverse (power = -1.) Let’s take look result.","code":"\nlibrary(car)\nboxCox(nnyfs_c$arm_circ ~ nnyfs_c$arm_length) \npowerTransform(nnyfs_c$arm_circ ~ nnyfs_c$arm_length)Estimated transformation parameter \n        Y1 \n-0.9783135 "},{"path":"linearizing-transformations.html","id":"plots-after-inverse-transformation","chapter":"12 Linearizing Transformations","heading":"12.5.4 Plots after Inverse Transformation","text":"Let’s build (left) revised scatterplot (right) revised residual plot transforming outcome (arm_circ) taking inverse.","code":"\nnnyfs_c <- nnyfs_c %>%\n    mutate(inv_arm_circ = 1/arm_circ)\n\np1 <- ggplot(nnyfs_c, aes(x = arm_length, y = inv_arm_circ)) +\n    geom_point(alpha = 0.2) +\n    geom_smooth(method = \"loess\", formula = y ~ x, \n                se = FALSE, color = \"blue\") +\n    geom_smooth(method = \"lm\", formula = y ~ x, \n                se = FALSE, color = \"red\") +\n    labs(title = \"Transformation reduces curve\")\n\nm2 <- lm(inv_arm_circ ~ arm_length, data = nnyfs_c)\n\nnnyfs_c_aug2 <- augment(m2, data = nnyfs_c)\n\np2 <- ggplot(nnyfs_c_aug2, aes(x = arm_length, y = .resid)) +\n    geom_point(alpha = 0.2) +\n    geom_smooth(method = \"loess\", col = \"purple\",\n                formula = y ~ x, se = FALSE) +\n    labs(title = \"Residuals much improved\")\n\np1 + p2 + \n    plot_annotation(title = \"Evaluating the Inverse Transformation\")"},{"path":"studying-crab-claws-crabs.html","id":"studying-crab-claws-crabs","chapter":"13 Studying Crab Claws (crabs)","heading":"13 Studying Crab Claws (crabs)","text":"next example, ’ll consider study zoology, specifically carcinology - study crustaceans. source data Chapter 7 Fred L. Ramsey Daniel W. Schafer30 drew data figure SB Yamada EG Boulding.31The available data mean closing forces (Newtons) propodus heights (mm) claws 38 crabs came three different species. propodus segment crab’s clawed leg immovable finger palm.\n(#fig:c13_crab1-fig)Source: http://txmarspecies.tamug.edu/crustglossary.cfm\npart study effects predatory intertidal crab species populations snails. three crab species study :14 Hemigraspus nudus, also called purple shore crab (14 crabs)12 Lophopanopeus bellus, also called black-clawed pebble crab, and12 Cancer productus, one several species red rock crabs (12)species information stored character variable. many different crabs talking species?turns , ’re going want treat species information factor three levels, rather character variable.’s quick summary data. Take care note useless results first two variables. least function flags * variables thinks non-numeric.Actually, ’re interested results grouping species.","code":"\ncrabs <- read_csv(\"data/crabs.csv\") Rows: 38 Columns: 4-- Column specification ------------------------------------\nDelimiter: \",\"\nchr (1): species\ndbl (3): crab, force, height\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\ncrabs# A tibble: 38 x 4\n    crab species              force height\n   <dbl> <chr>                <dbl>  <dbl>\n 1     1 Hemigraspus nudus      4      8  \n 2     2 Lophopanopeus bellus  15.1    7.9\n 3     3 Cancer productus       5      6.7\n 4     4 Lophopanopeus bellus   2.9    6.6\n 5     5 Hemigraspus nudus      3.2    5  \n 6     6 Hemigraspus nudus      9.5    7.9\n 7     7 Cancer productus      22.5    9.4\n 8     8 Hemigraspus nudus      7.4    8.3\n 9     9 Cancer productus      14.6   11.2\n10    10 Lophopanopeus bellus   8.7    8.6\n# ... with 28 more rows\ncrabs %>% tabyl(species)              species  n   percent\n     Cancer productus 12 0.3157895\n    Hemigraspus nudus 14 0.3684211\n Lophopanopeus bellus 12 0.3157895\ncrabs <- crabs %>%\n    mutate(species = factor(species))\npsych::describe(crabs)         vars  n  mean    sd median trimmed   mad min  max\ncrab        1 38 19.50 11.11  19.50   19.50 14.08   1 38.0\nspecies*    2 38  2.00  0.81   2.00    2.00  1.48   1  3.0\nforce       3 38 12.13  8.98   8.70   11.53  9.04   2 29.4\nheight      4 38  8.81  2.23   8.25    8.78  2.52   5 13.1\n         range skew kurtosis   se\ncrab      37.0 0.00    -1.30 1.80\nspecies*   2.0 0.00    -1.50 0.13\nforce     27.4 0.47    -1.25 1.46\nheight     8.1 0.19    -1.14 0.36\ncrabs %>%\n    group_by(species) %>%\n    summarise(n = n(), median(force), median(height))# A tibble: 3 x 4\n  species                  n `median(force)` `median(height)`\n  <fct>                <int>           <dbl>            <dbl>\n1 Cancer productus        12            19.7            11.0 \n2 Hemigraspus nudus       14             3.7             7.9 \n3 Lophopanopeus bellus    12            14.8             8.15"},{"path":"studying-crab-claws-crabs.html","id":"association-of-size-and-force","chapter":"13 Studying Crab Claws (crabs)","heading":"13.1 Association of Size and Force","text":"Suppose want describe force basis height, across 38 crabs. ’ll add titles identify three species crab, using shape color.faceted plot species really highlights difference force Hemigraspus nudus two species crab.","code":"\nggplot(crabs, aes(x = height, y = force, color = species, shape = species)) +\n    geom_point(size = 3) +\n    labs(title = \"Crab Claw Force by Size\", \n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\") +\n    theme_bw()\nggplot(crabs, aes(x = height, y = force, color = species)) +\n    geom_point(size = 3) +\n    facet_wrap(~ species) +\n    guides(color = \"none\") +\n    labs(title = \"Crab Claw Force by Size\", \n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\") +\n    theme_bw()"},{"path":"studying-crab-claws-crabs.html","id":"loess_smooth","chapter":"13 Studying Crab Claws (crabs)","heading":"13.2 The loess smooth","text":"can obtain smoothed curve (using several different approaches) summarize pattern presented data scatterplot. instance, might build plot complete set 38 crabs, adding non-linear smooth function (called loess smooth.)discussed previously, loess smooth fits curve data tracking (point x) points within neighborhood point x, emphasis given points near x. can adjusted tweaking span degree parameters.addition curve, smoothing procedures can also provide confidence intervals around main fitted line. Consider following plot crabs information, adjusts span (default 0.75) also adds confidence intervals.reducing size span, resulting picture shows much less smooth function generated previously.","code":"\nggplot(crabs, aes(x = height, y = force)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", se = FALSE, formula = y ~ x) +\n    labs(title = \"Crab Claw Force by Size\", \n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\")\nggplot(crabs, aes(x = height, y = force)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, span = 0.5, se = TRUE) +\n    labs(title = \"Crab Claw Force by Size\", \n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\")"},{"path":"studying-crab-claws-crabs.html","id":"smoothing-within-species","chapter":"13 Studying Crab Claws (crabs)","heading":"13.2.1 Smoothing within Species","text":"can, course, produce plot separate smooths three species crab.want add confidence intervals (’ll show 90% rather default 95%) plot faceted. Note default, displayed se = TRUE 95% prediction intervals - level function stat_smooth [can used place geom_smooth] used change coverage percentage 95% 90%.confidence intervals later, especially part B.","code":"\nggplot(crabs, aes(x = height, y = force, group = species, color = species)) +\n    geom_point(size = 3) +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = FALSE) +\n    labs(title = \"Crab Claw Force by Size\", \n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\")\nggplot(crabs, aes(x = height, y = force, group = species, color = species)) +\n    geom_point() +\n    stat_smooth(method = \"loess\", formula = y ~ x, level = 0.90, se = TRUE) +\n    guides(color = \"none\") +\n    labs(title = \"Crab Claw Force by Size\", \n         caption = \"with loess smooths and 90% confidence intervals\",\n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\") +\n    facet_wrap(~ species)"},{"path":"studying-crab-claws-crabs.html","id":"fitting-a-linear-regression-model","chapter":"13 Studying Crab Claws (crabs)","heading":"13.3 Fitting a Linear Regression Model","text":"Suppose plan use simple (least squares) linear regression model describe force function height. least squares model likely effective choice ?plot shows regression line predicting closing force function propodus height. annotate plot show actual fitted regression line, required fitting lm statement prior developing graph.lm function, , specifies linear model fit predict force using height. ’s summary., key things realize :outcome variable model force, predictor variable height.straight line model data fitted least squares force = -11.1 + 2.63 height.slope height positive, indicates height increases, expect force also increase. Specifically, expect every additional mm height, force increase 2.63 Newtons.multiple R-squared (squared correlation coefficient) 0.427, implies 42.7% variation force explained using linear model height. also implies Pearson correlation force height square root 0.427, 0.653.","code":"\nmod <- lm(force ~ height, data = crabs)\n\nggplot(crabs, aes(x = height, y = force)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x,  color = \"red\") +\n    labs(title = \"Crab Claw Force by Size with Linear Regression Model\", \n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\") +\n    annotate(\"text\", x = 11, y = 0, color = \"red\", fontface = \"italic\",\n             label = paste( \"Force - \", signif(coef(mod)[1],3), \" + \", \n                            signif(coef(mod)[2],3), \" Height\" ))\nrm(mod)\nsummary(lm(force ~ height, data = crabs))\nCall:\nlm(formula = force ~ height, data = crabs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.7945  -3.8113  -0.2394   4.1444  16.8814 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.0869     4.6224  -2.399   0.0218 *  \nheight        2.6348     0.5089   5.177 8.73e-06 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.892 on 36 degrees of freedom\nMultiple R-squared:  0.4268,    Adjusted R-squared:  0.4109 \nF-statistic:  26.8 on 1 and 36 DF,  p-value: 8.73e-06"},{"path":"studying-crab-claws-crabs.html","id":"is-a-linear-model-appropriate","chapter":"13 Studying Crab Claws (crabs)","heading":"13.4 Is a Linear Model Appropriate?","text":"zoology (least described Ramsey Schafer32) suggests actual nature relationship represented log-log relationship, log force predicted log height.log-log model appropriate model think percentage increases X (height, ) lead constant percentage increases Y (, force).see log-log model action, plot log force log height. use either base 10 (log10 R) natural (log R) logarithms.correlations raw force height logarithms turn quite similar, log transformation monotone data, ’s actually change Spearman correlations.","code":"\nggplot(crabs, aes(x = log(height), y = log(force))) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x) + \n    labs(title = \"Log-Log Model for Crabs data\")"},{"path":"studying-crab-claws-crabs.html","id":"the-log-log-model","chapter":"13 Studying Crab Claws (crabs)","heading":"13.4.1 The log-log model","text":"regression equation log(force) = -2.71 + 2.27 log(height)., example, found crab propodus height = 10 mm, prediction crab’s claw force (Newtons) based log-log model …log(force) = -2.71 + 2.27 log(10)log(force) = -2.71 + 2.27 x 2.3025851log(force) = 2.5190953and predicted force = exp(2.5190953) = 12.4173582 Newtons, , naturally, round 12.4 Newtons match data set’s level precision.","code":"\ncrab_loglog <- lm(log(force) ~ log(height), data = crabs)\n\nsummary(crab_loglog)\nCall:\nlm(formula = log(force) ~ log(height), data = crabs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5657 -0.4450  0.1884  0.4798  1.2422 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -2.7104     0.9251  -2.930  0.00585 ** \nlog(height)   2.2711     0.4284   5.302 5.96e-06 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6748 on 36 degrees of freedom\nMultiple R-squared:  0.4384,    Adjusted R-squared:  0.4228 \nF-statistic: 28.11 on 1 and 36 DF,  p-value: 5.96e-06"},{"path":"studying-crab-claws-crabs.html","id":"how-does-this-compare-to-our-original-linear-model","chapter":"13 Studying Crab Claws (crabs)","heading":"13.4.2 How does this compare to our original linear model?","text":"linear regression equation force = -11.1 + 2.63 height., example, found crab propodus height = 10 mm, prediction crab’s claw force (Newtons) based linear model …force = -11.0869025 + 2.6348232 x 10force = -11.0869025 + 26.3482321so predicted force = 15.2613297, round 15.3 Newtons., looks like two models give meaningfully different predictions.","code":"\ncrab_linear <- lm(force ~ height, data = crabs)\n\nsummary(crab_linear)\nCall:\nlm(formula = force ~ height, data = crabs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.7945  -3.8113  -0.2394   4.1444  16.8814 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.0869     4.6224  -2.399   0.0218 *  \nheight        2.6348     0.5089   5.177 8.73e-06 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.892 on 36 degrees of freedom\nMultiple R-squared:  0.4268,    Adjusted R-squared:  0.4109 \nF-statistic:  26.8 on 1 and 36 DF,  p-value: 8.73e-06"},{"path":"studying-crab-claws-crabs.html","id":"making-predictions-with-a-model","chapter":"13 Studying Crab Claws (crabs)","heading":"13.5 Making Predictions with a Model","text":"broom package’s augment function provides us consistent method obtaining predictions (also called fitted values) new crab original data. Suppose want predict force level two new crabs: one height = 10 mm, another height = 12 mm.want obtain prediction interval, can use predict function:’d interpret result saying linear model’s predicted force associated single new crab claw propodus height 10 mm 15.3 Newtons, 95% prediction interval true value force claw 1.0 29.5 Newtons. prediction intervals later.","code":"\nnewcrab <- tibble(crab = c(\"Crab_A\", \"Crab_B\"), height = c(10, 12))\n\naugment(crab_linear, newdata = newcrab)# A tibble: 2 x 3\n  crab   height .fitted\n  <chr>   <dbl>   <dbl>\n1 Crab_A     10    15.3\n2 Crab_B     12    20.5\npredict(crab_linear, newdata = newcrab, interval = \"prediction\", level = 0.95)       fit      lwr      upr\n1 15.26133 1.048691 29.47397\n2 20.53098 5.994208 35.06774"},{"path":"studying-crab-claws-crabs.html","id":"predictions-after-a-transformation","chapter":"13 Studying Crab Claws (crabs)","heading":"13.5.1 Predictions After a Transformation","text":"can also get predictions log-log model. default choice 95% prediction interval.course, predictions describe log(force) crab claw. get prediction terms simple force, ’d need back logarithm, exponentiating point estimate prediction interval endpoints.’d interpret result saying, first new crab, log-log model’s predicted force associated single new crab claw propodus height 10 mm 12.4 Newtons, 95% prediction interval true value force claw 3.1 50.0 Newtons.","code":"\npredict(crab_loglog, newdata = newcrab, interval = \"prediction\")       fit      lwr      upr\n1 2.519095 1.125900 3.912291\n2 2.933174 1.515548 4.350800\nexp(predict(crab_loglog, newdata = newcrab, interval = \"prediction\"))       fit      lwr      upr\n1 12.41736 3.082989 50.01341\n2 18.78716 4.551916 77.54044"},{"path":"studying-crab-claws-crabs.html","id":"comparing-model-predictions","chapter":"13 Studying Crab Claws (crabs)","heading":"13.5.2 Comparing Model Predictions","text":"Suppose wish build plot force vs height straight line linear model’s predictions, new curve log-log model’s predictions, can compare contrast implications two models common scale. predict function, given new data frame, use existing predictor values crabs data. predictions often called fitted values.put two sets predictions scale despite differing outcomes two models, ’ll exponentiate results log-log model, build little data frame containing heights predicted forces model.cleaner way might use augment function directly broom:Now, ’re ready use geom_smooth approach plot linear fit, geom_line (also fits curves) display log-log fit.Based 38 crabs, see modest differences predictions two models, log-log model predicting generally lower closing force given propodus height predicted linear model.","code":"\nloglogdat <- tibble(height = crabs$height, force = exp(predict(crab_loglog)))\naugment(crab_loglog)# A tibble: 38 x 7\n   `log(force)` `log(height)` .fitted   .hat .sigma  .cooksd\n          <dbl>         <dbl>   <dbl>  <dbl>  <dbl>    <dbl>\n 1         1.39          2.08   2.01  0.0280  0.676 1.28e- 2\n 2         2.71          2.07   1.98  0.0287  0.673 1.79e- 2\n 3         1.61          1.90   1.61  0.0499  0.684 8.06e-10\n 4         1.06          1.89   1.58  0.0530  0.679 1.69e- 2\n 5         1.16          1.61   0.945 0.142   0.683 1.01e- 2\n 6         2.25          2.07   1.98  0.0287  0.683 2.39e- 3\n 7         3.11          2.24   2.38  0.0301  0.673 1.90e- 2\n 8         2.00          2.12   2.10  0.0266  0.684 2.75e- 4\n 9         2.68          2.42   2.78  0.0561  0.684 6.30e- 4\n10         2.16          2.15   2.18  0.0263  0.684 5.34e- 6\n# ... with 28 more rows, and 1 more variable:\n#   .std.resid <dbl>\nggplot(crabs, aes(x = height, y = force)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, \n                formula = y ~ x, col=\"blue\", linetype = 2) +\n    geom_line(data = loglogdat, col = \"red\", linetype = 2, size = 1) +\n    annotate(\"text\", 7, 12, label = \"Linear Model\", col = \"blue\") +\n    annotate(\"text\", 10, 8, label = \"Log-Log Model\", col = \"red\") +\n    labs(title = \"Comparing the Linear and Log-Log Models for Crab Claw data\")"},{"path":"Hydrate-Study.html","id":"Hydrate-Study","chapter":"14 Dehydration Recovery","heading":"14 Dehydration Recovery","text":"hydrate data describe degree recovery takes place 90 minutes following treatment moderate severe dehydration, 36 children diagnosed hospital’s main pediatric clinic.Upon diagnosis study entry, patients treated electrolytic solution one seven dose levels (0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0 mEq/l) frozen, flavored, ice popsicle. degree rehydration determined using subjective scale based physical examination parental input, converted 0 100 point scale, representing percent recovery (recov.score). child’s age (years) weight (pounds) also available.First, ’ll check ranges (missing data) hydrate file.missing values, ranges make sense. especially egregious problems report.","code":"\nhydrate <- read_csv(\"data/hydrate.csv\")\n\nsummary(hydrate)       id         recov.score          dose      \n Min.   : 1.00   Min.   : 44.00   Min.   :0.000  \n 1st Qu.: 9.75   1st Qu.: 61.50   1st Qu.:1.000  \n Median :18.50   Median : 71.50   Median :1.500  \n Mean   :18.50   Mean   : 71.56   Mean   :1.569  \n 3rd Qu.:27.25   3rd Qu.: 80.00   3rd Qu.:2.500  \n Max.   :36.00   Max.   :100.00   Max.   :3.000  \n      age             weight     \n Min.   : 3.000   Min.   :22.00  \n 1st Qu.: 5.000   1st Qu.:34.50  \n Median : 6.500   Median :47.50  \n Mean   : 6.667   Mean   :46.89  \n 3rd Qu.: 8.000   3rd Qu.:57.25  \n Max.   :11.000   Max.   :76.00  "},{"path":"Hydrate-Study.html","id":"a-scatterplot-matrix","chapter":"14 Dehydration Recovery","heading":"14.1 A Scatterplot Matrix","text":"Next, ’ll use scatterplot matrix summarize relationships outcome recov.score key predictor dose well ancillary predictors age weight, less interest, expected related outcome. one uses ggpairs function GGally package, introduced Part Notes. place outcome bottom row, key predictor immediately , age weight top rows, using select function within `ggpairs call.can conclude ?looks like recov.score moderately strong negative relationship age weight (correlations case around -0.5), positive relationship dose (correlation = 0.36).distribution recov.score looks pretty close Normal. potential predictors (age, weight dose) show substantial non-Normality.age weight, ’d expect, show strong positive linear relationship, r = 0.94Neither age weight shows meaningful relationship dose. (r = 0.16)","code":"\nGGally::ggpairs(dplyr::select(hydrate, age, weight, dose, recov.score), \n                title = \"Scatterplot Matrix for hydrate data\")Registered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2"},{"path":"Hydrate-Study.html","id":"are-the-recovery-scores-well-described-by-a-normal-model","chapter":"14 Dehydration Recovery","heading":"14.2 Are the recovery scores well described by a Normal model?","text":"Next, ’ll thorough graphical summary outcome, recovery score.see serious problems assuming Normality recovery scores. outcome variable doesn’t way need follow Normal distribution, ’s nice , summaries involving means standard deviations make sense.","code":"\np1 <- ggplot(hydrate, aes(sample = recov.score)) +\n  geom_qq(col = '#440154') + geom_qq_line(col = \"red\") + \n  theme(aspect.ratio = 1) + \n  labs(title = \"Normal Q-Q plot: hydrate\")\n\np2 <- ggplot(hydrate, aes(x = recov.score)) +\n  geom_histogram(aes(y = stat(density)), \n                 bins = 10, fill = '#440154', col = '#FDE725') +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(hydrate$recov.score), \n                            sd = sd(hydrate$recov.score)),\n                col = \"red\", lwd = 1.5) +\n  labs(title = \"Density Function: hydrate\")\n\np3 <- ggplot(hydrate, aes(x = recov.score, y = \"\")) +\n  geom_boxplot(fill = '#440154', outlier.color = '#440154') + \n  labs(title = \"Boxplot: hydrate\", y = \"\")\n\np1 + (p2 / p3 + plot_layout(heights = c(4,1)))\nmosaic::favstats(~ recov.score, data = hydrate) %>% kable(digits = 1)"},{"path":"Hydrate-Study.html","id":"simple-regression-using-dose-to-predict-recovery","chapter":"14 Dehydration Recovery","heading":"14.3 Simple Regression: Using Dose to predict Recovery","text":"start, consider simple (one predictor) regression model using dose alone predict % Recovery (recov.score). Ignoring age weight covariates, can conclude relationship?","code":""},{"path":"Hydrate-Study.html","id":"the-scatterplot-with-fitted-linear-model","chapter":"14 Dehydration Recovery","heading":"14.4 The Scatterplot, with fitted Linear Model","text":"","code":"\nggplot(hydrate, aes(x = dose, y = recov.score)) +\n    geom_point(size = 2) +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    labs(title = \"Simple Regression model for the hydrate data\",\n         x = \"Dose (mEq/l)\", y = \"Recovery Score (points)\")"},{"path":"Hydrate-Study.html","id":"the-fitted-linear-model","chapter":"14 Dehydration Recovery","heading":"14.5 The Fitted Linear Model","text":"obtain fitted linear regression model, use lm function:, fitted regression model (prediction model) recov.score = 63.9 + 4.88 dose.","code":"\nm1 <- lm(recov.score ~ dose, data = hydrate)\n\ntidy(m1) %>% kable(digits = 2)"},{"path":"Hydrate-Study.html","id":"confidence-intervals","chapter":"14 Dehydration Recovery","heading":"14.5.1 Confidence Intervals","text":"can obtain confidence intervals around coefficients fitted model tidy, ., 90% confidence interval slope dose ranges 1.21 8.55.","code":"\ntidy(m1, conf.int = TRUE, conf.level = 0.90) %>% kable(digits = 2)"},{"path":"Hydrate-Study.html","id":"coefficient-plots","chapter":"14 Dehydration Recovery","heading":"14.6 Coefficient Plots","text":"tidy method makes easy construct coefficient plots using ggplot2.Another option use geom_errorbarh setting, perhaps different color scheme…","code":"\ntd <- tidy(m1, conf.int = TRUE, conf.level = 0.90)\nggplot(td, aes(x = estimate, y = term, col = term)) +\n  geom_point() +\n  geom_crossbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_vline(xintercept = 0) +\n  guides(col = \"none\") +\n  labs(title = \"Estimates with 90% confidence intervals from m1 in hydrate\")\ntd <- tidy(m1, conf.int = TRUE, conf.level = 0.90)\nggplot(td, aes(x = estimate, y = term, col = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_vline(xintercept = 0) +\n  scale_color_viridis_d(end = 0.5) +\n  guides(col = \"none\") +\n  labs(title = \"Estimates with 90% confidence intervals from m1 in hydrate\")"},{"path":"Hydrate-Study.html","id":"the-summary-output","chapter":"14 Dehydration Recovery","heading":"14.7 The Summary Output","text":"get complete understanding fitted model, ’ll summarize .","code":"\nsummary(lm(recov.score ~ dose, data = hydrate))\nCall:\nlm(formula = recov.score ~ dose, data = hydrate)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3360  -7.2763   0.0632   8.4233  23.9028 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   63.896      3.970  16.093   <2e-16 ***\ndose           4.881      2.172   2.247   0.0313 *  \n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.21 on 34 degrees of freedom\nMultiple R-squared:  0.1293,    Adjusted R-squared:  0.1037 \nF-statistic: 5.047 on 1 and 34 DF,  p-value: 0.03127"},{"path":"Hydrate-Study.html","id":"model-specification","chapter":"14 Dehydration Recovery","heading":"14.7.1 Model Specification","text":"first part output specifies model fit.\n, simple regression model predicts recov.score basis dose.\nNotice ’re treating dose quantitative variable. wanted dose treated factor, ’d specified model.\n, simple regression model predicts recov.score basis dose.Notice ’re treating dose quantitative variable. wanted dose treated factor, ’d specified model.","code":""},{"path":"Hydrate-Study.html","id":"residual-summary","chapter":"14 Dehydration Recovery","heading":"14.7.2 Residual Summary","text":"second part output summarizes regression residuals across subjects involved fitting model.\nresidual defined Actual value outcome minus predicted value outcome fitted model.\ncase, residual given child actual recov.score minus predicted recov.score according model, child.\nresidual summary gives us sense “incorrect” predictions hydrate observations.\npositive residual means observed value higher predicted value linear regression model, prediction low.\nnegative residual means observed value lower predicted value linear regression model, prediction high.\nresiduals center near 0 (ordinary least squares model fitting process designed mean residuals always zero)\nhope see median residuals also near zero, generally. case, median prediction 0.06 point low.\nminimum maximum show us largest prediction errors, made subjects used fit model.\n, predicted recovery score 22.3 points high one patient, another predicted recovery scores 23.9 points low.\nmiddle half predictions 8.4 points low 7.3 points high.\n\nresidual defined Actual value outcome minus predicted value outcome fitted model.case, residual given child actual recov.score minus predicted recov.score according model, child.residual summary gives us sense “incorrect” predictions hydrate observations.\npositive residual means observed value higher predicted value linear regression model, prediction low.\nnegative residual means observed value lower predicted value linear regression model, prediction high.\nresiduals center near 0 (ordinary least squares model fitting process designed mean residuals always zero)\nhope see median residuals also near zero, generally. case, median prediction 0.06 point low.\nminimum maximum show us largest prediction errors, made subjects used fit model.\n, predicted recovery score 22.3 points high one patient, another predicted recovery scores 23.9 points low.\nmiddle half predictions 8.4 points low 7.3 points high.\npositive residual means observed value higher predicted value linear regression model, prediction low.negative residual means observed value lower predicted value linear regression model, prediction high.residuals center near 0 (ordinary least squares model fitting process designed mean residuals always zero)hope see median residuals also near zero, generally. case, median prediction 0.06 point low.minimum maximum show us largest prediction errors, made subjects used fit model., predicted recovery score 22.3 points high one patient, another predicted recovery scores 23.9 points low.middle half predictions 8.4 points low 7.3 points high.","code":""},{"path":"Hydrate-Study.html","id":"coefficients-output","chapter":"14 Dehydration Recovery","heading":"14.7.3 Coefficients Output","text":"Coefficients output begins table estimated coefficients regression equation.\nGenerally, write simple regression model \\(y = \\beta_0 + \\beta_1 x\\).\nhydrate model, recov.score = \\(\\beta_0\\) + \\(\\beta_1\\) dose.\nfirst column table gives estimated \\(\\beta\\) coefficients model\nestimated intercept \\(\\hat{\\beta_0} = 63.9\\)\nestimated slope dose \\(\\hat{\\beta_1} = 4.88\\)\nThus, model recov.score= 63.9 + 4.88 dose\n\nGenerally, write simple regression model \\(y = \\beta_0 + \\beta_1 x\\).hydrate model, recov.score = \\(\\beta_0\\) + \\(\\beta_1\\) dose.first column table gives estimated \\(\\beta\\) coefficients model\nestimated intercept \\(\\hat{\\beta_0} = 63.9\\)\nestimated slope dose \\(\\hat{\\beta_1} = 4.88\\)\nThus, model recov.score= 63.9 + 4.88 dose\nestimated intercept \\(\\hat{\\beta_0} = 63.9\\)estimated slope dose \\(\\hat{\\beta_1} = 4.88\\)Thus, model recov.score= 63.9 + 4.88 doseWe interpret coefficients follows:intercept (63.9) predicted recov.score patient receiving dose 0 mEq/l electrolytic solution.slope (4.88) dose predicted change recov.score associated 1 mEq/l increase dose electrolytic solution.\nEssentially, two children like ones studied , give Roger popsicle dose X Sarah popsicle dose X + 1, model predicts Sarah recovery score 4.88 points higher Roger.\nconfidence interval output saw previously function confint(lm(recov.score ~ dose)), 95% confident true slope dose (0.47, 9.30) mEq/l. also 95% confident true intercept (55.8, 72.0).\nEssentially, two children like ones studied , give Roger popsicle dose X Sarah popsicle dose X + 1, model predicts Sarah recovery score 4.88 points higher Roger.confidence interval output saw previously function confint(lm(recov.score ~ dose)), 95% confident true slope dose (0.47, 9.30) mEq/l. also 95% confident true intercept (55.8, 72.0).","code":""},{"path":"Hydrate-Study.html","id":"correlation-and-slope","chapter":"14 Dehydration Recovery","heading":"14.7.4 Correlation and Slope","text":"like, can use cor function specify Pearson correlation recov.score dose, turns 0.36.\n- Note slope simple regression model follow sign Pearson correlation coefficient, case, positive.","code":"\nhydrate %$% cor(recov.score, dose)[1] 0.359528"},{"path":"Hydrate-Study.html","id":"coefficient-testing","chapter":"14 Dehydration Recovery","heading":"14.7.5 Coefficient Testing","text":"Next coefficient summary regression table estimated standard error, followed coefficient’s t value (coefficient value divided standard error), associated two-tailed p value test :H0: coefficient’s \\(\\beta\\) value = 0 vs. HA: coefficient’s \\(\\beta\\) value \\(\\neq\\) 0.slope coefficient, can interpret choice :H0: predictor adds predictive value model vs. HA: predictor adds predictive value model.hydrate simple regression model, running either tidy just confint function shown , can establish confidence interval estimated regression coefficients.slope dose fact zero, mean knowing dose information additional value predicting outcome just guessing mean recov.score every subject., since confidence interval slope dose include zero, appears least evidence model m1 effective model ignores dose information (simply predicts mean recov.score subject.) ’s saying much, actually.","code":"\nsummary(lm(recov.score ~ dose, data = hydrate))\nCall:\nlm(formula = recov.score ~ dose, data = hydrate)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3360  -7.2763   0.0632   8.4233  23.9028 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   63.896      3.970  16.093   <2e-16 ***\ndose           4.881      2.172   2.247   0.0313 *  \n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.21 on 34 degrees of freedom\nMultiple R-squared:  0.1293,    Adjusted R-squared:  0.1037 \nF-statistic: 5.047 on 1 and 34 DF,  p-value: 0.03127\ntidy(m1, conf.int = TRUE, conf.level = 0.95) %>% kable(digits = 2)\nconfint(m1, level = .95)                2.5 %    97.5 %\n(Intercept) 55.826922 71.964589\ndose         0.465695  9.295466"},{"path":"Hydrate-Study.html","id":"summarizing-the-quality-of-fit","chapter":"14 Dehydration Recovery","heading":"14.7.6 Summarizing the Quality of Fit","text":"next part regression summary output summary fit quality.residual standard error estimates standard deviation prediction errors made model.assumptions hold, model produce residuals follow Normal distribution mean 0 standard deviation equal residual standard error.\n’d expect roughly 95% residuals fall -2(12.21) +2(12.21), roughly -24.4 +24.4 ’d see virtually residuals outside range -3(12.21) +3(12.21), roughly -36.6 +36.6.\noutput top summary tells us observed regression residuals, actually range -22 +24.\ncontext, ’s hard know whether happy . scale 0 100, rarely missing 24 seems OK , terrific.\n’d expect roughly 95% residuals fall -2(12.21) +2(12.21), roughly -24.4 +24.4 ’d see virtually residuals outside range -3(12.21) +3(12.21), roughly -36.6 +36.6.output top summary tells us observed regression residuals, actually range -22 +24.context, ’s hard know whether happy . scale 0 100, rarely missing 24 seems OK , terrific.degrees freedom denominator degrees freedom ANOVA follow. calculation \\(n - k\\), \\(n\\) = number observations \\(k\\) number coefficients estimated regression (including intercept slopes).\n, 36 observations model, fit k = 2 coefficients; slope intercept, simple regression model, df = 36 - 2 = 34.\n, 36 observations model, fit k = 2 coefficients; slope intercept, simple regression model, df = 36 - 2 = 34.multiple R-squared value usually just referred R-squared.interpreted proportion variation outcome variable accounted regression model.\n, ’ve accounted just 13% variation % Recovery using Dose.\n, ’ve accounted just 13% variation % Recovery using Dose.R multiple R-squared Pearson correlation recov.score dose, case 0.3595.\nSquaring value gives R-squared simple regression.\n(0.3595)^2 = 0.129\nSquaring value gives R-squared simple regression.(0.3595)^2 = 0.129R-squared greedy.R-squared always suggest make models big possible, often including variables dubious predictive value.result, various methods adjusting penalizing R-squared wind smaller models.adjusted R-squared often useful way compare multiple models response.\n\\(R^2_{adj} = 1 - \\frac{(1-R^2)(n - 1)}{n - k}\\), \\(n\\) = number observations \\(k\\) number coefficients estimated regression (including intercept slopes).\n, case, \\(R^2_{adj} = 1 - \\frac{(1 - 0.1293)(35)}{34} = 0.1037\\)\nadjusted R-squared value , technically, proportion anything, comparable across models outcome.\nadjusted R-squared always less (unadjusted) R-squared.\n\\(R^2_{adj} = 1 - \\frac{(1-R^2)(n - 1)}{n - k}\\), \\(n\\) = number observations \\(k\\) number coefficients estimated regression (including intercept slopes)., case, \\(R^2_{adj} = 1 - \\frac{(1 - 0.1293)(35)}{34} = 0.1037\\)adjusted R-squared value , technically, proportion anything, comparable across models outcome.adjusted R-squared always less (unadjusted) R-squared.","code":""},{"path":"Hydrate-Study.html","id":"anova-f-test","chapter":"14 Dehydration Recovery","heading":"14.7.7 ANOVA F test","text":"last part standard summary regression model overall ANOVA F test.hypotheses test :H0: coefficients model (intercept) \\(\\beta\\) = 0 vs.HA: least one regression slope \\(\\beta \\neq\\) 0Since simple regression just one predictor, ANOVA F test hypotheses exactly t test dose:H0: slope dose \\(\\beta\\) = 0 vs.HA: slope dose \\(\\beta \\neq\\) 0In case, F statistic 5.05 1 34 degrees freedom, yielding p = 0.03This provides evidence “something” model (, dose predictor) predicts outcome degree beyond easily attributed chance alone. actually surprising, especially interesting. confidence interval slope definitely interesting .simple regression (regression one predictor), t test slope (dose) always provides p value ANOVA F test.\nF test statistic simple regression always definition just square slope’s t test statistic.\n, F = 5.047, square t = 2.247 Coefficients output\nF test statistic simple regression always definition just square slope’s t test statistic., F = 5.047, square t = 2.247 Coefficients outputThis test basically just combination R-squared value (13%) sample size. don’t learn much ’s practically interesting useful.","code":""},{"path":"Hydrate-Study.html","id":"viewing-the-complete-anova-table","chapter":"14 Dehydration Recovery","heading":"14.8 Viewing the complete ANOVA table","text":"can obtain complete ANOVA table associated particular model, details behind F test using anova function:R-squared regression model equal \\(\\eta^2\\) ANOVA model.\ndivide SS(dose) = 752.2 total sum squares (752.2 + 5066.7), ’ll get multiple R-squared [0.1293]\ndivide SS(dose) = 752.2 total sum squares (752.2 + 5066.7), ’ll get multiple R-squared [0.1293]Note ANOVA model get treated dose factor seven levels, rather quantitative variable.","code":"\nanova(lm(recov.score ~ dose, data = hydrate))Analysis of Variance Table\n\nResponse: recov.score\n          Df Sum Sq Mean Sq F value  Pr(>F)  \ndose       1  752.2  752.15  5.0473 0.03127 *\nResiduals 34 5066.7  149.02                  \n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"Hydrate-Study.html","id":"using-glance-to-summarize-the-models-fit","chapter":"14 Dehydration Recovery","heading":"14.9 Using glance to summarize the model’s fit","text":"applied linear model, glance function broom package summarizes 12 characteristics model’s fit.Let’s look eight ’ve already addressed.’ve discussed R-square value, shown r.squared.’ve also discussed adjusted R-square value, adj.r.squaredsigma residual standard error.statistic ANOVA F statistic.p.value p value associated ANOVA F statistic.df numerator degrees freedom (, df associated dose) ANOVA test associated model.df.residual denominator degrees freedom (df associated residual) ANOVA test.Remember F-statistic bottom summary output provides last four statistics, well.nobs number observations (rows) used fit model.Now, let’s look remaining four summaries:logLik log-likelihood value model, commonly used model (like ordinary least squares model fit lm fit using method maximum likelihood). Thus, log-likelihood value maximized fit.AIC Akaike Information Criterion model. comparing models fitted maximum likelihood outcome variable (using transformation, example), smaller AIC, better fit.BIC Bayes Information Criterion model. comparing models fitted maximum likelihood outcome variable (using transformation, example), smaller BIC, better fit. BIC often prefers models fewer coefficients estimate AIC.\nAIC BIC can estimated using several different approaches R, ’ll need use one across multiple models ’re comparing results, concepts defined constant.\nAIC BIC can estimated using several different approaches R, ’ll need use one across multiple models ’re comparing results, concepts defined constant.deviance fitted model’s deviance, measure lack fit. generalization residual sum squares seen ANOVA table, takes value case simple linear regression model fit lm . generalized linear models, ’ll use hypothesis testing, just ANOVA table linear model case.","code":"\nglance(m1) %>% select(r.squared:df, df.residual, nobs) %>%\n  kable(digits = c(3, 3, 1, 2, 3, 0, 0, 0))\nglance(m1) %>% select(logLik:deviance) %>%\n  kable(digits = 1)"},{"path":"Hydrate-Study.html","id":"plotting-residuals-vs.-fitted-values","chapter":"14 Dehydration Recovery","heading":"14.10 Plotting Residuals vs. Fitted Values","text":"save residuals predicted (fitted) values simple regression model, can use resid fitted commands, respectively, can use augment function broom package obtain tidy data set containing objects others.can also obtain plot residuals vs. fitted values m1 using following code base R.hope plot see generally random scatter points, perhaps looking like “fuzzy football.” Since seven possible dose values, obtain seven distinct predicted values, explains seven vertical lines plot. , smooth red line indicates gentle curve, evidence strong curve, regular pattern residual plot.","code":"\naugment(m1) %>%\n    ggplot(., aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = F) +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = F, col = \"red\") +\n    labs(title = \"Residuals vs. Fitted values for Model m1\")\nplot(m1, which = 1)"},{"path":"WCGS-Study.html","id":"WCGS-Study","chapter":"15 The WCGS","heading":"15 The WCGS","text":"","code":""},{"path":"WCGS-Study.html","id":"the-western-collaborative-group-study-wcgs-data-set","chapter":"15 The WCGS","heading":"15.1 The Western Collaborative Group Study (wcgs) data set","text":"Vittinghoff et al.33 explore data Western Collaborative Group Study (WCGS) great detail34. ’ll touch lightly key issues Chapter.Western Collaborative Group Study (WCGS) designed test hypothesis -called Type behavior pattern (TABP) - “characterized particularly excessive drive, aggressiveness, ambition, frequently association relatively greater preoccupation competitive activity, vocational deadlines, similar pressures” - cause coronary heart disease (CHD). Two additional goals, developed later study, (1) investigate comparability formulas developed WCGS Framingham Study (FS) prediction CHD risk, (2) determine addition TABP existing multivariate prediction formula affects ability select subjects intervention programs.study enrolled 3,000 men ages 39-59 employed San Francisco Los Angeles, 1960 1961., 3154 rows (subjects) 22 columns (variables). importing data creating tibble read_csv, used mutate(across((.character), as_factor) convert variables containing character data factors.","code":"\nwcgs <- read_csv(\"data/wcgs.csv\") %>%\n    mutate(across(where(is.character), as_factor))\n\nwcgs# A tibble: 3,154 x 22\n      id   age agec  height weight lnwght wghtcat   bmi\n   <dbl> <dbl> <fct>  <dbl>  <dbl>  <dbl> <fct>   <dbl>\n 1  2343    50 46-50     67    200   5.30 170-200  31.3\n 2  3656    51 51-55     73    192   5.26 170-200  25.3\n 3  3526    59 56-60     70    200   5.30 170-200  28.7\n 4 22057    51 51-55     69    150   5.01 140-170  22.1\n 5 12927    44 41-45     71    160   5.08 140-170  22.3\n 6 16029    47 46-50     64    158   5.06 140-170  27.1\n 7  3894    40 35-40     70    162   5.09 140-170  23.2\n 8 11389    41 41-45     70    160   5.08 140-170  23.0\n 9 12681    50 46-50     71    195   5.27 170-200  27.2\n10 10005    43 41-45     68    187   5.23 170-200  28.4\n# ... with 3,144 more rows, and 14 more variables:\n#   sbp <dbl>, lnsbp <dbl>, dbp <dbl>, chol <dbl>,\n#   behpat <fct>, dibpat <fct>, smoke <fct>, ncigs <dbl>,\n#   arcus <dbl>, chd69 <fct>, typchd69 <dbl>,\n#   time169 <dbl>, t1 <dbl>, uni <dbl>"},{"path":"WCGS-Study.html","id":"structure-of-wcgs","chapter":"15 The WCGS","heading":"15.1.1 Structure of wcgs","text":"can specify (sometimes terrible) variable names, names function, can add elements structure, can identify elements particular interest.","code":"\nstr(wcgs)spec_tbl_df [3,154 x 22] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ id      : num [1:3154] 2343 3656 3526 22057 12927 ...\n $ age     : num [1:3154] 50 51 59 51 44 47 40 41 50 43 ...\n $ agec    : Factor w/ 5 levels \"46-50\",\"51-55\",..: 1 2 3 2 4 1 5 4 1 4 ...\n $ height  : num [1:3154] 67 73 70 69 71 64 70 70 71 68 ...\n $ weight  : num [1:3154] 200 192 200 150 160 158 162 160 195 187 ...\n $ lnwght  : num [1:3154] 5.3 5.26 5.3 5.01 5.08 ...\n $ wghtcat : Factor w/ 4 levels \"170-200\",\"140-170\",..: 1 1 1 2 2 2 2 2 1 1 ...\n $ bmi     : num [1:3154] 31.3 25.3 28.7 22.1 22.3 ...\n $ sbp     : num [1:3154] 132 120 158 126 126 116 122 130 112 120 ...\n $ lnsbp   : num [1:3154] 4.88 4.79 5.06 4.84 4.84 ...\n $ dbp     : num [1:3154] 90 74 94 80 80 76 78 84 70 80 ...\n $ chol    : num [1:3154] 249 194 258 173 214 206 190 212 130 233 ...\n $ behpat  : Factor w/ 4 levels \"A1\",\"A2\",\"B3\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ dibpat  : Factor w/ 2 levels \"Type A\",\"Type B\": 1 1 1 1 1 1 1 1 1 1 ...\n $ smoke   : Factor w/ 2 levels \"Yes\",\"No\": 1 1 2 2 2 1 2 1 2 1 ...\n $ ncigs   : num [1:3154] 25 25 0 0 0 80 0 25 0 25 ...\n $ arcus   : num [1:3154] 1 0 1 1 0 0 0 0 1 0 ...\n $ chd69   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ typchd69: num [1:3154] 0 0 0 0 0 0 0 0 0 0 ...\n $ time169 : num [1:3154] 1367 2991 2960 3069 3081 ...\n $ t1      : num [1:3154] -1.63 -4.06 0.64 1.12 2.43 ...\n $ uni     : num [1:3154] 0.486 0.186 0.728 0.624 0.379 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   id = col_double(),\n  ..   age = col_double(),\n  ..   agec = col_character(),\n  ..   height = col_double(),\n  ..   weight = col_double(),\n  ..   lnwght = col_double(),\n  ..   wghtcat = col_character(),\n  ..   bmi = col_double(),\n  ..   sbp = col_double(),\n  ..   lnsbp = col_double(),\n  ..   dbp = col_double(),\n  ..   chol = col_double(),\n  ..   behpat = col_character(),\n  ..   dibpat = col_character(),\n  ..   smoke = col_character(),\n  ..   ncigs = col_double(),\n  ..   arcus = col_double(),\n  ..   chd69 = col_character(),\n  ..   typchd69 = col_double(),\n  ..   time169 = col_double(),\n  ..   t1 = col_double(),\n  ..   uni = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> "},{"path":"WCGS-Study.html","id":"codebook-for-wcgs","chapter":"15 The WCGS","heading":"15.1.2 Codebook for wcgs","text":"table lovingly hand-crafted, involved lot typing. ’ll look better ways 432.","code":""},{"path":"WCGS-Study.html","id":"quick-summary","chapter":"15 The WCGS","heading":"15.1.3 Quick Summary","text":"detailed description, might consider Hmisc::describe, psych::describe, mosaic::favstats, etc.","code":"\nsummary(wcgs)       id             age           agec     \n Min.   : 2001   Min.   :39.00   46-50: 750  \n 1st Qu.: 3741   1st Qu.:42.00   51-55: 528  \n Median :11406   Median :45.00   56-60: 242  \n Mean   :10478   Mean   :46.28   41-45:1091  \n 3rd Qu.:13115   3rd Qu.:50.00   35-40: 543  \n Max.   :22101   Max.   :59.00               \n                                             \n     height          weight        lnwght     \n Min.   :60.00   Min.   : 78   Min.   :4.357  \n 1st Qu.:68.00   1st Qu.:155   1st Qu.:5.043  \n Median :70.00   Median :170   Median :5.136  \n Mean   :69.78   Mean   :170   Mean   :5.128  \n 3rd Qu.:72.00   3rd Qu.:182   3rd Qu.:5.204  \n Max.   :78.00   Max.   :320   Max.   :5.768  \n                                              \n    wghtcat          bmi             sbp       \n 170-200:1171   Min.   :11.19   Min.   : 98.0  \n 140-170:1538   1st Qu.:22.96   1st Qu.:120.0  \n > 200  : 213   Median :24.39   Median :126.0  \n < 140  : 232   Mean   :24.52   Mean   :128.6  \n                3rd Qu.:25.84   3rd Qu.:136.0  \n                Max.   :38.95   Max.   :230.0  \n                                               \n     lnsbp            dbp              chol       behpat   \n Min.   :4.585   Min.   : 58.00   Min.   :103.0   A1: 264  \n 1st Qu.:4.787   1st Qu.: 76.00   1st Qu.:197.2   A2:1325  \n Median :4.836   Median : 80.00   Median :223.0   B3:1216  \n Mean   :4.850   Mean   : 82.02   Mean   :226.4   B4: 349  \n 3rd Qu.:4.913   3rd Qu.: 86.00   3rd Qu.:253.0            \n Max.   :5.438   Max.   :150.00   Max.   :645.0            \n                                  NA's   :12               \n    dibpat     smoke          ncigs          arcus       \n Type A:1589   Yes:1502   Min.   : 0.0   Min.   :0.0000  \n Type B:1565   No :1652   1st Qu.: 0.0   1st Qu.:0.0000  \n                          Median : 0.0   Median :0.0000  \n                          Mean   :11.6   Mean   :0.2985  \n                          3rd Qu.:20.0   3rd Qu.:1.0000  \n                          Max.   :99.0   Max.   :1.0000  \n                                         NA's   :2       \n chd69         typchd69         time169    \n No :2897   Min.   :0.0000   Min.   :  18  \n Yes: 257   1st Qu.:0.0000   1st Qu.:2842  \n            Median :0.0000   Median :2942  \n            Mean   :0.1363   Mean   :2684  \n            3rd Qu.:0.0000   3rd Qu.:3037  \n            Max.   :3.0000   Max.   :3430  \n                                           \n       t1                 uni           \n Min.   :-47.43147   Min.   :0.0007097  \n 1st Qu.: -1.00337   1st Qu.:0.2573755  \n Median :  0.00748   Median :0.5157779  \n Mean   : -0.03336   Mean   :0.5052159  \n 3rd Qu.:  0.97575   3rd Qu.:0.7559902  \n Max.   : 47.01623   Max.   :0.9994496  \n NA's   :39                             "},{"path":"WCGS-Study.html","id":"are-the-sbps-normally-distributed","chapter":"15 The WCGS","heading":"15.2 Are the SBPs Normally Distributed?","text":"Consider question whether distribution systolic blood pressure results well-approximated Normal.Since data contain sbp lnsbp (natural logarithm), let’s compare . Note preparing graph, ’ll need change location text annotation.can also look Normal Q-Q plots, instance…’s best small improvement sbp lnsbp terms approximation Normal distribution.","code":"\nres <- mosaic::favstats(~ sbp, data = wcgs)\nbin_w <- 5 # specify binwidth\n\nggplot(wcgs, aes(x = sbp)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"orchid\", \n                   col = \"blue\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"navy\") +\n    labs(title = \"Systolic BP for `wcgs` subjects\",\n     x = \"Systolic BP (mm Hg)\", y = \"\",\n     caption = \"Superimposed Normal model\")\nres <- mosaic::favstats(~ lnsbp, data = wcgs)\nbin_w <- 0.05 # specify binwidth\n\nggplot(wcgs, aes(x = lnsbp)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"orange\", \n                   col = \"blue\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"navy\") +\n    labs(title = \"ln(Systolic BP) for `wcgs` subjects\",\n     x = \"ln(Systolic BP)\", y = \"\",\n     caption = \"Superimposed Normal model\")\np1 <- ggplot(wcgs, aes(sample = sbp)) +\n    geom_qq(color = \"orchid\") + \n    geom_qq_line(color = \"red\") +\n    labs(y = \"Ordered SBP\", title = \"sbp in wcgs\")\n\np2 <- ggplot(wcgs, aes(sample = lnsbp)) +\n    geom_qq(color = \"orange\") + \n    geom_qq_line(color = \"red\") +\n    labs(y = \"Ordered ln(SBP)\", title = \"ln(sbp) in wcgs\")\n\n## next step requires library(patchwork)\n\np1 + p2 + \n    plot_annotation(title = \"Normal Q-Q plots of SBP and ln(SBP) in wcgs\")"},{"path":"WCGS-Study.html","id":"identifying-and-describing-sbp-outliers","chapter":"15 The WCGS","heading":"15.3 Identifying and Describing SBP outliers","text":"looks like ’s outlier (series ) SBP data.maximum value 230, clearly extreme value data set. One way gauge describe observation’s Z score, number standard deviations away mean observation falls. , maximum value, 230 6.71 standard deviations mean, thus Z score 6.7.negative Z score indicate point mean, positive Z score indicates, ’ve seen, point mean. minimum systolic blood pressure, 98 2.03 standard deviations mean, Z score -2.Recall Empirical Rule suggests variable follows Normal distribution, approximately 95% observations falling inside Z score (-2, 2), 99.74% falling inside Z score range (-3, 3). systolic blood pressures appear Normally distributed?","code":"\nggplot(wcgs, aes(x = \"\", y = sbp)) +\n    geom_violin() +\n    geom_boxplot(width = 0.3, fill = \"royalblue\", \n                 outlier.color = \"royalblue\") +\n    labs(title = \"Boxplot with Violin of SBP in `wcgs` data\",\n         y = \"Systolic Blood Pressure (mm Hg)\", \n         x = \"\") +\n    coord_flip() \nwcgs %$% Hmisc::describe(sbp)sbp \n       n  missing distinct     Info     Mean      Gmd \n    3154        0       62    0.996    128.6    16.25 \n     .05      .10      .25      .50      .75      .90 \n     110      112      120      126      136      148 \n     .95 \n     156 \n\nlowest :  98 100 102 104 106, highest: 200 208 210 212 230"},{"path":"WCGS-Study.html","id":"does-weight-category-relate-to-sbp","chapter":"15 The WCGS","heading":"15.4 Does Weight Category Relate to SBP?","text":"data collected four groups based subject’s weight (pounds).","code":"\nggplot(wcgs, aes(x = wghtcat, y = sbp)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = wghtcat), width = 0.3, notch = TRUE) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") + \n    labs(title = \"Boxplot of Systolic BP by Weight Category in WCGS\", \n         x = \"Weight Category\", y = \"Systolic Blood Pressure\")"},{"path":"WCGS-Study.html","id":"re-leveling-a-factor","chapter":"15 The WCGS","heading":"15.5 Re-Leveling a Factor","text":"Well, ’s good. really want weight categories (levels) ordered sensibly.Like factor variables R, categories specified levels. want change order levels new version factor variable make sense. multiple ways , prefer fct_relevel function forcats package (part tidyverse.) order appropriate?’ll add new variable wcgs data called weight_f relevels wghtcat data.forcats package, check Grolemund Wickham,35 especially Section Factors.","code":"\nwcgs %>% tabyl(wghtcat) wghtcat    n    percent\n 170-200 1171 0.37127457\n 140-170 1538 0.48763475\n   > 200  213 0.06753329\n   < 140  232 0.07355739\nwcgs <- wcgs %>%\n    mutate(weight_f = fct_relevel(wghtcat, \"< 140\", \"140-170\", \"170-200\", \"> 200\"))\n\nwcgs %>% tabyl(weight_f) weight_f    n    percent\n    < 140  232 0.07355739\n  140-170 1538 0.48763475\n  170-200 1171 0.37127457\n    > 200  213 0.06753329"},{"path":"WCGS-Study.html","id":"sbp-by-weight-category","chapter":"15 The WCGS","heading":"15.5.1 SBP by Weight Category","text":"might see details well ridgeline plot, .plots suggest, patients heavier groups generally higher systolic blood pressures.","code":"\nggplot(wcgs, aes(x = weight_f, y = sbp, fill = weight_f)) +\n    geom_boxplot(notch = TRUE) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Systolic Blood Pressure by Reordered Weight Category in WCGS\", \n         x = \"Weight Category\", y = \"Systolic Blood Pressure\")\nwcgs %>%\n    ggplot(aes(x = sbp, y = weight_f, fill = weight_f, height = ..density..)) +\n    ggridges::geom_density_ridges(scale = 2) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"SBP by Weight Category (wcgs)\",\n         x = \"Systolic Blood Pressure\",\n         y = \"Weight Category\") Picking joint bandwidth of 3.74\nmosaic::favstats(sbp ~ weight_f, data = wcgs)  weight_f min  Q1 median  Q3 max     mean       sd    n\n1    < 140  98 112    120 130 196 123.1379 14.73394  232\n2  140-170 100 118    124 134 192 126.2939 13.65294 1538\n3  170-200 100 120    130 140 230 131.1136 15.57024 1171\n4    > 200 110 126    132 150 212 137.8685 16.75522  213\n  missing\n1       0\n2       0\n3       0\n4       0"},{"path":"WCGS-Study.html","id":"are-weight-and-sbp-linked","chapter":"15 The WCGS","heading":"15.6 Are Weight and SBP Linked?","text":"Let’s build scatter plot SBP (Outcome) Weight (Predictor), rather breaking categories.mass data hidden us - showing 3154 points one plot can produce little blur lots points top .least squares regression line (red), loess scatterplot smoother, (blue) can help.relationship systolic blood pressure weight appears close linear, course considerable scatter around generally linear relationship. turns Pearson correlation two variables 0.253.","code":"\nggplot(wcgs, aes(x = weight, y = sbp)) +\n    geom_point(size=3, shape=1, color=\"forestgreen\") + ## default size = 2\n    stat_smooth(method=lm, color=\"red\") + ## add se=FALSE to hide conf. interval\n    stat_smooth(method=loess, se=FALSE, color=\"blue\") +\n    ggtitle(\"SBP vs. Weight in 3,154 WCGS Subjects\") "},{"path":"WCGS-Study.html","id":"sbp-and-weight-by-arcus-senilis-groups","chapter":"15 The WCGS","heading":"15.7 SBP and Weight by Arcus Senilis groups?","text":"issue interest us assess whether SBP-Weight relationship see similar among subjects arcus senilis .Arcus senilis old age syndrome white, grey, blue opaque ring corneal margin (peripheral corneal opacity), white ring front periphery iris. present birth fades; however, quite commonly present elderly. can also appear earlier life result hypercholesterolemia.Wikipedia article Arcus Senilis, retrieved 2017-08-15Let’s start quick look arcus data.2 missing values, probably want something plotting data, may also want create factor variable meaningful labels 1 (means yes, arcus senilis present) 0 (means , isn’t.)Let’s build version wcgs data eliminates missing data variables immediate interest, plot SBP-weight relationship groups patients without arcus senilis.","code":"\nwcgs %>% tabyl(arcus) arcus    n      percent valid_percent\n     0 2211 0.7010145847     0.7014594\n     1  941 0.2983512999     0.2985406\n    NA    2 0.0006341154            NA\nwcgs <- wcgs %>%\n    mutate(arcus_f = fct_recode(factor(arcus),\n                                \"Arcus senilis\" = \"1\",\n                                \"No arcus senilis\" = \"0\"),\n           arcus_f = fct_relevel(arcus_f, \"Arcus senilis\"))\n\nwcgs %>% tabyl(arcus_f, arcus)          arcus_f    0   1 NA_\n    Arcus senilis    0 941   0\n No arcus senilis 2211   0   0\n             <NA>    0   0   2\nwcgs %>%\n    filter(complete.cases(arcus_f, sbp, weight)) %>%\n    ggplot(aes(x = weight, y = sbp, group = arcus_f)) +\n    geom_point(shape = 1) + \n    stat_smooth(method=lm, color=\"red\") +\n    stat_smooth(method=loess, se=FALSE, color=\"blue\") +\n    labs(title = \"SBP vs. Weight by Arcus Senilis status\",\n         caption = \"3,152 Western Collaborative Group Study subjects with known arcus senilis status\") + \n    facet_wrap(~ arcus_f) "},{"path":"WCGS-Study.html","id":"linear-model-for-sbp-weight-relationship-subjects-without-arcus-senilis","chapter":"15 The WCGS","heading":"15.8 Linear Model for SBP-Weight Relationship: subjects without Arcus Senilis","text":"linear model 2211 patients without Arcus Senilis R-squared = 6.87%.regression equation 95.92 - 0.19 weight, patients without Arcus Senilis.","code":"\nmodel.noarcus <- \n    lm(sbp ~ weight, data = filter(wcgs, arcus == 0))\n\ntidy(model.noarcus) %>% kable(digits = 2)\nglance(model.noarcus) %>% select(r.squared:p.value, AIC) %>% kable(digits = 3)\nsummary(model.noarcus)\nCall:\nlm(formula = sbp ~ weight, data = filter(wcgs, arcus == 0))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.011 -10.251  -2.447   7.553  99.848 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  95.9219     2.5552   37.54   <2e-16 ***\nweight        0.1902     0.0149   12.77   <2e-16 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.8 on 2209 degrees of freedom\nMultiple R-squared:  0.0687,    Adjusted R-squared:  0.06828 \nF-statistic:   163 on 1 and 2209 DF,  p-value: < 2.2e-16"},{"path":"WCGS-Study.html","id":"linear-model-for-sbp-weight-relationship-subjects-with-arcus-senilis","chapter":"15 The WCGS","heading":"15.9 Linear Model for SBP-Weight Relationship: subjects with Arcus Senilis","text":"linear model 941 patients Arcus Senilis R-squared = 5.49%.regression equation 101.88 - 0.163 weight, patients Arcus Senilis.","code":"\nmodel.witharcus <- \n    lm(sbp ~ weight, data = filter(wcgs, arcus == 1))\n\ntidy(model.witharcus) %>% kable(digits = 2)\nglance(model.witharcus) %>% select(r.squared:p.value, AIC) %>% kable(digits = 3)\nsummary(model.witharcus)\nCall:\nlm(formula = sbp ~ weight, data = filter(wcgs, arcus == 1))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-30.335  -9.636  -1.961   7.973  76.738 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 101.87847    3.75572  27.126  < 2e-16 ***\nweight        0.16261    0.02201   7.388 3.29e-13 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.19 on 939 degrees of freedom\nMultiple R-squared:  0.05494,   Adjusted R-squared:  0.05393 \nF-statistic: 54.58 on 1 and 939 DF,  p-value: 3.29e-13"},{"path":"WCGS-Study.html","id":"including-arcus-status-in-the-model","chapter":"15 The WCGS","heading":"15.10 Including Arcus Status in the model","text":"actual regression equation setting includes weight, indicator variable (1 = yes, 0 = ) arcus senilis status, product term combining weight 1/0 indicator. 432, ’ll spend substantial time energy discussing product terms, ’ll much 431.Note use product term weight*arcus setup model allow slope weight intercept term model change depending arcus senilis status.\npatient arcus, regression equation SBP = 95.92 + 0.19 weight + 5.96 (1) - 0.028 weight (1) = 101.88 + 0.162 weight.\npatient without arcus senilis, regression equation SBP = 95.92 + 0.19 weight + 5.96 (0) - 0.028 weight (0) = 95.92 + 0.19 weight.\npatient arcus, regression equation SBP = 95.92 + 0.19 weight + 5.96 (1) - 0.028 weight (1) = 101.88 + 0.162 weight.patient without arcus senilis, regression equation SBP = 95.92 + 0.19 weight + 5.96 (0) - 0.028 weight (0) = 95.92 + 0.19 weight.linear model including interaction weight arcus predict sbp 3152 patients known Arcus Senilis status R-squared = 6.6%. , ’ll discuss interaction substantially 432.","code":"\nmodel3 <- lm(sbp ~ weight * arcus, data = filter(wcgs, !is.na(arcus)))\n\ntidy(model3) %>% kable(digits = 2)\nglance(model3) %>% select(r.squared:p.value, AIC) %>% kable(digits = 3)\nsummary(model3)\nCall:\nlm(formula = sbp ~ weight * arcus, data = filter(wcgs, !is.na(arcus)))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-30.335 -10.152  -2.349   7.669  99.848 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  95.92190    2.52440  37.998   <2e-16 ***\nweight        0.19017    0.01472  12.921   <2e-16 ***\narcus         5.95657    4.61972   1.289    0.197    \nweight:arcus -0.02756    0.02703  -1.019    0.308    \n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.62 on 3148 degrees of freedom\nMultiple R-squared:  0.06595,   Adjusted R-squared:  0.06506 \nF-statistic: 74.09 on 3 and 3148 DF,  p-value: < 2.2e-16"},{"path":"WCGS-Study.html","id":"predictions-from-these-linear-models","chapter":"15 The WCGS","heading":"15.11 Predictions from these Linear Models","text":"predicted SBP subject weighing 175 pounds?change subject weighs 200 pounds?Recall thatWithout Arcus Senilis, linear model SBP = 95.9 + 0.19 x weightWith Arcus Senilis, linear model SBP = 101.9 + 0.16 x weightSo predictions 175 pound subject :95.9 + 0.19 x 175 = 129 mm Hg without Arcus Senilis, and95.9 + 0.19 x 175 = 129 mm Hg without Arcus Senilis, and101.9 + 0.16 x 175 = 130 mm Hg Arcus Senilis.101.9 + 0.16 x 175 = 130 mm Hg Arcus Senilis.thus, predictions 200 pound subject :95.9 + 0.19 x 200 = 134 mm Hg without Arcus Senilis, and95.9 + 0.19 x 200 = 134 mm Hg without Arcus Senilis, and101.9 + 0.16 x 200 = 134.4 mm Hg Arcus Senilis.101.9 + 0.16 x 200 = 134.4 mm Hg Arcus Senilis.","code":""},{"path":"WCGS-Study.html","id":"scatterplots-with-facets-across-a-categorical-variable","chapter":"15 The WCGS","heading":"15.12 Scatterplots with Facets Across a Categorical Variable","text":"can use facets ggplot2 show scatterplots across levels categorical variable, like behpat.","code":"\nggplot(wcgs, aes(x = weight, y = sbp, col = behpat)) +\n    geom_point() +\n    facet_wrap(~ behpat) +\n    geom_smooth(method = \"lm\", se = FALSE, \n                formula = y ~ x, col = \"black\") +\n    guides(color = \"none\") +\n    theme(strip.text = element_text(face=\"bold\", size=rel(1.25), color=\"white\"),\n          strip.background = element_rect(fill=\"royalblue\")) +\n    labs(title = \"Scatterplots of SBP vs. Weight within Behavior Pattern\")"},{"path":"WCGS-Study.html","id":"scatterplot-and-correlation-matrices","chapter":"15 The WCGS","heading":"15.13 Scatterplot and Correlation Matrices","text":"scatterplot matrix can helpful understanding relationships multiple variables simultaneously. several ways build thing, including pairs function…","code":"\npairs (~ sbp + age + weight + height, data=wcgs, main=\"Simple Scatterplot Matrix\")"},{"path":"WCGS-Study.html","id":"displaying-a-correlation-matrix","chapter":"15 The WCGS","heading":"15.13.1 Displaying a Correlation Matrix","text":"","code":"\nwcgs %>%\n    dplyr::select(sbp, age, weight, height) %>%\n    cor() %>% # obtain correlation coefficients for this subgroup\n    signif(., 3) # round them off to three significant figures before printing          sbp     age  weight  height\nsbp    1.0000  0.1660  0.2530  0.0184\nage    0.1660  1.0000 -0.0344 -0.0954\nweight 0.2530 -0.0344  1.0000  0.5330\nheight 0.0184 -0.0954  0.5330  1.0000"},{"path":"WCGS-Study.html","id":"using-the-ggally-package","chapter":"15 The WCGS","heading":"15.13.2 Using the GGally package","text":"ggplot2 system doesn’t built-scatterplot system. nice add-ins world, though. One option sort like GGally package, can produce correlation matrices scatterplot matrices.ggpairs function provides density plot diagonal, Pearson correlations upper right scatterplots lower left matrix.","code":"\nGGally::ggpairs(wcgs %>% select(sbp, age, weight, height), \n                title = \"Scatterplot Matrix via ggpairs\")"},{"path":"confidence-intervals-for-a-mean.html","id":"confidence-intervals-for-a-mean","chapter":"16 Confidence Intervals for a Mean","heading":"16 Confidence Intervals for a Mean","text":"","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"love-boost.r-is-something-well-start-using-now","chapter":"16 Confidence Intervals for a Mean","heading":"16.1 Love-boost.R is something we’ll start using now","text":"part course, ’ll make use scripts ’ve gathered .","code":"\nsource(\"data/Love-boost.R\")"},{"path":"confidence-intervals-for-a-mean.html","id":"introduction","chapter":"16 Confidence Intervals for a Mean","heading":"16.2 Introduction","text":"basic theory estimation can used indicate probable accuracy potential bias estimating based limited samples. point estimate provides single best guess value population process parameter.confidence interval particularly useful way convey people just much error one must allow given estimate. particular, confidence interval allows us quantify just close expect, instance, sample mean population process mean. computer calculations; need interpret results.key things need trade cost vs. precision, precision vs. confidence correctness statement. Often, dissatisfied width confidence interval want make smaller, little choice reconsider sample – larger samples produce shorter intervals.","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"this-chapters-goals","chapter":"16 Confidence Intervals for a Mean","heading":"16.3 This Chapter’s Goals","text":"Suppose interested learning something population process, can obtain sample consists subset potential results population process. main goal many parametric models large part statistics estimate population parameters, like population mean, regression coefficient, basis sample. , want describe best guess parameter (referred point estimate) also say something useful uncertainty estimate, let us completely assess data tell us. key tool confidence interval.Essentially every textbook introductory statistics describes development confidence interval, least mean. Good supplemental resources highlighted references ’ve provided course syllabus.’ll develop confidence intervals compare parameters two populations (either matched pairs independent samples) confidence intervals soon. , ’ll consider problem estimating confidence interval describe mean (median) population represented single sample quantitative data.","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"serum-zinc-levels-in-462-teenage-males-serzinc","chapter":"16 Confidence Intervals for a Mean","heading":"16.4 Serum Zinc Levels in 462 Teenage Males (serzinc)","text":"serzinc data include serum zinc levels micrograms per deciliter gathered sample 462 males aged 15-17, source data Appendix B1 Marcello Pagano Kimberlee Gauvreau.36 Serum zinc deficiency associated anemia, loss strength endurance, thought 25% world’s population risk zinc deficiency. deficiency can indicate poor nutrition, can affect growth vision, instance. “Typical” values37 said 0.66-1.10 mcg/ml, 66 - 110 micrograms per deciliter.","code":"\nserzinc <- read_csv(\"data/serzinc.csv\")Rows: 462 Columns: 2-- Column specification ------------------------------------\nDelimiter: \",\"\nchr (1): ID\ndbl (1): zinc\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\nsummary(serzinc)      ID                 zinc       \n Length:462         Min.   : 50.00  \n Class :character   1st Qu.: 76.00  \n Mode  :character   Median : 86.00  \n                    Mean   : 87.94  \n                    3rd Qu.: 98.00  \n                    Max.   :153.00  "},{"path":"confidence-intervals-for-a-mean.html","id":"our-goal-a-confidence-interval-for-the-population-mean","chapter":"16 Confidence Intervals for a Mean","heading":"16.5 Our Goal: A Confidence Interval for the Population Mean","text":"assess data bit, satisfied understand , first inferential goal produce confidence interval true (population) mean males age 15-17 based sample, assuming 462 males random sample population interest, serum zinc level drawn independently identical distribution describing population., several different procedures available, including:confidence interval population mean based t distribution, assume data drawn approximately Normal distribution, using sample standard deviation. (Interval corresponding t test, good choice data really approximately Normally distributed.)resampling approach generate bootstrap confidence interval population mean, require assume either population standard deviation known, data drawn approximately Normal distribution, weaknesses.rank-based procedure called Wilcoxon signed rank test can also used yield confidence interval statement population pseudo-median, measure population distribution’s center (population’s mean).","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"exploratory-data-analysis-for-serum-zinc","chapter":"16 Confidence Intervals for a Mean","heading":"16.6 Exploratory Data Analysis for Serum Zinc","text":"","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"graphical-summaries","chapter":"16 Confidence Intervals for a Mean","heading":"16.6.1 Graphical Summaries","text":"code presented builds:histogram (Normal model superimposed),boxplot (median notch) anda Normal Q-Q plot (guiding straight line quartiles)zinc results serzinc tibble.results include useful plots numerical summaries assessing shape, center spread. zinc data serzinc data frame appear slightly right skewed, five outlier values high end scale, particular.","code":"\np1 <- ggplot(serzinc, aes(sample = zinc)) +\n  geom_qq(col = \"dodgerblue\") + geom_qq_line(col = \"navy\") + \n  theme(aspect.ratio = 1) + \n  labs(title = \"Normal Q-Q plot\")\n\np2 <- ggplot(serzinc, aes(x = zinc)) +\n  geom_histogram(aes(y = stat(density)), \n                 bins = 10, fill = \"dodgerblue\", col = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(serzinc$zinc), \n                            sd = sd(serzinc$zinc)),\n                col = \"navy\", lwd = 1.5) +\n  labs(title = \"Histogram and Normal Density\")\n\np3 <- ggplot(serzinc, aes(x = zinc, y = \"\")) +\n  geom_boxplot(fill = \"dodgerblue\", outlier.color = \"dodgerblue\") + \n  labs(title = \"Boxplot\", y = \"\")\n\np1 + (p2 / p3 + plot_layout(heights = c(4,1))) +\n  plot_annotation(title = \"Serum Zinc (micrograms per deciliter) for 462 Teenage Males\")"},{"path":"confidence-intervals-for-a-mean.html","id":"numerical-summaries","chapter":"16 Confidence Intervals for a Mean","heading":"16.6.2 Numerical Summaries","text":"section describes numerical summaries interest augment plots summarizing center, spread shape distribution serum zinc among 462 teenage males.skew1 value (mean - median divided standard deviation) backs graphical assessment, data slightly right skewed.Rounded two decimal places, standard deviation serum zinc data turns 16, standard error mean, shown se psych::describe output, 16 divided square root sample size, n = 462. standard error become quite important us building statistical inferences mean entire population teenage males based sample.","code":"\nmosaic::favstats(~ zinc, data = serzinc) %>% \n    kable(digits = 3)\nserzinc %>%\n    summarize(mean(zinc), median(zinc), sd(zinc),\n              skew1 = (mean(zinc) - median(zinc))/sd(zinc)) %>%\n    kable(digits = 3)\nserzinc %$% psych::describe(zinc)   vars   n  mean sd median trimmed   mad min max range\nX1    1 462 87.94 16     86   87.17 16.31  50 153   103\n   skew kurtosis   se\nX1 0.62     0.87 0.74"},{"path":"confidence-intervals-for-a-mean.html","id":"defining-a-confidence-interval","chapter":"16 Confidence Intervals for a Mean","heading":"16.7 Defining a Confidence Interval","text":"confidence interval population process mean uses data sample (perhaps additional information) identify range potential values population mean, , certain assumptions hold, can assumed provide reasonable estimate true population mean. confidence interval consists :interval estimate describing population parameter interest (population mean), andA probability statement, expressed terms confidence level.","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"estimating-the-population-mean-from-the-serum-zinc-data","chapter":"16 Confidence Intervals for a Mean","heading":"16.8 Estimating the Population Mean from the Serum Zinc data","text":"example, suppose willing assume mean serum zinc level across entire population teenage males, \\(\\mu\\), follows Normal distribution (, summarizing mean rational thing .) Suppose also willing assume 462 teenage males contained serzinc tibble random sample complete population. know mean sample 462 boys, don’t know \\(\\mu\\), mean across teenage males. need estimate .Earlier estimated 90% confidence interval mean serum zinc level (\\(\\mu\\)) across entire population teenage males (86.71, 89.16) micrograms per deciliter. interpret result?people think means 90% chance true mean population, \\(\\mu\\), falls 86.71 89.16 micrograms per deciliter. ’s correct.population mean constant parameter population interest. constant random variable, change. actual probability population mean falling inside range either 0 1.confidence process.\n’s sampling method (random sampling) used generate data, assumption population follows Normal distribution.\n’s captured accounting one particular type error (called sampling error) developing interval estimate, assuming potential sources error negligible.\n’s sampling method (random sampling) used generate data, assumption population follows Normal distribution.’s captured accounting one particular type error (called sampling error) developing interval estimate, assuming potential sources error negligible., ’s closer truth :used method sample data true population teenage males, built 100 90% confidence intervals, 90 contain true population mean.","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"confidence-vs.-significance-level","chapter":"16 Confidence Intervals for a Mean","heading":"16.9 Confidence vs. Significance Level","text":"’ve estimated 90% confidence interval population mean serum zinc level among teenage boys using serzinc data.call 100(1-\\(\\alpha\\))%, , 90%, 0.90, confidence level, \\(\\alpha\\) = 10%, 0.10 called significance level.instead built series 100 different 95% confidence intervals, 95 contain true value \\(\\mu\\).Let’s look closely issue estimating population mean based sample observations. need three critical pieces - sample, confidence level, margin error, based standard error sample mean, estimating population mean.","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"the-standard-error-of-a-sample-mean","chapter":"16 Confidence Intervals for a Mean","heading":"16.10 The Standard Error of a Sample Mean","text":"standard error, generally, name give standard deviation associated particular parameter estimate.using sample mean based sample size \\(n\\) estimate population mean, standard error sample mean standard deviation measurements population, divided square root sample size.using sample mean based sample size \\(n\\) estimate population mean, standard error sample mean standard deviation measurements population, divided square root sample size.often estimate particular standard error \\(s\\) (sample standard deviation) divided square root sample size.often estimate particular standard error \\(s\\) (sample standard deviation) divided square root sample size.statistics different standard errors.\n\\(\\sqrt{p (1-p) / n}\\) standard error sample proportion \\(p\\) estimated using sample size \\(n\\).\n\\(\\sqrt{\\frac{1-r^2}{n-2}}\\) standard error sample Pearson correlation \\(r\\) estimated using \\(n\\) pairs observations.\n\\(\\sqrt{\\frac{SD_1^2}{n_1} + \\frac{SD_2^2}{n_2}}\\) standard error difference two means \\(\\bar{x}_1\\) \\(\\bar{x}_2\\), estimated using samples sizes \\(n_1\\) \\(n_2\\) sample standard devistions \\(SD_1\\) \\(SD_2\\), respectively.\nstatistics different standard errors.\\(\\sqrt{p (1-p) / n}\\) standard error sample proportion \\(p\\) estimated using sample size \\(n\\).\\(\\sqrt{\\frac{1-r^2}{n-2}}\\) standard error sample Pearson correlation \\(r\\) estimated using \\(n\\) pairs observations.\\(\\sqrt{\\frac{SD_1^2}{n_1} + \\frac{SD_2^2}{n_2}}\\) standard error difference two means \\(\\bar{x}_1\\) \\(\\bar{x}_2\\), estimated using samples sizes \\(n_1\\) \\(n_2\\) sample standard devistions \\(SD_1\\) \\(SD_2\\), respectively.developing confidence interval population mean, may willing assume data sample drawn Normally distributed population. , common useful means building confidence interval makes use t distribution (sometimes called Student’s t) notion standard error.","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"the-t-distribution-and-cis-for-a-mean","chapter":"16 Confidence Intervals for a Mean","heading":"16.11 The t distribution and CIs for a Mean","text":"practical settings, use t distribution estimate confidence interval population mean whenever :willing assume sample drawn random population process Normal distribution,using sample estimate mean standard deviation, andhave small sample size.","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"the-formula","chapter":"16 Confidence Intervals for a Mean","heading":"16.11.1 The Formula","text":"two-sided \\(100(1-\\alpha)\\)% confidence interval (based \\(t\\) test) :\\[\\bar{x} \\pm t_{\\alpha/2, n-1}(s / \\sqrt{n})\\]\\(t_{\\alpha/2, n-1}\\) value cuts top \\(\\alpha/2\\) percent \\(t\\) distribution, \\(n - 1\\) degrees freedom.obtain relevant cutoff value R substituting values alphaover2 n-1 following line R code:qt(alphaover2, df = n-1, lower.tail=FALSE)","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"students-t-distribution","chapter":"16 Confidence Intervals for a Mean","heading":"16.11.2 Student’s t distribution","text":"Student’s t distribution looks lot like Normal distribution, sample size large. Unlike normal distribution, specified two parameters, mean standard deviation, t distribution specified one parameter, degrees freedom.t distributions large numbers degrees freedom less indistinguishable standard Normal distribution.t distributions smaller degrees freedom (say, df < 30, particular) still symmetric, outlier-prone Normal distribution","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"building-the-ci-in-r","chapter":"16 Confidence Intervals for a Mean","heading":"16.12 Building the CI in R","text":"Suppose wish build 90% confidence interval true mean serum zinc level across entire population teenage males. confidence level 90%, 0.90, \\(\\alpha\\) value, 1 - confidence = 0.10.know going :want \\(\\alpha\\) = 0.10, ’re creating 90% confidence interval.sample size n = 462 serum zinc measurements.sample mean measurements, \\(\\bar{x}\\) = 87.937 micrograms per deciliter.sample standard deviation measurements, s = 16.005 micrograms per deciliter.","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"using-an-intercept-only-regression-model","chapter":"16 Confidence Intervals for a Mean","heading":"16.13 Using an intercept-only regression model","text":"context fitting intercept-linear regression model. intercept-model fitted putting number 1 right hand side linear model. resulting model simply fits overall mean data prediction subjects.Generally, though, ’ll use tidy() function broom obtain key information model like :alternative, also use t.test function, can build (case) two-sided confidence interval zinc levels like :tidy() function broom package works , ., 90% confidence interval true population mean serum zinc level, based sample 462 patients, (86.71, 89.16) micrograms per deciliter38.","code":"\nmodel_zinc <- lm(zinc ~ 1, data = serzinc)\nsummary(model_zinc)\nCall:\nlm(formula = zinc ~ 1, data = serzinc)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.937 -11.937  -1.937  10.063  65.063 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  87.9372     0.7446   118.1   <2e-16 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16 on 461 degrees of freedom\nconfint(model_zinc, level = 0.90)              5 %     95 %\n(Intercept) 86.71 89.16446\ntidy(model_zinc, conf.int = TRUE, conf = 0.90) %>%\n    kable(digits = 2)\ntt <- t.test(serzinc$zinc, \n             conf.level = 0.90, \n             alternative = \"two.sided\")\n\ntt\n    One Sample t-test\n\ndata:  serzinc$zinc\nt = 118.1, df = 461, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 86.71000 89.16446\nsample estimates:\nmean of x \n 87.93723 \n# requires library(broom)\ntidy(tt, conf.int = TRUE, conf = 0.90) %>% \n  knitr::kable(digits = 2)"},{"path":"confidence-intervals-for-a-mean.html","id":"interpreting-the-result","chapter":"16 Confidence Intervals for a Mean","heading":"16.14 Interpreting the Result","text":"appropriate interpretation 90% two-sided confidence interval follows:(86.71, 89.16) micrograms per deciliter 90% two-sided confidence interval population mean serum zinc level among teenage males.point estimate true population mean serum zinc level 87.94. values interval (86.71, 89.16) represent reasonable range estimates true population mean serum zinc level, 90% confident method creating confidence interval produce result containing true population mean serum zinc level.draw 100 samples size 462 population described sample, use sample produce confidence interval manner, approximately 90 confidence intervals cover true population mean serum zinc level.","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"what-if-we-want-a-95-or-99-confidence-interval-instead","chapter":"16 Confidence Intervals for a Mean","heading":"16.15 What if we want a 95% or 99% confidence interval instead?","text":"can obtain using tidy modeling approach.","code":"\ntidy(model_zinc, conf.int = TRUE, conf.level = 0.95)# A tibble: 1 x 7\n  term        estimate std.error statistic p.value conf.low\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>    <dbl>\n1 (Intercept)     87.9     0.745      118.       0     86.5\n# ... with 1 more variable: conf.high <dbl>\ntidy(model_zinc, conf.int = TRUE, conf.level = 0.99)# A tibble: 1 x 7\n  term        estimate std.error statistic p.value conf.low\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>    <dbl>\n1 (Intercept)     87.9     0.745      118.       0     86.0\n# ... with 1 more variable: conf.high <dbl>"},{"path":"confidence-intervals-for-a-mean.html","id":"using-the-broom-package-with-the-t-test","chapter":"16 Confidence Intervals for a Mean","heading":"16.16 Using the broom package with the t test","text":"broom package takes messy output built-functions R, lm, t.test wilcox.test, turns tidy data frames. detailed description package three key functions found https://github.com/tidyverse/broom.example, can use tidy function within broom create single-row tibble key results t test.can thus pull endpoints 95% confidence interval directly output. broom also glance function, returns information tidy case t-test.","code":"\ntt <- t.test(serzinc$zinc, conf.level = 0.95, alternative = \"two.sided\")\ntidy(tt)# A tibble: 1 x 8\n  estimate statistic p.value parameter conf.low conf.high\n     <dbl>     <dbl>   <dbl>     <dbl>    <dbl>     <dbl>\n1     87.9      118.       0       461     86.5      89.4\n# ... with 2 more variables: method <chr>,\n#   alternative <chr>"},{"path":"confidence-intervals-for-a-mean.html","id":"effect-of-changing-the-confidence-level","chapter":"16 Confidence Intervals for a Mean","heading":"16.16.1 Effect of Changing the Confidence Level","text":", see two-sided confidence intervals various levels \\(\\alpha\\).happens width confidence interval table confidence level changes?","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"one-sided-vs.-two-sided-confidence-intervals","chapter":"16 Confidence Intervals for a Mean","heading":"16.17 One-sided vs. Two-sided Confidence Intervals","text":"Occasionally, want estimate either upper limit population mean \\(\\mu\\), lower limit \\(\\mu\\), .Note relationship two-sided 80% confidence interval, one-sided 90% confidence intervals.happen? 80% two-sided interval placed cut top 10% distribution upper bound, bottom 10% distribution lower bound. 90% “less ” one-sided interval placed lower bound cut top 10% distribution.issue appears consider two-sided 90% one-sided 95% confidence intervals., 90% two-sided interval cuts top 5% bottom 5% distribution bounds. 95% “less ” one-sided interval also lower bound cut top 5% distribution.","code":"\nt.test(serzinc$zinc, conf.level = 0.90, alternative = \"greater\")\n    One Sample t-test\n\ndata:  serzinc$zinc\nt = 118.1, df = 461, p-value < 2.2e-16\nalternative hypothesis: true mean is greater than 0\n90 percent confidence interval:\n 86.98161      Inf\nsample estimates:\nmean of x \n 87.93723 \nt.test(serzinc$zinc, conf.level = 0.90, alternative = \"less\")\n    One Sample t-test\n\ndata:  serzinc$zinc\nt = 118.1, df = 461, p-value = 1\nalternative hypothesis: true mean is less than 0\n90 percent confidence interval:\n     -Inf 88.89285\nsample estimates:\nmean of x \n 87.93723 "},{"path":"confidence-intervals-for-a-mean.html","id":"bootstrap-confidence-intervals","chapter":"16 Confidence Intervals for a Mean","heading":"16.18 Bootstrap Confidence Intervals","text":"bootstrap (particular, ’s known bootstrap resampling) really good idea know little bit .want know accurately sample mean estimates population mean, ideally like take , large sample, , conclude something eventually approach mathematical certainty sample mean close population mean.can rarely draw enormous samples. can ?","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"resampling-is-a-big-idea","chapter":"16 Confidence Intervals for a Mean","heading":"16.19 Resampling is A Big Idea","text":"One way find precise estimates run multiple samples size. resampling approach codified originally Brad Efron 1979.Oversimplifying lot, idea sample (replacement) current sample, can draw new sample size original.repeat many times, can generate many samples , say, 462 zinc levels, like.take thousands samples calculate (instance) sample mean , plot histogram means.cut top bottom 5% sample means, obtain reasonable 90% confidence interval population mean.","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"when-is-a-bootstrap-confidence-interval-reasonable","chapter":"16 Confidence Intervals for a Mean","heading":"16.20 When is a Bootstrap Confidence Interval Reasonable?","text":"bootstrapped interval estimate population mean, \\(\\mu\\), reasonable long ’re willing believe :original sample random sample (least completely representative sample) population,samples independent ,even population interest doesn’t follow Normal, even symmetric distribution.downside bootstrap get (somewhat) different answers resample data without setting random seed.","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"bootstrap-confidence-interval-for-the-mean-process","chapter":"16 Confidence Intervals for a Mean","heading":"16.21 Bootstrap confidence interval for the mean: Process","text":"avoid Normality assumption, take advantage modern computing power, use R obtain bootstrap confidence interval population mean based sample.computer :Re-sample data replacement, obtains new sample equal size original data set.Calculates statistic interest (, sample mean.)Repeat steps many times (default 1,000 using approach) obtain set 1,000 sample means.Sort 1,000 sample means order, estimate 95% confidence interval population mean based middle 95% 1,000 bootstrap samples.Send us result, containing sample mean, 95% confidence interval population mean","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"using-r-to-estimate-a-bootstrap-ci","chapter":"16 Confidence Intervals for a Mean","heading":"16.22 Using R to estimate a bootstrap CI","text":"command use obtain Confidence Interval \\(\\mu\\) using basic nonparametric bootstrap without assuming Normally distributed population, smean.cl.boot, part Hmisc package R.Remember t-based 90% CI \\(\\mu\\) (86.71, 89.16), according following output…","code":"\nset.seed(431)\nserzinc %$% Hmisc::smean.cl.boot(zinc, B = 1000, conf.int = 0.90)    Mean    Lower    Upper \n87.93723 86.76775 89.20617 \ntidy(lm(zinc ~ 1, data = serzinc), conf.int = TRUE, conf.level = 0.90)# A tibble: 1 x 7\n  term        estimate std.error statistic p.value conf.low\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>    <dbl>\n1 (Intercept)     87.9     0.745      118.       0     86.7\n# ... with 1 more variable: conf.high <dbl>"},{"path":"confidence-intervals-for-a-mean.html","id":"comparing-bootstrap-and-t-based-confidence-intervals","chapter":"16 Confidence Intervals for a Mean","heading":"16.23 Comparing Bootstrap and T-Based Confidence Intervals","text":"smean.cl.boot function (unlike R functions) deletes missing data automatically, smean.cl.normal function, can also used produce t-based confidence interval.Bootstrap resampling confidence intervals follow general confidence interval strategy using point estimate plus minus margin error.bootstrap interval often asymmetric, generally point estimate (sample mean) near center, highly skewed data, necessarily case.usually use either 1,000 (default) 10,000 bootstrap replications building confidence intervals – practically, makes little difference.","code":"\nset.seed(431)\nserzinc %$% Hmisc::smean.cl.boot(zinc, B = 1000, conf.int = 0.90)    Mean    Lower    Upper \n87.93723 86.76775 89.20617 \nserzinc %$% Hmisc::smean.cl.normal(zinc, conf.int = 0.90)    Mean    Lower    Upper \n87.93723 86.71000 89.16446 "},{"path":"confidence-intervals-for-a-mean.html","id":"bootstrap-resampling-advantages-and-caveats","chapter":"16 Confidence Intervals for a Mean","heading":"16.23.1 Bootstrap Resampling: Advantages and Caveats","text":"bootstrap may seem like solution estimation problems. theory, use approach find confidence interval parameter – ’s perfect, useful. Bootstrap procedures exist virtually statistical comparison - t-test analog just one many possibilities, bootstrap methods rapidly gaining traditional approaches literature thanks mostly faster computers.great advantage bootstrap relative simplicity, don’t forget many original assumptions t-based confidence interval still hold.Using bootstrap eliminate need worry Normality assumption small sample size settings, still requires independent identically distributed samples population interest.bootstrap produces clean robust inferences (confidence intervals) many tricky situations. still possible results can :inaccurate (.e. can include true value unknown population mean less often stated confidence probability) andimprecise (.e., can include extraneous values unknown population mean desirable).","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"using-the-bootstrap-to-develop-other-cis","chapter":"16 Confidence Intervals for a Mean","heading":"16.24 Using the Bootstrap to develop other CIs","text":"","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"changing-the-confidence-level","chapter":"16 Confidence Intervals for a Mean","heading":"16.24.1 Changing the Confidence Level","text":"wanted change confidence level?","code":"\nset.seed(431654)\nserzinc %$% Hmisc::smean.cl.boot(zinc, B = 1000, conf.int = 0.95)    Mean    Lower    Upper \n87.93723 86.51066 89.42002 \nset.seed(431321)\nserzinc %$% Hmisc::smean.cl.boot(zinc, B = 1000, conf.int = 0.99)    Mean    Lower    Upper \n87.93723 86.20657 89.68619 "},{"path":"confidence-intervals-for-a-mean.html","id":"one-tailed-bootstrap-confidence-intervals","chapter":"16 Confidence Intervals for a Mean","heading":"16.25 One-Tailed Bootstrap Confidence Intervals","text":"want estimate one tailed confidence interval population mean using bootstrap, procedure follows:Determine \\(\\alpha\\), significance level want use one-sided confidence interval. Remember \\(\\alpha\\) 1 minus confidence level. Let’s assume want 90% one-sided interval, \\(\\alpha\\) = 0.10.Double \\(\\alpha\\) determine significance level use next step fit two-sided confidence interval.Fit two-sided confidence interval confidence level \\(100(1 - 2*\\alpha)\\). Let bounds interval (, b).one-sided (greater ) confidence interval lower bound.one-sided (less ) confidence interval b upper bound.Suppose want find 95% one-sided upper bound population mean serum zinc level among teenage males, \\(\\mu\\), using bootstrap.Since want 95% confidence interval, \\(\\alpha\\) = 0.05. double get \\(\\alpha\\) = 0.10, implies need instead fit two-sided 90% confidence interval.upper bound two-sided 90% CI also upper bound 95% one-sided CI.","code":"\nset.seed(43101)\nserzinc %$% Hmisc::smean.cl.boot(zinc, B = 1000, conf.int = 0.90)    Mean    Lower    Upper \n87.93723 86.70509 89.11266 "},{"path":"confidence-intervals-for-a-mean.html","id":"bootstrap-ci-for-the-population-median","chapter":"16 Confidence Intervals for a Mean","heading":"16.25.1 Bootstrap CI for the Population Median","text":"willing small amount programming work R, can obtain bootstrap confidence intervals population parameters besides mean. One statistic common interest median. find confidence interval population median using bootstrap approach? easiest way know makes use boot package, follows.step 1, specify new function capture medians sample.step 2, summon boot package call boot.ci function.yields 90% confidence interval population median serum zinc level. Recall sample median serum zinc levels sample 462 teenage males 86 micrograms per deciliter.Actually, boot.ci function can provide five different types confidence interval (see help file) change type=\"\", versions attractive properties. However, ’ll stick basic approach 431.","code":"\nf.median <- function(y, id) \n{    median ( y[id])  }\nset.seed(431787)\nboot::boot.ci(boot::boot (serzinc$zinc, f.median, 1000), conf=0.90, type=\"basic\")BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot::boot.ci(boot.out = boot::boot(serzinc$zinc, f.median, 1000), \n    conf = 0.9, type = \"basic\")\n\nIntervals : \nLevel      Basic         \n90%   (84, 87 )  \nCalculations and Intervals on Original Scale\nmosaic::favstats(~ zinc, data = serzinc) min Q1 median Q3 max     mean       sd   n missing\n  50 76     86 98 153 87.93723 16.00469 462       0"},{"path":"confidence-intervals-for-a-mean.html","id":"bootstrap-ci-for-the-iqr","chapter":"16 Confidence Intervals for a Mean","heading":"16.25.2 Bootstrap CI for the IQR","text":"reason, want find 95% confidence interval population value inter-quartile range via bootstrap, can .","code":"\nIQR(serzinc$zinc)[1] 22\nf.IQR <- function(y, id) \n{    IQR (y[id]) }\n\nset.seed(431207)\nboot::boot.ci(boot::boot (serzinc$zinc, f.IQR, 1000), \n        conf=0.95, type=\"basic\")BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot::boot.ci(boot.out = boot::boot(serzinc$zinc, f.IQR, 1000), \n    conf = 0.95, type = \"basic\")\n\nIntervals : \nLevel      Basic         \n95%   (20.00, 24.24 )  \nCalculations and Intervals on Original Scale"},{"path":"confidence-intervals-for-a-mean.html","id":"wilcoxon-signed-rank-procedure-for-cis","chapter":"16 Confidence Intervals for a Mean","heading":"16.26 Wilcoxon Signed Rank Procedure for CIs","text":"turns difficult, without bootstrap, estimate appropriate confidence interval median population, might appealing thing , particularly sample data clearly Normally distributed, median seems like better summary center data. Bootstrap procedures available perform task.Wilcoxon signed rank approach can used alternative t-based procedures build interval estimates population pseudo-median population assumed follow Normal distribution.turns , ’re willing assume population symmetric (necessarily Normally distributed) pseudo-median actually equal population median.","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"what-is-a-pseudo-median","chapter":"16 Confidence Intervals for a Mean","heading":"16.26.1 What is a Pseudo-Median?","text":"pseudo-median particular distribution G median distribution (u + v)/2, u v distribution (G).distribution G symmetric, pseudomedian equal median.distribution skewed, pseudomedian median.sample, pseudomedian defined median midpoints pairs observations sample.","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"wilcoxon-signed-rank-based-ci-in-r","chapter":"16 Confidence Intervals for a Mean","heading":"16.27 Wilcoxon Signed Rank-based CI in R","text":"","code":"\nserzinc %$% wilcox.test(zinc, conf.int = TRUE, conf.level = 0.95)\n    Wilcoxon signed rank test with continuity correction\n\ndata:  zinc\nV = 106953, p-value < 2.2e-16\nalternative hypothesis: true location is not equal to 0\n95 percent confidence interval:\n 85.99997 88.50002\nsample estimates:\n(pseudo)median \n      87.49996 "},{"path":"confidence-intervals-for-a-mean.html","id":"interpreting-the-wilcoxon-ci-for-the-population-median","chapter":"16 Confidence Intervals for a Mean","heading":"16.27.1 Interpreting the Wilcoxon CI for the Population Median","text":"’re willing believe zinc levels come population symmetric distribution, 95% Confidence Interval population median (86, 88.5)non-symmetric population, applies pseudo-median.Note pseudo-median (87.5) actually closer sample mean (87.9) sample median (86).","code":""},{"path":"confidence-intervals-for-a-mean.html","id":"using-the-broom-package-with-the-wilcoxon-test","chapter":"16 Confidence Intervals for a Mean","heading":"16.27.2 Using the broom package with the Wilcoxon test","text":"can also use tidy function within broom create single-row tibble key results Wilcoxon test, long run wilcox.test specifying want confidence interval.","code":"\nwt <- serzinc %$% wilcox.test(zinc, conf.int = TRUE, conf.level = 0.95)\ntidy(wt)# A tibble: 1 x 7\n  estimate statistic  p.value conf.low conf.high method     \n     <dbl>     <dbl>    <dbl>    <dbl>     <dbl> <chr>      \n1     87.5    106953 2.00e-77     86.0      88.5 Wilcoxon s~\n# ... with 1 more variable: alternative <chr>"},{"path":"confidence-intervals-for-a-mean.html","id":"general-advice","chapter":"16 Confidence Intervals for a Mean","heading":"16.28 General Advice","text":"described several approaches estimating confidence interval center distribution quantitative data.commonly used approach uses t distribution estimate confidence interval population/process mean. requires extra assumptions, particularly underlying distribution population values least approximately Normally distributed. identical result get intercept-linear regression model.modern general approach uses idea bootstrap estimate confidence population/process parameter, mean, median summary statistic. bootstrap, underlying notion resampling important idea lets us avoid assumptions (particular Normality) required methods. Bootstrap confidence intervals involve random sampling, actual values obtained differ bit across replications.Finally, Wilcoxon signed-rank method one number inferential tools transform data ranks estimating confidence interval. avoids assumptions, yields inferences less-familiar parameter - pseudo-median.time, bootstrap provides reasonably adequate confidence interval estimate population value parameter (mean median, commonly) distribution data consists single sample quantitative information.","code":""},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"the-ibuprofen-in-sepsis-randomized-clinical-trial","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","text":"next study randomized controlled trial comparing ibuprofen vs. placebo patients sepsis, uses independent samples design compare two samples quantitative data. working sample Ibuprofen Sepsis study, also studied William D. Dupont.39 Quoting abstract Gordon R. Bernard et al.40Ibuprofen shown effects sepsis humans, small samples (fewer 30 patients), previous studies inadequate assess effects mortality. sought determine whether ibuprofen can alter rates organ failure mortality patients sepsis syndrome, drug affects increased metabolic demand sepsis (e.g., fever, tachypnea, tachycardia, hypoxemia, lactic acidosis), potential adverse effects drug sepsis syndrome.study, patients meeting specific criteria (including elevated temperature) diagnosis sepsis recruited fulfilled additional set study criteria intensive care unit one seven participating centers.full trial involved 455 patients, sample includes 300. 150 patients randomly assigned Ibuprofen group 150 Placebo group41. picked sepsis sample work excluding patients missing values outcome interest, selected random sample 150 Ibuprofen 150 Placebo patients rest group, converted temperatures changes Fahrenheit Celsius. data gathered sepsis data file.moment, focus two variables:treat, specifies treatment group (intravenous Ibuprofen intravenous Placebo), assigned via randomization patient, andtemp_drop, outcome interest, measured change baseline 2 hours later degrees Celsius. Positive values indicate improvement, , drop temperature 2 hours following baseline measurement.sepsis.csv file also contains subject’sid, just coderace (three levels: White, AfricanA )apache = baseline APACHE II score, severity disease score ranging 0 71 higher scores indicating severe disease higher mortality risktemp_0 = baseline temperature, degrees Celsius.moment, won’t worry .","code":"\nsepsis <- read_csv(\"data/sepsis.csv\")\nsepsis <- sepsis %>%\n    mutate(treat = factor(treat),\n           race = factor(race))\n\nsummary(sepsis)      id                  treat           race    \n Length:300         Ibuprofen:150   AfricanA: 80  \n Class :character   Placebo  :150   Other   : 23  \n Mode  :character                   White   :197  \n                                                  \n                                                  \n                                                  \n     apache         temp_0        temp_drop      \n Min.   : 0.0   Min.   :33.10   Min.   :-2.7000  \n 1st Qu.:10.0   1st Qu.:37.48   1st Qu.:-0.1000  \n Median :14.0   Median :38.20   Median : 0.3000  \n Mean   :15.4   Mean   :38.00   Mean   : 0.3083  \n 3rd Qu.:20.0   3rd Qu.:38.70   3rd Qu.: 0.7000  \n Max.   :35.0   Max.   :41.70   Max.   : 3.1000  "},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"comparing-two-groups","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.1 Comparing Two Groups","text":"making choice two alternatives, questions following become paramount.status quo?standard approach?costs incorrect decisions?costs balanced?process comparing means/medians/proportions/rates populations represented two independently obtained samples can challenging, approach always best choice. Often, specially designed experiments can informative lower cost (.e. smaller sample size). one might expect, using sophisticated procedures introduces trade-offs, costs typically small relative gain information.faced comparison two alternatives, test based paired data often much better test based two distinct, independent samples. ? done experiment properly, pairing lets us eliminate background variation otherwise hides meaningful differences.","code":""},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"model-based-comparisons-and-anovaregression","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.1.1 Model-Based Comparisons and ANOVA/Regression","text":"Comparisons based independent samples quantitative variables also frequently accomplished equivalent methods, including analysis variance approach dummy variable regression, produce identical confidence intervals pooled variance t test comparison.also discuss main ideas developing, designing analyzing statistical experiments, specifically terms making comparisons. ideas present section allow comparison two populations terms population means. statistical techniques employed analyze sample variance order test estimate population means reason method called analysis variance (ANOVA), discuss approach alone, within context linear regression model using dummy indicator variables.","code":""},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"key-questions-for-comparing-with-independent-samples","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.2 Key Questions for Comparing with Independent Samples","text":"","code":""},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"what-is-the-population-under-study","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.2.1 What is the population under study?","text":"patients intensive care unit sepsis meet inclusion exclusion criteria study, entire population health centers like ones included trial.","code":""},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"what-is-the-sample-is-it-representative-of-the-population","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.2.2 What is the sample? Is it representative of the population?","text":"sample consists 300 patients. convenient sample population study.randomized clinical trial. 150 patients assigned Ibuprofen, rest Placebo. treatment assignment randomized, selection sample whole.expectation, randomization individuals treatments, study, expected eliminate treatment selection bias.","code":""},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"who-are-the-subjects-individuals-within-the-sample","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.2.3 Who are the subjects / individuals within the sample?","text":"150 patients received Ibuprofen completely different set 150 patients received Placebo.match link patients. best thought independent samples.","code":""},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"what-data-are-available-on-each-individual","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.2.4 What data are available on each individual?","text":"key variables treatment indicator (Ibuprofen Placebo) outcome (drop temperature 2 hours following administration randomly assigned treatment.)","code":""},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"rct-caveats","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.2.5 RCT Caveats","text":"placebo-controlled, double-blind randomized clinical trial, especially pre-registered, often considered best feasible study assessing effectiveness treatment. ’s always true, solid design. primary caveat patients included trials rarely excellent representations population potentially affected patients whole.","code":""},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"exploratory-data-analysis","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.3 Exploratory Data Analysis","text":"Consider following boxplot violin temp_drop data within treat group.Next, ’ll consider faceted histograms data.’s pair Normal Q-Q plots. ’s hard use Normal model approximate Ibuprofen data, model probably good choice Placebo results.’ll perhaps also look ridgeline plot.center ibuprofen distribution shifted bit towards positive (greater improvement) direction, seems, distribution placebo patients. conclusion matches see key numerical summaries, within treatment groups.","code":"\nggplot(sepsis, aes(x = treat, y = temp_drop, fill = treat)) +\n    geom_violin() +\n    geom_boxplot(width = 0.3, fill = \"white\") +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") + \n    labs(title = \"Boxplot of Temperature Drop in Sepsis Patients\",\n         x = \"\", y = \"Drop in Temperature (degrees C)\") + \n    coord_flip() \nggplot(sepsis, aes(x = temp_drop, fill = treat, color = treat)) +\n    geom_histogram(bins = 20) +\n    scale_fill_viridis_d() +\n    scale_color_viridis_d(direction = -1) +\n    guides(fill = \"none\", color = \"none\") + \n    labs(title = \"Histograms of Temperature Drop in Sepsis Patients\",\n         x = \"Drop in Temperature (degrees Celsius\") +\n    facet_wrap(~ treat)\nggplot(sepsis, aes(sample = temp_drop)) +\n    geom_qq() + geom_qq_line(col = \"red\") +\n    facet_wrap(~ treat) + \n    labs(y = \"Temperature Drop Values (in degrees C)\")\nggplot(sepsis, aes(x = temp_drop, y = treat, fill = treat)) +\n    ggridges::geom_density_ridges(scale = 0.9) +\n    guides(fill = \"none\") + \n    labs(title = \"Temperature Drop in Sepsis Patients\",\n         x = \"Drop in Temperature (degrees Celsius)\", y = \"\") +\n    ggridges::theme_ridges()Picking joint bandwidth of 0.182\nmosaic::favstats(temp_drop ~ treat, data = sepsis)      treat  min     Q1 median  Q3 max      mean        sd\n1 Ibuprofen -1.5  0.000    0.5 0.9 3.1 0.4640000 0.6877919\n2   Placebo -2.7 -0.175    0.1 0.4 1.9 0.1526667 0.5709637\n    n missing\n1 150       0\n2 150       0"},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"estimating-the-difference-in-population-means","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.4 Estimating the Difference in Population Means","text":"Next, build point estimate 90% confidence interval difference mean temp_drop treated Ibuprofen mean temp_drop treated Placebo. ’ll use regression model single predictor (treat group) .point estimate “Ibuprofen - Placebo” difference population means 0.311 degrees C, 90% confidence interval (0.191, 0.432) degrees C.also run model like :therefore conclude Placebo - Ibuprofen difference estimated -0.311, 90% confidence interval (-0.432, -0.191), course equivalent previous estimate.Fundamentally, regression model approach identical two-sample t test, assuming equal population variances, also called pooled t test. just one possible way us estimate difference population means, turns .","code":"\nmodel_sep <- lm(temp_drop ~ treat == \"Ibuprofen\", data = sepsis)\n\ntidy(model_sep, conf.int = TRUE, conf.level = 0.90) %>%\n    kable(digits = 3)\nmodel_sep2 <- lm(temp_drop ~ treat, data = sepsis)\n\ntidy(model_sep2, conf.int = TRUE, conf.level = 0.90) %>%\n    kable(digits = 3)"},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"t-based-ci-for-population-mean1---mean2-difference","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.5 t-based CI for population mean1 - mean2 difference","text":"","code":""},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"the-pooled-t-procedure","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.5.1 The Pooled t procedure","text":"commonly used t-procedure building confidence interval assumes two populations compared follows Normal distribution, also population variance. pooled t-test, people usually mean describe two-sample t test., can use tidy object:","code":"\nsepsis %$% t.test(temp_drop ~ treat,\n                  conf.level = 0.90,\n                  alt = \"two.sided\",\n                  var.equal = TRUE)\n    Two Sample t-test\n\ndata:  temp_drop by treat\nt = 4.2656, df = 298, p-value = 2.68e-05\nalternative hypothesis: true difference in means between group Ibuprofen and group Placebo is not equal to 0\n90 percent confidence interval:\n 0.1909066 0.4317600\nsample estimates:\nmean in group Ibuprofen   mean in group Placebo \n              0.4640000               0.1526667 \ntt1 <- sepsis %$% t.test(temp_drop ~ treat,\n                  conf.level = 0.90,\n                  alt = \"two.sided\",\n                  var.equal = TRUE)\ntidy(tt1)# A tibble: 1 x 10\n  estimate estimate1 estimate2 statistic   p.value parameter\n     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>\n1    0.311     0.464     0.153      4.27 0.0000268       298\n# ... with 4 more variables: conf.low <dbl>,\n#   conf.high <dbl>, method <chr>, alternative <chr>"},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"using-linear-regression-to-obtain-a-pooled-t-confidence-interval","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.5.2 Using linear regression to obtain a pooled t confidence interval","text":"’ve seen, demonstrate , linear regression model, using outcome predictor (group) pooled t procedure, produces confidence interval, , assumption two populations comparing follow Normal distribution (population) variance.see point estimate linear regression model difference temp_drop -0.3113333, Ibuprofen subjects higher temp_drop values Placebo subjects, 90% confidence interval difference ranges -0.43176 -0.1909066.can obtain t-based confidence interval parameter estimates linear model directly using tidy broom package. Linear models usually summarize estimate standard error. Remember reasonable approximation large samples 95% confidence interval regression estimate (slope intercept) can obtained estimate plus minus two times standard error., case treatPlacebo estimate, can obtain approximate 95% confidence interval (-0.457, -0.165). Compare 95% confidence interval available model directly, shown tidied output , confint command , ’ll see small difference.Note can also use summary confint build estimates.","code":"\nmodel1 <- lm(temp_drop ~ treat, data = sepsis)\n\ntidy(model1, conf.int = TRUE, conf.level = 0.90)# A tibble: 2 x 7\n  term         estimate std.error statistic  p.value conf.low\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>    <dbl>\n1 (Intercept)     0.464    0.0516      8.99 2.91e-17    0.379\n2 treatPlacebo   -0.311    0.0730     -4.27 2.68e- 5   -0.432\n# ... with 1 more variable: conf.high <dbl>\ntidy(model1, conf.int = TRUE, conf.level = 0.95) %>% kable(digits = 3)\nsummary(model1)\nCall:\nlm(formula = temp_drop ~ treat, data = sepsis)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.85267 -0.36400 -0.05267  0.34733  2.63600 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.46400    0.05161   8.991  < 2e-16 ***\ntreatPlacebo -0.31133    0.07299  -4.266 2.68e-05 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6321 on 298 degrees of freedom\nMultiple R-squared:  0.05755,   Adjusted R-squared:  0.05438 \nF-statistic:  18.2 on 1 and 298 DF,  p-value: 2.68e-05\nconfint(model1, level = 0.95)                  2.5 %     97.5 %\n(Intercept)   0.3624351  0.5655649\ntreatPlacebo -0.4549679 -0.1676988"},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"the-welch-t-procedure","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.5.3 The Welch t procedure","text":"default confidence interval based t test independent samples R uses something called Welch test, two populations compared assumed variance. population assumed follow Normal distribution.Tidying works situation, .balanced design, , number observations appear two samples, Welch t test Pooled t test produce confidence interval. Differences appear sample sizes two groups compared different.","code":"\nsepsis %$% t.test(temp_drop ~ treat,\n                  conf.level = 0.90, \n                  alt = \"two.sided\")\n    Welch Two Sample t-test\n\ndata:  temp_drop by treat\nt = 4.2656, df = 288.24, p-value = 2.706e-05\nalternative hypothesis: true difference in means between group Ibuprofen and group Placebo is not equal to 0\n90 percent confidence interval:\n 0.1908939 0.4317728\nsample estimates:\nmean in group Ibuprofen   mean in group Placebo \n              0.4640000               0.1526667 \ntt0 <- sepsis %$% t.test(temp_drop ~ treat,\n                  conf.level = 0.90, \n                  alt = \"two.sided\")\n\ntidy(tt0)# A tibble: 1 x 10\n  estimate estimate1 estimate2 statistic   p.value parameter\n     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>\n1    0.311     0.464     0.153      4.27 0.0000271      288.\n# ... with 4 more variables: conf.low <dbl>,\n#   conf.high <dbl>, method <chr>, alternative <chr>"},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"wilcoxon-mann-whitney-rank-sum-ci","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.6 Wilcoxon-Mann-Whitney “Rank Sum” CI","text":"one-sample case, rank-based alternative attributed Wilcoxon (sometimes Mann Whitney) provides two-sample comparison pseudomedians two treat groups terms temp_drop. called rank sum test, rather Wilcoxon signed rank test used inference single sample. ’s resulting 90% confidence interval difference pseudomedians.","code":"\nwt <- sepsis %$% wilcox.test(temp_drop ~ treat,\n                       conf.int = TRUE, conf.level = 0.90,\n                       alt = \"two.sided\") \n\nwt\n    Wilcoxon rank sum test with continuity correction\n\ndata:  temp_drop by treat\nW = 14614, p-value = 7.281e-06\nalternative hypothesis: true location shift is not equal to 0\n90 percent confidence interval:\n 0.1999699 0.4000330\nsample estimates:\ndifference in location \n             0.3000368 \ntidy(wt)# A tibble: 1 x 7\n  estimate statistic    p.value conf.low conf.high method   \n     <dbl>     <dbl>      <dbl>    <dbl>     <dbl> <chr>    \n1    0.300    14614. 0.00000728    0.200     0.400 Wilcoxon~\n# ... with 1 more variable: alternative <chr>"},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"bootstrapping-a-more-robust-approach","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.7 Bootstrapping: A More Robust Approach","text":"Within script called Love-boost.R, provided following R code create function called bootdif.Running code place new function called bootdif environment, help us calculate appropriate confidence interval using bootstrap procedure. bootdif function contained Love-boost.R script slightly edited version function http://biostat.mc.vanderbilt.edu/wiki/Main/BootstrapMeansSoftware.","code":"\nbootdif <-\n  function(y, g, conf.level=0.95, B.reps = 2000) {\n    lowq = (1 - conf.level)/2\n    g <- as.factor(g)\n    a <- attr(Hmisc::smean.cl.boot(y[g==levels(g)[1]], B=B.reps, reps=TRUE),'reps')\n    b <- attr(Hmisc::smean.cl.boot(y[g==levels(g)[2]], B=B.reps, reps=TRUE),'reps')\n    meandif <- diff(tapply(y, g, mean, na.rm=TRUE))\n    a.b <- quantile(b-a, c(lowq,1-lowq))\n    res <- c(meandif, a.b)\n    names(res) <- c('Mean Difference',lowq, 1-lowq)\n    res\n  }"},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"bootstrap-ci-for-the-sepsis-study","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.7.1 Bootstrap CI for the Sepsis study","text":"Note approach uses comma separate outcome variable (, temp_drop) variable identifying exposure groups (, treat).approach calculates 90% confidence interval difference means two treatment groups. Note sign opposite direction ’ve seen previous work. can tell mean difference (summarized means data group) approach finding confidence interval using bootstrap procedure Placebo - Ibuprofen difference, specifically (-0.431, -0.183).find confidence interval using bootstrap approach Ibuprofen - Placebo difference, just need switch signs, conclude 90% bootstrap confidence interval difference (0.183, 0.431).","code":"\nset.seed(431212)\n\nsepsis %$% bootdif(temp_drop, treat, conf.level = 0.90)Mean Difference            0.05            0.95 \n     -0.3113333      -0.4313667      -0.1833000 \nmosaic::favstats(temp_drop ~ treat, data = sepsis)      treat  min     Q1 median  Q3 max      mean        sd\n1 Ibuprofen -1.5  0.000    0.5 0.9 3.1 0.4640000 0.6877919\n2   Placebo -2.7 -0.175    0.1 0.4 1.9 0.1526667 0.5709637\n    n missing\n1 150       0\n2 150       0"},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"summary-specifying-a-two-sample-study-design","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.8 Summary: Specifying A Two-Sample Study Design","text":"questions help specify details study design involved comparison two populations quantitative outcome, perhaps means.outcome study?(case, two) treatment/exposure groups?data collected using matched / paired samples independent samples?data random sample population(s) interest? least reasonable argument generalizing sample population(s)?significance level (, confidence level) require ?one-sided two-sided testing/confidence interval generation?paired samples, pairing help reduce nuisance variation?paired samples, distribution sample paired differences tell us inferential procedure use?independent samples, distribution individual sample tell us inferential procedure use?","code":""},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"results-for-the-sepsis-study","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.9 Results for the sepsis study","text":"outcome temp_drop, change body temperature (\\(^{\\circ}\\)C) baseline 2 hours later, positive numbers indicate drops temperature (good outcome.)groups Ibuprofen Placebo contained treat variable sepsis tibble.data collected using independent samples. Ibuprofen subjects matched linked individual Placebo subjects - separate groups.subjects study aren’t drawn random sample population interest, randomly assigned respective treatments (Ibuprofen Placebo) provide reasoned basis inferences.’ll use 10% significance level (90% confidence level) setting, previous work data.’ll use two-sided testing confidence interval approach.Questions 7 8 don’t apply, independent samples data, rather paired samples.address question 9, ’ll need look data sample, previously allow us assess Normality distributions (separately) temp_drop results Ibuprofen Placebo groups. ’ll repeat .plots conclude data Ibuprofen sample follow reasonably Normal distribution, isn’t quite true Placebo sample. ’s hard know whether apparent Placebo group outliers affect whether Normal distribution assumption reasonable, can see confidence intervals change much don’t assume Normality (instance, comparing bootstrap t-based approaches), way understanding whether Normal model large impact conclusions.","code":"\nggplot(sepsis, aes(x = treat, y = temp_drop, fill = treat)) +\n    geom_violin() +\n    geom_boxplot(width = 0.3, fill = \"white\") +\n    scale_fill_viridis_d(alpha = 0.3) +\n    guides(fill = \"none\") + \n    labs(title = \"Boxplot of Temperature Drop in Sepsis Patients\",\n         x = \"\", y = \"Drop in Temperature (degrees C)\") + \n    coord_flip() \nggplot(sepsis, aes(sample = temp_drop)) +\n    geom_qq() + geom_qq_line(col = \"red\") +\n    facet_wrap(~ treat) + \n    labs(y = \"Temperature Drop Values (in degrees C)\")"},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"sepsis-estimation-results","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.9.1 Sepsis Estimation Results","text":"’s set confidence interval estimates (’ll use 90% confidence ) using methods discussed Chapter.conclusions can draw setting?","code":"\nmosaic::favstats(temp_drop ~ treat, data = sepsis)      treat  min     Q1 median  Q3 max      mean        sd\n1 Ibuprofen -1.5  0.000    0.5 0.9 3.1 0.4640000 0.6877919\n2   Placebo -2.7 -0.175    0.1 0.4 1.9 0.1526667 0.5709637\n    n missing\n1 150       0\n2 150       0\ns_pooled_t_test <- sepsis %$% t.test(temp_drop ~ treat, \n                           conf.level = 0.90,\n                           alt = \"two.sided\", \n                           var.equal = TRUE)\n\ntidy(s_pooled_t_test) %>% \n    select(conf.low, conf.high)# A tibble: 1 x 2\n  conf.low conf.high\n     <dbl>     <dbl>\n1    0.191     0.432\ns_welch_t_test <- sepsis %$% t.test(temp_drop ~ treat, \n                           conf.level = 0.90,\n                           alt = \"two.sided\", \n                           var.equal = FALSE)\n\ntidy(s_welch_t_test) %>% \n    select(estimate, conf.low, conf.high)# A tibble: 1 x 3\n  estimate conf.low conf.high\n     <dbl>    <dbl>     <dbl>\n1    0.311    0.191     0.432\ns_wilcoxon_test <- sepsis %$% wilcox.test(temp_drop ~ treat,\n                       conf.int = TRUE, conf.level = 0.90,\n                       alt = \"two.sided\") \n\ntidy(s_wilcoxon_test) %>% \n    select(estimate, conf.low, conf.high)# A tibble: 1 x 3\n  estimate conf.low conf.high\n     <dbl>    <dbl>     <dbl>\n1    0.300    0.200     0.400\nset.seed(431212)\ns_bootstrap <- sepsis %$% bootdif(temp_drop, treat, \n                                  conf.level = 0.90)\n\ns_bootstrapMean Difference            0.05            0.95 \n     -0.3113333      -0.4313667      -0.1833000 "},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"categorizing-the-outcome-and-comparing-rates","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.10 Categorizing the Outcome and Comparing Rates","text":"Suppose interested comparing percentage patients arm trial (Ibuprofen vs. Placebo) showed improvement temperature (temp_drop > 0). build cross-tabulation interest, create new variable, called dropped indicates whether subject’s temperature dropped, use tabyl.primary interest comparing percentage Ibuprofen patients whose temperature dropped percentage Placebo patients whose temperature dropped.","code":"\nsepsis <- sepsis %>%\n    mutate(dropped = ifelse(temp_drop > 0, \"Drop\", \"No Drop\"))\n\nsepsis %>% tabyl(treat, dropped)     treat Drop No Drop\n Ibuprofen  107      43\n   Placebo   80      70\nsepsis %>% tabyl(treat, dropped) %>% \n    adorn_totals() %>%\n    adorn_percentages(denom = \"row\") %>%\n    adorn_pct_formatting(digits = 1) %>%\n    adorn_ns(position = \"front\")     treat        Drop     No Drop\n Ibuprofen 107 (71.3%)  43 (28.7%)\n   Placebo  80 (53.3%)  70 (46.7%)\n     Total 187 (62.3%) 113 (37.7%)"},{"path":"the-ibuprofen-in-sepsis-randomized-clinical-trial.html","id":"estimating-the-difference-in-proportions","chapter":"17 The Ibuprofen in Sepsis Randomized Clinical Trial","heading":"17.11 Estimating the Difference in Proportions","text":"sample, 71.3% Ibuprofen subjects, 53.3% Placebo subjects, experienced drop temperature. point estimate difference percentages 18.0 percentage points, usually set instead terms proportions, difference 0.180.Now, ’ll find confidence interval difference, can several ways, including twoby2 function Epi package.lot additional output , ’ll look now just Probability difference row, see point estimate (0.180) 90% confidence interval estimate difference proportions (0.088, 0.268) comparing Ibuprofen vs. Placebo outcome Dropping Temperature.estimation difference population proportions found later.","code":"\nsepsis %$% table(treat, dropped) %>% Epi::twoby2(alpha = 0.10)2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Drop \nComparing : Ibuprofen vs. Placebo \n\n          Drop No Drop    P(Drop) 90% conf. interval\nIbuprofen  107      43     0.7133    0.6490   0.7701\nPlacebo     80      70     0.5333    0.4661   0.5993\n\n                                   90% conf. interval\n             Relative Risk: 1.3375    1.1492   1.5567\n         Sample Odds Ratio: 2.1773    1.4583   3.2509\nConditional MLE Odds Ratio: 2.1716    1.4177   3.3437\n    Probability difference: 0.1800    0.0881   0.2677\n\n             Exact P-value: 0.0019 \n        Asymptotic P-value: 0.0014 \n------------------------------------------------------"},{"path":"comparing-means-with-paired-samples.html","id":"comparing-means-with-paired-samples","chapter":"18 Comparing Means with Paired Samples","heading":"18 Comparing Means with Paired Samples","text":", ’ll consider problem estimating confidence interval describe difference population means (medians) based comparison two samples quantitative data, gathered using matched pairs design.Specifically, ’ll use example Lead Blood Children study, described .","code":""},{"path":"comparing-means-with-paired-samples.html","id":"lead-in-the-blood-of-children","chapter":"18 Comparing Means with Paired Samples","heading":"18.1 Lead in the Blood of Children","text":"One best ways eliminate source variation errors interpretation associated use matched pairs. subject one group matched closely possible subject group. 45-year-old African-American male hypertension given [treatment designed lower blood pressure], give second, similarly built 45-year old African-American male hypertension placebo.Phillip . Good,42 section 5.2.4","code":""},{"path":"comparing-means-with-paired-samples.html","id":"the-lead-in-the-blood-of-children-study","chapter":"18 Comparing Means with Paired Samples","heading":"18.2 The Lead in the Blood of Children Study","text":"D. Morton et al.43 studied absorption lead blood children. matched-sample study, exposed group interest contained 33 children parents worked battery manufacturing factory (lead used) state Oklahoma. Specifically, child lead-exposed parent matched another child age, exposure traffic, living neighborhood whose parents work lead-related industries. complete study 66 children, arranged 33 matched pairs. outcome interest, gathered sample whole blood children, lead content, measured mg/dl.One motivation study captured Abstract Morton et al.44It repeatedly reported children employees lead-related industry increased risk lead absorption high levels lead found household dust workers.data available several places, including Table 5 Robert M. Pruzek James E. Helmreich,45 BloodLead data set within PairedData package R, also make available bloodlead.csv file. table first pairs observations (blood lead levels one child exposed lead matched control) shown .pair, one child exposed (parent working factory) .Otherwise, though, child similar matched partner.data exposed control blood lead content, mg/dl.primary goal estimate difference lead content exposed control children, use sample estimate make inferences difference lead content population children like exposed group population children like control group.","code":"\nbloodlead <- read_csv(\"data/bloodlead.csv\")Rows: 33 Columns: 3-- Column specification ------------------------------------\nDelimiter: \",\"\nchr (1): pair\ndbl (2): exposed, control\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\nbloodlead# A tibble: 33 x 3\n   pair  exposed control\n   <chr>   <dbl>   <dbl>\n 1 P01        38      16\n 2 P02        23      18\n 3 P03        41      18\n 4 P04        18      24\n 5 P05        37      19\n 6 P06        36      11\n 7 P07        23      10\n 8 P08        62      15\n 9 P09        31      16\n10 P10        34      18\n# ... with 23 more rows"},{"path":"comparing-means-with-paired-samples.html","id":"our-key-questions-for-a-paired-samples-comparison","chapter":"18 Comparing Means with Paired Samples","heading":"18.2.1 Our Key Questions for a Paired Samples Comparison","text":"population study?pairs children living Oklahoma near factory question, one parent working factory exposed lead, .sample? representative population?sample consists 33 pairs one exposed one control child.case-control study, children carefully enrolled meet design criteria. Absent information, ’re likely assume serious bias associated pairs, assuming represent population effectively (perhaps broader population kids whose parents work lead-based industries generally) may well least reasonable assuming don’t.subjects / individuals within sample?33 pairs children includes one exposed child one unexposed (control) child.data available individual?blood lead content, measured mg/dl whole blood.","code":""},{"path":"comparing-means-with-paired-samples.html","id":"lead-study-caveats","chapter":"18 Comparing Means with Paired Samples","heading":"18.2.2 Lead Study Caveats","text":"Note children randomly selected general populations kids whose parents work lead-based industries.make inferences populations, must make strong assumptions believe, instance, sample exposed children representative random sample children similar exposures across world .researchers detailed theory exposed children might increased risk lead absorption, fact part study gathered additional information whether possible explanation might related quality hygiene parents (fathers, actually) worked factory.observational study, estimation causal effect parental work lead-based industry children’s blood lead content can made, without substantial (perhaps heroic) assumptions.","code":""},{"path":"comparing-means-with-paired-samples.html","id":"exploratory-data-analysis-for-paired-samples","chapter":"18 Comparing Means with Paired Samples","heading":"18.3 Exploratory Data Analysis for Paired Samples","text":"’ll begin adjusting data two ways.’d like first variable (pair) factor rather character type R, want able summarize effectively. ’ll make change.Also, ’d like calculate difference lead content exposed control children pair, ’ll save within-pair difference variable called lead_diff. ’ll take lead_diff = exposed - control positive values indicate increased lead exposed child.","code":"\nbloodlead_original <- bloodlead\n\nbloodlead <- bloodlead_original %>%\n    mutate(pair = factor(pair),\n           lead_diff = exposed - control)\n\nbloodlead# A tibble: 33 x 4\n   pair  exposed control lead_diff\n   <fct>   <dbl>   <dbl>     <dbl>\n 1 P01        38      16        22\n 2 P02        23      18         5\n 3 P03        41      18        23\n 4 P04        18      24        -6\n 5 P05        37      19        18\n 6 P06        36      11        25\n 7 P07        23      10        13\n 8 P08        62      15        47\n 9 P09        31      16        15\n10 P10        34      18        16\n# ... with 23 more rows"},{"path":"comparing-means-with-paired-samples.html","id":"the-paired-differences","chapter":"18 Comparing Means with Paired Samples","heading":"18.3.1 The Paired Differences","text":"begin, focus lead_diff exploratory work, exposed - control difference lead content within 33 pairs. , ’ll 33 observations, compared 462 serum zinc data, tools still helpful.Note work, plotted paired differences. One obvious way tell paired samples can pair every single subject one exposure group unique subject exposure group. Everyone paired, sample sizes always two groups.’s summary paired differences.","code":"\np1 <- ggplot(bloodlead, aes(sample = lead_diff)) +\n  geom_qq(col = \"darkslategray\") + geom_qq_line(col = \"navy\") + \n  theme(aspect.ratio = 1) + \n  labs(title = \"Normal Q-Q plot\")\n\np2 <- ggplot(bloodlead, aes(x = lead_diff)) +\n  geom_histogram(aes(y = stat(density)), \n                 binwidth = 5, fill = \"darkslategray\", col = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(bloodlead$lead_diff), \n                            sd = sd(bloodlead$lead_diff)),\n                col = \"navy\", lwd = 1.5) +\n  labs(title = \"Histogram with Normal Density\")\n\np3 <- ggplot(bloodlead, aes(x = lead_diff, y = \"\")) +\n  geom_boxplot(fill = \"darkslategray\", outlier.color = \"darkslategray\") + \n  labs(title = \"Boxplot\", y = \"\")\n\np1 + (p2 / p3 + plot_layout(heights = c(4,1))) + \n    plot_annotation(title = \"Difference in Blood Lead Content (mg/dl) for 33 Pairs of Children\")\nmosaic::favstats(~ lead_diff, data = bloodlead) min Q1 median Q3 max    mean       sd  n missing\n  -9  4     15 25  60 15.9697 15.86365 33       0\nbloodlead %>% summarize(skew1 = \n                            (mean(lead_diff) - median(lead_diff)) / \n                            sd(lead_diff))# A tibble: 1 x 1\n   skew1\n   <dbl>\n1 0.0611"},{"path":"comparing-means-with-paired-samples.html","id":"impact-of-matching---scatterplot-and-correlation","chapter":"18 Comparing Means with Paired Samples","heading":"18.3.2 Impact of Matching - Scatterplot and Correlation","text":", data paired study matching neighborhood, age exposure traffic. individual child’s outcome value part pair outcome value /matching partner. can see pairing several ways, perhaps drawing scatterplot pairs.point represents pair observations, one control child, one matched exposed child. strong linear relationship (usually positive slope, thus positive correlation) paired outcomes, pairing helpful terms improving statistical power estimates build weak relationship.stronger Pearson correlation coefficient, helpful pairing ., straight line model using control child’s blood lead content accounts 3.2% variation blood lead content exposed child.turns , pairing modest impact inferences draw study. still treat data paired, despite .","code":"\nggplot(bloodlead, aes(x = control, y = exposed)) +\n    geom_point(size = 2) + \n    geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE) +\n    geom_text(x = 20, y = 65, col = \"blue\", \n             label = \n                 paste(\"Pearson r = \",\n                       round(bloodlead %$% \n                                 cor(control, exposed),2))) +\n    labs(title = \"Paired Samples in Blood Lead study\",\n         x = \"Blood Lead Content (mg/dl) in Control Child\",\n         y = \"Blood Lead Content (mg/dl) in Exposed Child\")"},{"path":"comparing-means-with-paired-samples.html","id":"looking-at-separate-samples-using-pivot_longer","chapter":"18 Comparing Means with Paired Samples","heading":"18.4 Looking at Separate Samples: Using pivot_longer","text":"purpose estimating difference exposed control children, summaries paired differences ’ll need.settings, however, might also look boxplot, violin plot, ridgeline plot showed distributions exposed control children separately. run trouble one variable (blood lead content) spread across multiple columns (control exposed.) solution “pivot” tibble current format build new, tidy tibble. data aren’t tidied , one row subject one column variable, work get form usual plotting strategy work well.pivot_longer() “lengthens” data, increasing number rows decreasing number columns.pivot_wider() performs inverse transformation, “widening” data.original bloodlead data, drop lead_diff addition made, wide data, row representing two different subjects.want accomplish one row subject, instead one row pair subjects. want make data longer.approach (case, ’re making data “longer” opposite making data “wider”), visit Tidy data chapter Grolemund Wickham46 tidyr repository Github https://github.com/tidyverse/tidyr.now, can plot usual compare two samples.First, ’ll look boxplot, showing data.’ll also look ridgeline plot, Dr. Love likes , even though ’re really useful ’re comparing two samples.center spread distribution substantially larger exposed group controls. course, numerical summaries show patterns, .","code":"\nhead(bloodlead_original, 3)# A tibble: 3 x 3\n  pair  exposed control\n  <chr>   <dbl>   <dbl>\n1 P01        38      16\n2 P02        23      18\n3 P03        41      18\nbloodlead_longer <- bloodlead_original %>%\n    pivot_longer(\n        cols = -c(pair),\n        names_to = \"status\",\n        values_to = \"lead_level\")\n\nbloodlead_longer# A tibble: 66 x 3\n   pair  status  lead_level\n   <chr> <chr>        <dbl>\n 1 P01   exposed         38\n 2 P01   control         16\n 3 P02   exposed         23\n 4 P02   control         18\n 5 P03   exposed         41\n 6 P03   control         18\n 7 P04   exposed         18\n 8 P04   control         24\n 9 P05   exposed         37\n10 P05   control         19\n# ... with 56 more rows\nggplot(bloodlead_longer, aes(x = status, y = lead_level)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = status), width = 0.2) +\n    scale_fill_viridis_d(begin = 0.5) +\n    guides(fill = \"none\") + \n    coord_flip() +\n    labs(title = \"Boxplot of Lead Content in Exposed and Control kids\") \nggplot(bloodlead_longer, aes(x = lead_level, y = status, fill = status)) +\n    ggridges::geom_density_ridges(scale = 0.9) +\n    guides(fill = \"none\") + \n    labs(title = \"Lead Content in Exposed and Control kids\") +\n    ggridges::theme_ridges()Picking joint bandwidth of 4.01\nmosaic::favstats(lead_level ~ status, data = bloodlead_longer) %>%\n    kable(digits = 2)"},{"path":"comparing-means-with-paired-samples.html","id":"estimating-the-difference-in-means-with-paired-samples","chapter":"18 Comparing Means with Paired Samples","heading":"18.5 Estimating the Difference in Means with Paired Samples","text":"Suppose want estimate difference mean blood level across population children represented sample taken study. , must take advantage matched samples design, complete estimation paired differences, treating single sample data.One way accomplish simply run usual intercept-linear regression model paired differences.point estimate difference (exposed - control) lead levels 15.97 mg/dl, 90% confidence interval (11.29, 20.65) mg/dl.","code":"\nmodel_lead <- lm(lead_diff ~ 1, data = bloodlead)\n\ntidy(model_lead, conf.int = TRUE, conf.level = 0.90) %>%\n    kable(digits = 2)"},{"path":"comparing-means-with-paired-samples.html","id":"paired-data-in-longer-format","chapter":"18 Comparing Means with Paired Samples","heading":"18.5.1 Paired Data in Longer Format?","text":"data “longer” format, bloodlead_longer, pairs identified pair variable, obtained confidence interval using:key elements found statusexposed row, can focus nicely (since output tidy() function always tibble) :, 90% confidence interval estimate population mean difference exposed control children.","code":"\nmodel2_lead <- lm(lead_level ~ status + factor(pair), data = bloodlead_longer)\n\ntidy(model2_lead, conf.int = TRUE, conf.level = 0.90) # A tibble: 34 x 7\n   term       estimate std.error statistic  p.value conf.low\n   <chr>         <dbl>     <dbl>     <dbl>    <dbl>    <dbl>\n 1 (Intercep~    19.0       8.05    2.36    2.44e-2     5.38\n 2 statusexp~    16.0       2.76    5.78    2.04e-6    11.3 \n 3 factor(pa~    -6.50     11.2    -0.579   5.66e-1   -25.5 \n 4 factor(pa~     2.50     11.2     0.223   8.25e-1   -16.5 \n 5 factor(pa~    -6.00     11.2    -0.535   5.96e-1   -25.0 \n 6 factor(pa~     1.00     11.2     0.0891  9.30e-1   -18.0 \n 7 factor(pa~    -3.50     11.2    -0.312   7.57e-1   -22.5 \n 8 factor(pa~   -10.5      11.2    -0.936   3.56e-1   -29.5 \n 9 factor(pa~    11.5      11.2     1.03    3.13e-1    -7.50\n10 factor(pa~    -3.50     11.2    -0.312   7.57e-1   -22.5 \n# ... with 24 more rows, and 1 more variable:\n#   conf.high <dbl>\ntidy(model2_lead, conf.int = TRUE, conf.level = 0.90) %>%\n    filter(term == \"statusexposed\") %>% \n    knitr::kable(digits = 2)"},{"path":"comparing-means-with-paired-samples.html","id":"matched-pairs-vs.-two-independent-samples","chapter":"18 Comparing Means with Paired Samples","heading":"18.6 Matched Pairs vs. Two Independent Samples","text":"data obtained two independent samples, rather matched pairs.matched pairs individual observation “treatment” group matched one one observation “control” group way data gathered. Paired (matched) data can arise several ways.\ncommon “pre-post” study subjects measured exposure happens.\nobservational studies, often match subjects receive exposure account differences things like age, sex, race covariates. happens Lead Blood Children study.\ncommon “pre-post” study subjects measured exposure happens.observational studies, often match subjects receive exposure account differences things like age, sex, race covariates. happens Lead Blood Children study.data paired samples, (fact) must form paired differences, subject left unpaired.\nline data comparing two samples quantitative data links individual “treated” “control” observations form matched pairs evident, data paired.\nsample sizes different, ’d know independent samples, matched pairs requires subject “treated” group matched single, unique member “control” group, thus exactly many “treated” “control” subjects.\nmany subjects one treatment group (called balanced design) necessary, sufficient, us conclude matched pairs used.\nline data comparing two samples quantitative data links individual “treated” “control” observations form matched pairs evident, data paired.sample sizes different, ’d know independent samples, matched pairs requires subject “treated” group matched single, unique member “control” group, thus exactly many “treated” “control” subjects.many subjects one treatment group (called balanced design) necessary, sufficient, us conclude matched pairs used.Bock, Velleman, De Veaux47 suggest,… know data paired, can take advantage fact - fact, must take advantage . … must decide whether data paired understanding collected mean. … test determine whether data paired.","code":""},{"path":"comparing-means-with-paired-samples.html","id":"estimating-the-population-mean-of-the-paired-differences","chapter":"18 Comparing Means with Paired Samples","heading":"18.7 Estimating the Population Mean of the Paired Differences","text":"two main approaches used frequently estimate population mean paired differences.Estimation using t distribution (assuming least approximately Normal distribution paired differences)Estimation using bootstrap (doesn’t require Normal assumption)addition, might consider estimating alternate statistic data don’t follow symmetric distribution, like median, bootstrap. settings, rank-based alternative called Wilcoxon signed rank test available estimate psuedo-median. approaches mirror single sample, earlier Notes.","code":""},{"path":"comparing-means-with-paired-samples.html","id":"t-based-ci-for-population-mean-of-paired-differences","chapter":"18 Comparing Means with Paired Samples","heading":"18.8 t-based CI for Population Mean of Paired Differences","text":"R, least five different methods obtaining t-based confidence interval population difference means paired samples. mathematically identical. key idea calculate paired differences (exposed - control, example) pair, treat result single sample apply methods developed situation earlier Notes.","code":""},{"path":"comparing-means-with-paired-samples.html","id":"method-1","chapter":"18 Comparing Means with Paired Samples","heading":"18.8.1 Method 1","text":"can use single-sample approach, applied variable containing paired differences. Let’s build 90% two-sided confidence interval population mean difference blood lead content across possible pairs exposed (parent works lead-based industry) control (parent ) child.90% confidence interval (11.29, 20.65) according t-based procedure. appropriate interpretation 90% two-sided confidence interval :(11.29, 20.65) milligrams per deciliter 90% two-sided confidence interval population mean difference blood lead content exposed control children.point estimate true population difference mean blood lead content 15.97 mg.dl. values interval (11.29, 20.65) mg/dl represent reasonable range estimates true population difference mean blood lead content, 90% confident method creating confidence interval produce result containing true population mean difference.draw 100 samples 33 matched pairs population described sample, use sample produce confidence interval manner, approximately 90 confidence intervals cover true population mean difference blood lead content levels.","code":"\ntt1 <- bloodlead %$% t.test(lead_diff, conf.level = 0.90, \n                            alt = \"two.sided\")\n\ntt1\n    One Sample t-test\n\ndata:  lead_diff\nt = 5.783, df = 32, p-value = 2.036e-06\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 11.29201 20.64738\nsample estimates:\nmean of x \n  15.9697 \ntidy(tt1) %>% knitr::kable(digits = 2)"},{"path":"comparing-means-with-paired-samples.html","id":"method-2","chapter":"18 Comparing Means with Paired Samples","heading":"18.8.2 Method 2","text":", can apply single-sample approach calculated difference blood lead content exposed control groups. , ’ll get 95% two-sided confidence interval difference, instead 90% interval obtained .","code":"\ntt2 <- bloodlead %$% t.test(exposed - control, \n       conf.level = 0.95, alt = \"two.sided\")\n\ntt2\n    One Sample t-test\n\ndata:  exposed - control\nt = 5.783, df = 32, p-value = 2.036e-06\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 10.34469 21.59470\nsample estimates:\nmean of x \n  15.9697 \ntidy(tt2) %>% knitr::kable(digits = 2)"},{"path":"comparing-means-with-paired-samples.html","id":"method-3","chapter":"18 Comparing Means with Paired Samples","heading":"18.8.3 Method 3","text":", can provide R two separate samples (unaffected affected) specify samples paired. , ’ll get 99% one-sided confidence interval (lower bound) population mean difference blood lead content., three different methods using t.test paired samples produce identical results feed confidence level type interval (two-sided, greater less ).","code":"\ntt3 <- bloodlead %$% t.test(exposed, control, conf.level = 0.99,\n       paired = TRUE, alt = \"greater\")\n\ntt3\n    Paired t-test\n\ndata:  exposed and control\nt = 5.783, df = 32, p-value = 1.018e-06\nalternative hypothesis: true difference in means is greater than 0\n99 percent confidence interval:\n 9.207658      Inf\nsample estimates:\nmean of the differences \n                15.9697 \ntidy(tt3) %>% knitr::kable(digits = 2)"},{"path":"comparing-means-with-paired-samples.html","id":"method-4","chapter":"18 Comparing Means with Paired Samples","heading":"18.8.4 Method 4","text":"can also use intercept-linear regression model estimate population mean paired differences two-tailed confidence interval, creating variable containing paired differences.","code":"\nmodel_lead <- lm(lead_diff ~ 1, data = bloodlead)\n\ntidy(model_lead, conf.int = TRUE, conf.level = 0.95)# A tibble: 1 x 7\n  term        estimate std.error statistic  p.value conf.low\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>\n1 (Intercept)     16.0      2.76      5.78  2.04e-6     10.3\n# ... with 1 more variable: conf.high <dbl>"},{"path":"comparing-means-with-paired-samples.html","id":"method-5","chapter":"18 Comparing Means with Paired Samples","heading":"18.8.5 Method 5","text":"data longer format, variable identifying matched pairs, can use different specification linear model obtain estimate.","code":"\nmodel2_lead <- lm(lead_level ~ status + factor(pair), data = bloodlead_longer)\n\ntidy(model2_lead, conf.int = TRUE, conf.level = 0.95) %>%\n    filter(term == \"statusexposed\")# A tibble: 1 x 7\n  term          estimate std.error statistic p.value conf.low\n  <chr>            <dbl>     <dbl>     <dbl>   <dbl>    <dbl>\n1 statusexposed     16.0      2.76      5.78 2.04e-6     10.3\n# ... with 1 more variable: conf.high <dbl>"},{"path":"comparing-means-with-paired-samples.html","id":"assumptions","chapter":"18 Comparing Means with Paired Samples","heading":"18.8.6 Assumptions","text":"building confidence interval based sample observations drawn population, must pay close attention assumptions procedures. confidence interval procedure population mean paired difference using t distribution assumes :want estimate population mean paired difference.drawn sample paired differences random population interest.sampled paired differences drawn population set paired differences independently identical distributions.population follows Normal distribution. least, sample approximately Normal.","code":""},{"path":"comparing-means-with-paired-samples.html","id":"bootstrap-ci-for-mean-difference-using-paired-samples","chapter":"18 Comparing Means with Paired Samples","heading":"18.9 Bootstrap CI for mean difference using paired samples","text":"bootstrap approach used paired differences single sample. use smean.cl.boot() function Hmisc package obtain bootstrap confidence intervals population mean paired differences blood lead content.Note case, confidence interval difference means bit less wide 95% confidence interval generated t test, (10.34, 21.59). ’s common bootstrap produce narrower range (.e. apparently precise estimate) population mean, ’s automatic endpoints bootstrap inside provided t test, either.example, bootstrap CI doesn’t contain t-test based interval, since upper bound exceeds t-based interval:demonstration aside, appropriate thing applying bootstrap specify confidence interval select seed number (B = 1,000 10,000, usually) desired bootstrap replications, run bootstrap just move , rather repeating process multiple times looking particular result.","code":"\nset.seed(431555)\nbloodlead %$% Hmisc::smean.cl.boot(lead_diff, B = 1000,\n                                   conf.int = 0.95)    Mean    Lower    Upper \n15.96970 10.81742 21.48788 \nset.seed(431002)\nbloodlead %$% Hmisc::smean.cl.boot(lead_diff, B = 1000,\n                                   conf.int = 0.95)    Mean    Lower    Upper \n15.96970 10.81667 21.66667 "},{"path":"comparing-means-with-paired-samples.html","id":"assumptions-1","chapter":"18 Comparing Means with Paired Samples","heading":"18.9.1 Assumptions","text":"bootstrap confidence interval procedure population mean (median) set paired differences assumes :want estimate population mean paired differences (population median).drawn sample observations random population interest.sampled observations drawn population paired differences independently identical distributions.willing put fact different people (using random seed) get somewhat different confidence interval estimates using data.’ve seen, major part bootstrap’s appeal ability relax assumptions.","code":""},{"path":"comparing-means-with-paired-samples.html","id":"wilcoxon-signed-rank-based-ci-for-paired-samples","chapter":"18 Comparing Means with Paired Samples","heading":"18.10 Wilcoxon Signed Rank-based CI for paired samples","text":"also use Wilcoxon signed rank procedure generate CI pseudo-median paired differences.one sample case, can revise code slightly specify different confidence level, gather one-sided rather two-sided confidence interval.","code":"\nwt <- bloodlead %$% wilcox.test(lead_diff, conf.int = TRUE,\n                                conf.level = 0.90, \n                                exact = FALSE)\nwt\n    Wilcoxon signed rank test with continuity correction\n\ndata:  lead_diff\nV = 499, p-value = 1.155e-05\nalternative hypothesis: true location is not equal to 0\n90 percent confidence interval:\n 10.99992 20.49998\nsample estimates:\n(pseudo)median \n      15.49996 \ntidy(wt)# A tibble: 1 x 7\n  estimate statistic   p.value conf.low conf.high method    \n     <dbl>     <dbl>     <dbl>    <dbl>     <dbl> <chr>     \n1     15.5       499 0.0000115     11.0      20.5 Wilcoxon ~\n# ... with 1 more variable: alternative <chr>"},{"path":"comparing-means-with-paired-samples.html","id":"assumptions-2","chapter":"18 Comparing Means with Paired Samples","heading":"18.10.1 Assumptions","text":"Wilcoxon signed rank confidence interval procedure working paired differences assumes :want estimate population pseudo-median paired differences.drawn sample observations random population paired differences interest.sampled observations drawn population paired differences independently identical distributions.population follows symmetric distribution. least, sample shows substantial skew, sample pseudo-median reasonable estimate population median.","code":""},{"path":"comparing-means-with-paired-samples.html","id":"choosing-a-confidence-interval-approach","chapter":"18 Comparing Means with Paired Samples","heading":"18.11 Choosing a Confidence Interval Approach","text":"Suppose want find confidence interval population mean difference two populations based matched pairs.willing assume population distribution Normal\nusually use t-based CI.\nusually use t-based CI.unwilling assume population Normal,\nuse bootstrap procedure get CI population mean, even median\nwilling assume population symmetric, consider Wilcoxon signed rank procedure get CI median, rather mean.\nuse bootstrap procedure get CI population mean, even medianbut willing assume population symmetric, consider Wilcoxon signed rank procedure get CI median, rather mean.two methods ’ll use often bootstrap (especially data don’t appear least pretty well fit Normal model) t-based confidence intervals (data appear fit Normal model reasonably well.)","code":""},{"path":"comparing-means-with-paired-samples.html","id":"conclusions-for-the-bloodlead-study","chapter":"18 Comparing Means with Paired Samples","heading":"18.12 Conclusions for the bloodlead study","text":"Using procedures, conclude null hypothesis (true mean paired Exposed - Control differences 0 mg/dl) consonant see 90% confidence interval.Note one-sided one-tailed hypothesis testing procedures work way paired samples single sample.","code":""},{"path":"comparing-means-with-paired-samples.html","id":"the-sign-test","chapter":"18 Comparing Means with Paired Samples","heading":"18.13 The Sign test","text":"sign test something ’ve skipped discussion far. test consistent differences pairs observations, just paired t, Wilcoxon signed rank bootstrap paired samples can provide. advantage relatively easy calculate hand, doesn’t require paired differences follow Normal distribution. fact, even work data substantially skewed.Calculate paired difference pair, drop difference = 0.Let N number pairs remain, 2N data points.Let W, test statistic, number pairs (N) difference positive.Assuming H0 true, W follows binomial distribution probability 0.5 N trials.example, consider data blood lead content:N = 32 pairs, \\(W\\) = 28 positive. use binom.test approach R:one-tailed test can obtained substituting “less” “greater” alternative interest.confidence interval provided doesn’t relate back original population means. ’s just showing confidence interval around probability exposed mean greater control mean pair children.","code":"\nbloodlead$lead_diff [1] 22  5 23 -6 18 25 13 47 15 16  6  1  2  7  0  4 -9 -3\n[19] 36 25  1 16 42 30 25 23 32 17  9 -3 60 14 14\nbinom.test(x = 28, n = 32, p = 0.5, \n           alternative = \"two.sided\")\n\n\ndata:  28 out of 32\nnumber of successes = 28, number of trials = 32,\np-value = 1.93e-05\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.7100516 0.9648693\nsample estimates:\nprobability of success \n                 0.875 "},{"path":"comparing-means-with-paired-samples.html","id":"paired-dependent-vs.-independent-samples","chapter":"18 Comparing Means with Paired Samples","heading":"18.14 Paired (Dependent) vs. Independent Samples","text":"One area consistently trips students course thought process involved distinguishing studies comparing means analyzed using dependent (.e. paired matched) samples analyzed using independent samples. dependent samples analysis uses additional information sample pair/match subjects receiving various exposures. additional information part independent samples analysis (unpaired testing situation.) reasons () increase statistical power, /(b) reduce effect confounding. thoughts subject.design experiments, blocking term often used process arranging subjects groups (blocks) similar one another. Typically, blocking factor source variability primary interest researcher example blocking factor might sex patient; blocking sex, source variability controlled , thus leading greater accuracy.sample sizes balanced (equal), samples must treated independent, since way precisely link subjects. , 10 subjects receiving exposure 12 subjects receiving exposure B, dependent samples analysis (paired t test) correct.sample sizes balanced (equal), samples must treated independent, since way precisely link subjects. , 10 subjects receiving exposure 12 subjects receiving exposure B, dependent samples analysis (paired t test) correct.key element meaningful link observation one exposure group specific observation exposure group. Given balanced design, common strategy indicating dependent samples involves two repeated measures subjects. example, comparing outcomes application exposure, , say, 20 subjects provide us data exposure, comparison results exposure use dependent samples analysis. link subjects subject - exposed subject serves control.key element meaningful link observation one exposure group specific observation exposure group. Given balanced design, common strategy indicating dependent samples involves two repeated measures subjects. example, comparing outcomes application exposure, , say, 20 subjects provide us data exposure, comparison results exposure use dependent samples analysis. link subjects subject - exposed subject serves control.second common strategy indicating dependent samples involves deliberate matching subjects receiving two exposures. matched set observations (often pair, trio quartet, etc.) determined using baseline information (pair involved) one subject receives exposure member pair receives exposure B, calculating paired difference, learn effect exposure, controlling variables made similar across two subjects matching process.second common strategy indicating dependent samples involves deliberate matching subjects receiving two exposures. matched set observations (often pair, trio quartet, etc.) determined using baseline information (pair involved) one subject receives exposure member pair receives exposure B, calculating paired difference, learn effect exposure, controlling variables made similar across two subjects matching process.order dependent samples analysis used, need () link observation across exposure groups based way data collected, (b) consistent measure (units measurement) paired differences can calculated interpreted sensibly.order dependent samples analysis used, need () link observation across exposure groups based way data collected, (b) consistent measure (units measurement) paired differences can calculated interpreted sensibly.samples collected facilitate dependent samples analysis, correlation outcome measurements across groups often moderately strong positive. ’s case, use dependent samples analysis reduce effect baseline differences exposure groups, thus provide precise estimate. even correlation quite small, dependent samples analysis provide powerful estimate impact exposure outcome independent samples analysis number observations.samples collected facilitate dependent samples analysis, correlation outcome measurements across groups often moderately strong positive. ’s case, use dependent samples analysis reduce effect baseline differences exposure groups, thus provide precise estimate. even correlation quite small, dependent samples analysis provide powerful estimate impact exposure outcome independent samples analysis number observations.","code":""},{"path":"comparing-means-with-paired-samples.html","id":"three-tricky-examples","chapter":"18 Comparing Means with Paired Samples","heading":"18.14.1 Three “Tricky” Examples","text":"Suppose take convenient sample 200 patients population patients complete blood test April 2017 including check triglycerides, triglyceride level high category (200 499 mg/dl). Next, select patient random group 200 patients, identify another patient group 200 age (within 2 years) also sex. randomly assign intervention one two patients usual care without intervention patient. set two patients aside return original sample, repeating process find patients age range gender. generates total 77 patients receive intervention 77 . trying assess effect intervention triglyceride level October 2017 using sample 154 people, use dependent (paired) independent samples?Suppose take convenient sample 200 patients population patients complete blood test April 2017 including check triglycerides, triglyceride level high category (200 499 mg/dl). Next, select patient random group 200 patients, identify another patient group 200 age (within 2 years) also sex. randomly assign intervention one two patients usual care without intervention patient. set two patients aside return original sample, repeating process find patients age range gender. generates total 77 patients receive intervention 77 . trying assess effect intervention triglyceride level October 2017 using sample 154 people, use dependent (paired) independent samples?Suppose take convenient sample 77 patients population patients complete blood test April 2017 including check triglycerides, triglyceride level high category (200 499 mg/dl). Next, take convenient sample 77 patients population patients complete blood test May 2017 including check triglycerides, triglyceride level high category (200 499 mg/dl). flip coin determine whether intervention given 77 patients April 2017 (coin comes “HEADS”) instead 77 patients May 2017 (coin comes “TAILS”). , assign intervention patients seen month specified coin assign usual care without intervention patients seen month. trying assess effect intervention triglyceride level October 2017 using sample 154 people, use dependent (paired) independent samples?Suppose take convenient sample 77 patients population patients complete blood test April 2017 including check triglycerides, triglyceride level high category (200 499 mg/dl). Next, take convenient sample 77 patients population patients complete blood test May 2017 including check triglycerides, triglyceride level high category (200 499 mg/dl). flip coin determine whether intervention given 77 patients April 2017 (coin comes “HEADS”) instead 77 patients May 2017 (coin comes “TAILS”). , assign intervention patients seen month specified coin assign usual care without intervention patients seen month. trying assess effect intervention triglyceride level October 2017 using sample 154 people, use dependent (paired) independent samples?Suppose take convenient sample 200 patients population patients complete blood test April 2017 including check triglycerides, triglyceride level high category (200 499 mg/dl). patient, re-measure October 2017, checking triglyceride level. , take first 77 patients randomly sorted list assign intervention (takes place June September 2017) take additional group 77 patients remaining part list assign usual care without intervention time period. trying assess effect intervention individual’s change triglyceride level (April/May October) using sample 154 people, use dependent (paired) independent samples?Suppose take convenient sample 200 patients population patients complete blood test April 2017 including check triglycerides, triglyceride level high category (200 499 mg/dl). patient, re-measure October 2017, checking triglyceride level. , take first 77 patients randomly sorted list assign intervention (takes place June September 2017) take additional group 77 patients remaining part list assign usual care without intervention time period. trying assess effect intervention individual’s change triglyceride level (April/May October) using sample 154 people, use dependent (paired) independent samples?Answers “tricky” examples appear end Chapter.","code":""},{"path":"comparing-means-with-paired-samples.html","id":"a-more-complete-decision-support-tool-comparing-means","chapter":"18 Comparing Means with Paired Samples","heading":"18.15 A More Complete Decision Support Tool: Comparing Means","text":"paired independent samples?paired independent samples?paired samples, paired differences approximately Normally distributed?\nyes, paired t test confidence interval likely best choice.\n, main concern outliers (generally symmetric data), skew?\npaired differences appear generally symmetric substantial outliers, Wilcoxon signed rank test appropriate choice, bootstrap confidence interval population mean paired differences.\npaired differences appear seriously skewed, ’ll usually build bootstrap confidence interval, although sign test another reasonable possibility, although doesn’t provide confidence interval population mean paired differences.\n\npaired samples, paired differences approximately Normally distributed?yes, paired t test confidence interval likely best choice., main concern outliers (generally symmetric data), skew?\npaired differences appear generally symmetric substantial outliers, Wilcoxon signed rank test appropriate choice, bootstrap confidence interval population mean paired differences.\npaired differences appear seriously skewed, ’ll usually build bootstrap confidence interval, although sign test another reasonable possibility, although doesn’t provide confidence interval population mean paired differences.\npaired differences appear generally symmetric substantial outliers, Wilcoxon signed rank test appropriate choice, bootstrap confidence interval population mean paired differences.paired differences appear seriously skewed, ’ll usually build bootstrap confidence interval, although sign test another reasonable possibility, although doesn’t provide confidence interval population mean paired differences.independent, sample Normally distributed?\n–> use Wilcoxon-Mann-Whitney rank sum test bootstrap via bootdif.\nYes –> sample sizes equal?\nBalanced Design (equal sample sizes) - use pooled t test\nUnbalanced Design - use Welch test\n\nindependent, sample Normally distributed?–> use Wilcoxon-Mann-Whitney rank sum test bootstrap via bootdif.Yes –> sample sizes equal?\nBalanced Design (equal sample sizes) - use pooled t test\nUnbalanced Design - use Welch test\nBalanced Design (equal sample sizes) - use pooled t testUnbalanced Design - use Welch test","code":""},{"path":"comparing-means-with-paired-samples.html","id":"answers-for-the-three-tricky-examples","chapter":"18 Comparing Means with Paired Samples","heading":"18.15.1 Answers for the Three “Tricky” Examples","text":"Answer 1. first task identify outcome exposure groups. , comparing distribution outcome (triglyceride level October) across two exposures: () receiving intervention (b) receiving intervention. sample 77 patients receiving intervention, different sample 77 patients receiving usual care. 77 subjects receiving intervention matched (age sex) specific subject receiving intervention. , can calculate paired differences taking triglyceride level exposed member pair subtracting triglyceride level usual care member pair. Thus comparison exposure groups accomplished using dependent samples analysis, paired t test.Answer 2. , begin identfying outcome (triglyceride level October) exposure groups. , compare two exposures: () receiving intervention (b) receiving usual care. sample 77 patients receiving intervention, different sample 77 patients receiving usual care. pairing matching involved. connection implied way data collected implies , example, patient 1 intervention group linked particular subject usual care group. need analyze data using independent samples.Answer 3. , identfy outcome (now within-subject change triglyceride level April October) exposure groups. , compare two exposures: () receiving intervention (b) receiving usual care. sample 77 patients receiving intervention, different sample 77 patients receiving usual care. , pairing matching patients receiving intervention patients receiving usual care. outcome value difference (change) triglyceride levels, ’s connection implied way data collected implies , example, patient 1 intervention group linked particular subject usual care group. , , need analyze data using independent samples.background fundamental material, might consider Wikipedia pages Paired Difference Test Blocking (statistics).","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"hypothesis-testing-what-is-it-good-for","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19 Hypothesis Testing: What is it good for?","text":"Hypothesis testing uses sample data attempt reject hypothesis nothing interesting happening – , reject notion chance alone can explain sample results. can, many settings, use confidence intervals summarize results, well, confidence intervals hypothesis tests closely connected.particular, ’s worth stressing :significant effect necessarily thing interesting effect. example, results calculated large samples nearly always “significant” even effects quite small magnitude. test, always ask effect large enough practical interest. , test?significant effect necessarily thing interesting effect. example, results calculated large samples nearly always “significant” even effects quite small magnitude. test, always ask effect large enough practical interest. , test?non-significant effect necessarily thing difference. large effect real practical interest may still produce non-significant result simply sample small.non-significant effect necessarily thing difference. large effect real practical interest may still produce non-significant result simply sample small.assumptions behind statistical inferences. Checking assumptions crucial validating inference made test confidence interval.assumptions behind statistical inferences. Checking assumptions crucial validating inference made test confidence interval.","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"five-steps-in-any-hypothesis-test","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.1 Five Steps in any Hypothesis Test","text":"Specify null hypothesis, \\(H_0\\) (usually indicates difference association results various groups subjects)Specify research alternative hypothesis, \\(H_A\\), sometimes called \\(H_1\\) (usually indicates difference association results groups subjects).Specify test procedure test statistic used make inferences population based sample data. usually specify \\(\\alpha\\), probability incorrectly rejecting \\(H_0\\) willing accept. absence information, often use \\(\\alpha = 0.05\\)Obtain data, summarize obtain relevant test statistic, gets summarized \\(p\\) value.Use \\(p\\) value either\nreject \\(H_0\\) favor alternative \\(H_A\\) (concluding statistically significant difference/association \\(\\alpha\\) significance level) \nretain \\(H_0\\) (conclude statistically significant difference/association \\(\\alpha\\) significance level)\nreject \\(H_0\\) favor alternative \\(H_A\\) (concluding statistically significant difference/association \\(\\alpha\\) significance level) orretain \\(H_0\\) (conclude statistically significant difference/association \\(\\alpha\\) significance level)","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"type-i-and-type-ii-error","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.2 Type I and Type II Error","text":"know unlikely results null hypothesis true, must make one two choices:p value small enough convincingly rule chance. Therefore, reject null hypothesis explanation results.p value small enough convincingly rule chance. reject null hypothesis accept alternative hypothesis.small must p value order rule null hypothesis? standard choice 5%. standardization substantial disadvantages. simply convention become accepted years, many situations 5% cutoff unwise. give specific level keep mind, suggests rather mindless cutpoint nothing importance decision costs losses associated outcomes.","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"the-courtroom-analogy","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.3 The Courtroom Analogy","text":"Consider analogy jury courtroom.evidence strong enough convincingly rule defendant innocent. Therefore, reject null hypothesis, innocence defendant.evidence strong enough willing rule possibility innocent person (stated null hypothesis) produced observed data. reject null hypothesis, defendant innocent, assert alternative hypothesis.Consistent thinking hypothesis testing, many cases accept hypothesis defendant innocent. simply conclude evidence strong enough rule possibility innocence.p value probability getting result extreme extreme one observed proposed null hypothesis true. Notice valid actually accept null hypothesis true. say essentially convinced chance alone produced observed results – common mistake.","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"significance-vs.-importance","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.4 Significance vs. Importance","text":"Remember statistically significant relationship difference necessarily mean important one. result significant statistical meaning word may significant clinically. Statistical significance technical term. Findings can statistically significant practically significant either neither.large samples, regularly find small differences small p value even though practical importance. extreme, small samples, even large differences often large enough create small p value. notion statistical significance helped science, won’t perpetuate .","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"what-does-dr.-love-dislike-about-p-values-and-statistical-significance","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.5 What does Dr. Love dislike about p values and “statistical significance?”","text":"lot things. major issue believe p values impossible explain way [] technically correct [b] straightforward time. evidence , might want look article associated video Christie Aschwanden 538.comThe notion p value incredibly impressive achievement back Wald others work 1940s, might still useful recently 10 years ago. notion p value relies lot flawed assumptions, null hypothesis significance testing fraught difficulties. Nonetheless, researchers use p values every day.","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"the-asa-articles-in-2016-and-2019-on-statistical-significance-and-p-values","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.6 The ASA Articles in 2016 and 2019 on Statistical Significance and P-Values","text":"However, primary motivation taking approach ’m taking comes pieces two key reference collections ’ll read discuss thoroughly 431 432.American Statistical Association’s 2016 Statement p-Values: Context, Process Purpose.ASA Statement p-Values Statistical Significance (Wasserstein Lazar 2016) developed primarily decades, warnings don’ts gone mostly unheeded. statement , widespread agreement don’ts.Statistical Inference 21st Century: World Beyond p < 0.05 2019 American StatisticianThis world researchers free treat “p = 0.051” “p = 0.049” categorically different, authors longer find constrained selectively publish results based single magic number. world, studies “p < 0.05” studies “p > 0.05” automatically conflict, researchers see results easily replicated-, even , better understand . venture path, begin see fewer false alarms, fewer overlooked discoveries, development customized statistical strategies. Researchers free communicate findings glorious uncertainty, knowing work judged quality effective communication science, p-values. “statistical significance” used less, statistical thinking used . ASA Statement P-Values Statistical Significance started moving us toward world…. Now must go .ASA Statement P-Values Statistical Significance stopped just short recommending declarations “statistical significance” abandoned. take step . conclude, based review articles special issue broader literature, time stop using term “statistically significant” entirely. variants “significantly different,” “p < 0.05,” “nonsignificant” survive, whether expressed words, asterisks table, way…. Regardless whether ever useful, declaration “statistical significance” today become meaningless.moment, say . emphasize confidence intervals p values, best partial solution. …rarely situation emerge p value can available looking associated confidence interval isn’t far helpful making comparison interest.use p value requires making least many assumptions population, sample, individuals data confidence interval.null hypotheses clearly exactly true prior data collection, test summarized p value questionable value time.one truly adequate definition p value, terms precision parsimony. Brief, understandable definitions always fail technically accurate.Bayesian approaches avoid pitfalls, come issues.Many smart people agree , sworn p values whenever can., ’ll look issues greater depth later course.","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"errors-in-hypothesis-testing","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.7 Errors in Hypothesis Testing","text":"testing hypotheses, two potential decisions one brings possibility mistake made.Let’s use courtroom analogy. potential choices associated potential errors. Although seriousness errors depends seriousness crime punishment, potential error choice 2 usually serious.rule defendant innocent, (s)set free without penalty.\nPotential Error: criminal erroneously freed.\nPotential Error: criminal erroneously freed.believe enough evidence conclude defendant guilty.\nPotential Error: innocent person convicted / penalized guilty person remains free.\nPotential Error: innocent person convicted / penalized guilty person remains free.another example, consider tested disease. tests diseases 100% accurate. lab technician physician must make choice:opinion medical practitioner, healthy. test result weak enough called “negative” disease.\nPotential Error: actually disease told . called false negative.\nPotential Error: actually disease told . called false negative.opinion medical practitioner, disease. test results strong enough called “positive” disease.\nPotential Error: actually healthy told disease. called false positive.\nPotential Error: actually healthy told disease. called false positive.","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"the-two-types-of-hypothesis-testing-errors","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.8 The Two Types of Hypothesis Testing Errors","text":"Type error can made null hypothesis actually true.Type II error can made alternative hypothesis actually true.","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"the-significance-level-is-the-probability-of-a-type-i-error","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.9 The Significance Level is the Probability of a Type I Error","text":"null hypothesis true, p value probability making error choosing alternative hypothesis instead. Alpha (\\(\\alpha\\)) defined probability rejecting null hypothesis null hypothesis actually true, creating Type error. also called significance level, 100(1-\\(\\alpha\\)) confidence level – probability correctly concluding difference (retaining \\(H_0\\)) null hypothesis true.","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"the-probability-of-avoiding-a-type-ii-error-is-called-power","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.10 The Probability of avoiding a Type II Error is called Power","text":"Type II error made alternative hypothesis true, fail choose . probability depends exactly part alternative hypothesis true, computing probability making Type II error feasible. power test probability making correct decision alternative hypothesis true. Beta (\\(\\beta\\)) defined probability concluding difference, fact one (Type II error). Power just 1 - \\(\\beta\\), probability concluding difference, , fact, one.Traditionally, people like power test least 80%, meaning \\(\\beta\\) 0.20. Often, ’ll arguing 90% minimum power requirement, ’ll presenting range power calculations variety sample size choices.","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"incorporating-the-costs-of-various-types-of-errors","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.11 Incorporating the Costs of Various Types of Errors","text":"error serious medical testing, think \\(H_0\\): patient healthy vs. \\(H_A\\): disease present?depends disease consequences negative positive test result. false negative screening test cancer lead fatal delay treatment, whereas false positive probably lead retest. troublesome example occurs testing infectious disease. Inevitably, trade-two types errors. depends consequences.nice specify probability making error potential decision. weigh consequence error probability. Unfortunately, cases, can specify conditional probability making Type error, given null hypothesis true.deciding whether reject null hypothesis, need consider consequences two potential types errors. Type error serious, reject null hypothesis p value small. Conversely, Type II error serious, willing reject null hypothesis larger p value, perhaps 0.10 0.20, instead 0.05.\\(\\alpha\\) probability rejecting \\(H_0\\) \\(H_0\\) true.\n1 - \\(\\alpha\\), confidence level, probability retaining \\(H_0\\) ’s right thing .\n1 - \\(\\alpha\\), confidence level, probability retaining \\(H_0\\) ’s right thing .\\(\\beta\\) probability retaining \\(H_0\\) \\(H_A\\) true.\n1 - \\(\\beta\\), power, probability rejecting \\(H_0\\) ’s right thing .\n1 - \\(\\beta\\), power, probability rejecting \\(H_0\\) ’s right thing .","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"power-and-sample-size-considerations","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.12 Power and Sample Size Considerations","text":"statistical tests, theoretically possible estimate power test design stage, (data collected) various sample sizes, can hone sample size choice enable us collect data many subjects truly necessary.power calculation likely common element scientific grant proposal statistician consulted. fine idea theory, practice…tests power calculations worked intensive detail using R mostly substantial assumptions. Examples include t tests assume population normality, common population variance balanced designs independent samples setting, paired t tests assume population normality paired samples setting.power calculations also usually based tests rather confidence intervals, much useful settings. Simulation friend .Even unfortunately, process power related calculations far art science.result, value many power calculations negligible, since assumptions made arbitrary poorly connected real data.several occasions, stood front large audience medical statisticians actively engaged clinical trials studies require power calculations funding. ask show hands people power calculations prior study whose assumptions matched eventual data perfectly, get lots laughs. doesn’t happen.Even underlying framework assumes power 80% significance level 5% sufficient studies pretty silly.said, feel obliged show examples power calculations done using R, provide insight make key assumptions way won’t alert reviewers much silliness enterprise. situations described Chapter toy problems, may instructive fundamental ideas.","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"sample-size-in-a-one-sample-t-test","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.13 Sample Size in a One-Sample t test","text":"t test, R can estimate one following elements, given four, using power.t.test command, either one-tailed two-tailed single-sample t test…n = sample size\\(\\delta\\) = delta = true difference population means null hypothesis value particular alternatives = sd = true standard deviation population\\(\\alpha\\) = sig.level = significance level test (maximum acceptable risk Type error)1 - \\(\\beta\\) = power = power t test detect effect size \\(\\delta\\)","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"a-toy-example","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.13.1 A Toy Example","text":"Suppose recent health survey, average beef consumption U.S. per person 90 pounds per year. Suppose planning new study see beef consumption levels changed. plan take random sample 25 people build new estimate, test whether current pounds beef consumed per year 90. Suppose want two-sided (two-tailed) test 95% confidence (\\(\\alpha\\) = 0.05), expect true difference need least \\(\\delta\\) = 5 pounds (.e. 85 less 95 ) order result real, practical interest. Suppose also willing assume true standard deviation measurements population 10 pounds., course, lot suppose.Now, want know power proposed experiment detect change 5 pounds () away original 90 pounds, specifications, tweaking specifications affect power study., \n- n = 25 data points collected\n- \\(\\delta\\) = 5 pounds minimum clinically meaningful effect size\n- s = 10 assumed population standard deviation, pounds per year\n- \\(\\alpha\\) 0.05, ’ll two-sided test","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"using-the-power.t.test-function","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.13.2 Using the power.t.test function","text":", study design, expect detect effect size \\(\\delta\\) = 5 pounds just 67% power, .e. probability incorrect retention \\(H_0\\) just 1/3. time, ’d like improve power, , ’d need adjust assumptions.","code":"\npower.t.test(n = 25, delta = 5, sd = 10, sig.level = 0.05, \n             type=\"one.sample\", alternative=\"two.sided\")\n     One-sample t test power calculation \n\n              n = 25\n          delta = 5\n             sd = 10\n      sig.level = 0.05\n          power = 0.6697014\n    alternative = two.sided"},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"changing-assumptions","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.14 Changing Assumptions","text":"made assumptions sample size n, minimum clinically meaningful effect size (change population mean) \\(\\delta\\), population standard deviation s, significance level \\(\\alpha\\), mention decisions test, like ’d one-sample t test, rather another sort test single sample, ’d two-tailed, two-sided test. Often, assumptions tweaked bit make power look like reviewer/funder hoping see.","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"increasing-sample-size-increases-power","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.14.1 Increasing Sample Size Increases Power","text":"Suppose, committed using resources gathering data 40 subjects instead 25 assumed initially – effect power?samples, powerful test, able detect difference greater probability. fact, sample 40 paired differences yields 87% power. turns , need least 44 observations scenario get 90% power, shown calculation , puts power , leaves sample size.see need least 44 observations achieve 90% power. Note: always round sample size power calculation – calculation actually suggested n = 43.1 paired differences needed, still rounded 44.","code":"\npower.t.test(n = 40, delta = 5, sd = 10, sig.level = 0.05, \n             type=\"one.sample\", alternative=\"two.sided\")\n     One-sample t test power calculation \n\n              n = 40\n          delta = 5\n             sd = 10\n      sig.level = 0.05\n          power = 0.8693979\n    alternative = two.sided\npower.t.test(power=0.9, delta = 5, sd = 10, sig.level = 0.05, \n             type=\"one.sample\", alternative=\"two.sided\")\n     One-sample t test power calculation \n\n              n = 43.99552\n          delta = 5\n             sd = 10\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided"},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"increasing-effect-size-will-increase-power","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.14.2 Increasing Effect Size will increase Power","text":"larger effect easier detect. go back original calculation, 67% power detect effect size \\(\\delta\\) = 5, now change desired effect size \\(\\delta\\) = 6 pounds (.e. value 84 less 96 ), obtain powerful design.see change effect size 5 6, leaving everything else , increases power 67% 82%. reach 90% power, ’d need increase effect size trying detect least 6.76 pounds., note rounding .Using \\(\\delta\\) = 6.75 quite make 90.00% power.Using \\(\\delta\\) = 6.76 guarantees power 90% , just round 90%.","code":"\npower.t.test(n = 25, delta = 6, sd = 10, sig.level = 0.05, \n             type=\"one.sample\", alternative=\"two.sided\")\n     One-sample t test power calculation \n\n              n = 25\n          delta = 6\n             sd = 10\n      sig.level = 0.05\n          power = 0.8207213\n    alternative = two.sided\npower.t.test(n = 25, power = 0.9, sd = 10, sig.level = 0.05, \n             type=\"one.sample\", alternative=\"two.sided\")\n     One-sample t test power calculation \n\n              n = 25\n          delta = 6.759051\n             sd = 10\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided"},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"decreasing-the-standard-deviation-will-increase-power","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.14.3 Decreasing the Standard Deviation will increase Power","text":"choice standard deviation usually motivated pilot study, else pulled thin air - ’s relatively easy convince true standard deviation might little smaller ’d guessed initially. Let’s see happens power reduce sample standard deviation 10 pounds 9. make effect 5 pounds easier detect, smaller variation associated .change standard deviation 10 9, leaving everything else , increases power 67% nearly 76%. reach 90% power, ’d need decrease standard deviation population paired differences 7.39 pounds.Note rounding .Using s = 7.4 pounds quite make 90.00% power.Note also order get R treat standard debviation unknown, must specify NULL formula.","code":"\npower.t.test(n = 25, delta = 5, sd = 9, sig.level = 0.05, \n             type=\"one.sample\", alternative=\"two.sided\")\n     One-sample t test power calculation \n\n              n = 25\n          delta = 5\n             sd = 9\n      sig.level = 0.05\n          power = 0.759672\n    alternative = two.sided\npower.t.test(n = 25, delta = 5, sd = NULL, power = 0.9, sig.level = 0.05, \n             type=\"one.sample\", alternative=\"two.sided\")\n     One-sample t test power calculation \n\n              n = 25\n          delta = 5\n             sd = 7.397486\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided"},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"larger-significance-level-increases-power","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.14.4 Larger Significance Level increases Power","text":"can trade Type II error (lack power) Type error. willing trade Type error (described \\(\\alpha\\)), can improve power. instance, suppose decided run original test 90% confidence.calculation suggests power thus increase 67% just 78%.","code":"\npower.t.test(n = 25, delta = 5, sd = 10, sig.level = 0.1, \n             type=\"one.sample\", alternative=\"two.sided\")\n     One-sample t test power calculation \n\n              n = 25\n          delta = 5\n             sd = 10\n      sig.level = 0.1\n          power = 0.7833861\n    alternative = two.sided"},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"paired-sample-t-tests-and-powersample-size","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.15 Paired Sample t Tests and Power/Sample Size","text":"paired-samples t test, R can estimate one following elements, given four, using power.t.test command, either one-tailed two-tailed paired t test…n = sample size (# pairs) compared\\(\\delta\\) = delta = true difference means two groupss = sd = true standard deviation paired differences\\(\\alpha\\) = sig.level = significance level comparison (maximum acceptable risk Type error)1 - \\(\\beta\\) = power = power paired t test detect effect size \\(\\delta\\)","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"a-toy-example-1","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.15.1 A Toy Example","text":"toy example, suppose planning paired samples experiment involving n = 30 subjects provide “” “” result, measured days.Suppose want two-sided (two-tailed) test 95% confidence (\\(\\alpha\\) = 0.05), expect true difference “” “” groups least \\(\\delta\\) = 5 days real interest. Suppose also willing assume true standard deviation paired differences 10 days., course, lot suppose.Now, want know power proposed experiment detect difference specifications, tweaking specifications affect power study., \n- n = 30 paired differences collected\n- \\(\\delta\\) = 5 days minimum clinically meaningful difference\n- s = 10 days assumed population standard deviation paired differences\n- \\(\\alpha\\) 0.05, ’ll two-sided test","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"using-the-power.t.test-function-1","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.15.2 Using the power.t.test function","text":", study design, expect detect effect size \\(\\delta\\) = 5 days 75% power, .e. probability incorrect retention \\(H_0\\) 0.25. time, ’d like improve power, , ’d need adjust assumptions.","code":"\npower.t.test(n = 30, delta = 5, sd = 10, sig.level = 0.05, \n             type=\"paired\", alternative=\"two.sided\")\n     Paired t test power calculation \n\n              n = 30\n          delta = 5\n             sd = 10\n      sig.level = 0.05\n          power = 0.7539627\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs"},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"changing-assumptions-in-a-power-calculation","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.15.3 Changing Assumptions in a Power Calculation","text":"made assumptions sample size n, minimum clinically meaningful difference means \\(\\delta\\), population standard deviation s, significance level \\(\\alpha\\), mention decisions test, like ’d paired t test, rather another sort test paired samples, use independent samples approach, ’d two-tailed, two-sided test. Often, assumptions tweaked bit make power look like reviewer/funder hoping see.","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"changing-the-sample-size","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.15.4 Changing the Sample Size","text":"Suppose, committed using resources gathering “” “” data 40 subjects instead 30 assumed initially – effect power?samples, powerful test, able detect difference greater probability. fact, sample 40 paired differences yields 87% power. turns , need least 44 paired differences scenario get 90% power, shown calculation , puts power , leaves sample size.see need least 44 paired differences achieve 90% power. Note: always round sample size power calculation – calculation actually suggested n = 43.1 paired differences needed, still rounded 44.","code":"\npower.t.test(n = 40, delta = 5, sd = 10, sig.level = 0.05, \n             type=\"paired\", alternative=\"two.sided\")\n     Paired t test power calculation \n\n              n = 40\n          delta = 5\n             sd = 10\n      sig.level = 0.05\n          power = 0.8693979\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs\npower.t.test(power=0.9, delta = 5, sd = 10, sig.level = 0.05, \n             type=\"paired\", alternative=\"two.sided\")\n     Paired t test power calculation \n\n              n = 43.99552\n          delta = 5\n             sd = 10\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs"},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"changing-the-effect-size","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.15.5 Changing the Effect Size","text":"larger effect easier detect. go back original calculation, 75% power detect effect (.e. true population mean difference) size \\(\\delta\\) = 5, now change desired effect size \\(\\delta\\) = 6, obtain powerful design.see change effect size 5 6, leaving everything else , increases power 75% nearly 89%. reach 90% power, ’d need increase effect size trying detect least 6.13 days., note rounding .Using \\(\\delta\\) = 6.12 quite make 90.00% power.Using \\(\\delta\\) = 6.13 guarantees power 90% , just round 90%..","code":"\npower.t.test(n = 30, delta = 6, sd = 10, sig.level = 0.05, \n             type=\"paired\", alternative=\"two.sided\")\n     Paired t test power calculation \n\n              n = 30\n          delta = 6\n             sd = 10\n      sig.level = 0.05\n          power = 0.887962\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs"},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"changing-the-standard-deviation","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.15.6 Changing the Standard Deviation","text":"choice standard deviation usually motivated pilot study, else pulled thin air. ’s relatively easy convince true standard deviation might little smaller ’d guessed initially. Let’s see happens power reduce sample standard deviation 10 days 9 days. make effect 5 days easier detect different null hypothesized value 0, smaller variation associated .change standard deviation 10 9, leaving everything else , increases power 75% nearly 84%. reach 90% power, ’d need decrease standard deviation population paired differences 8.16 days.Note rounding , using \\(s\\) = 8.17 days quite make 90.00% power. Note also order get R treat sd unknown, must specify NULL formula…","code":"\npower.t.test(n = 30, delta = 5, sd = 9, sig.level = 0.05, \n             type=\"paired\", alternative=\"two.sided\")\n     Paired t test power calculation \n\n              n = 30\n          delta = 5\n             sd = 9\n      sig.level = 0.05\n          power = 0.8366514\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs\npower.t.test(n = 30, delta = 5, sd = NULL, power = 0.9, \n             sig.level = 0.05, type=\"paired\", alternative=\"two.sided\")\n     Paired t test power calculation \n\n              n = 30\n          delta = 5\n             sd = 8.163989\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs"},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"changing-the-significance-level","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.15.7 Changing the Significance Level","text":"can trade Type II error (lack power) Type error. willing trade Type error (described \\(\\alpha\\)), can improve power. instance, suppose decided run original test 90% confidence.calculation suggests power thus increase 75% nearly 85%.","code":"\npower.t.test(n = 30, delta = 5, sd = 10, sig.level = 0.1, \n             type=\"paired\", alternative=\"two.sided\")\n     Paired t test power calculation \n\n              n = 30\n          delta = 5\n             sd = 10\n      sig.level = 0.1\n          power = 0.8482542\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs"},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"two-independent-samples-power-for-t-tests","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.16 Two Independent Samples: Power for t Tests","text":"independent-samples t test, balanced design (\\(n_1\\) = \\(n_2\\)), R can estimate one following elements, given four, using power.t.test command, either one-tailed two-tailed t test…n = sample size two groups compared\\(\\delta\\) = delta = true difference means two groupss = sd = true standard deviation individual values group (assumed constant – since assume equal population variances)\\(\\alpha\\) = sig.level = significance level comparison (maximum acceptable risk Type error)1 - \\(\\beta\\) = power = power t test detect effect size \\(\\delta\\)method produces power calculations balanced designs – sample size equal two groups. want two-sample power calculation unbalanced design, need use different library function R, ’ll see.","code":""},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"a-new-example","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.17 A New Example","text":"Suppose plan study time relapse patients drug trial, subjects assigned randomly (new) treatment placebo. Suppose anticipate placebo group mean 9 months, want detect improvement (increase) time relapse 50%, treatment group mean least 13.5 months. ’ll use \\(\\alpha\\) = .10 \\(\\beta\\) = .10, well. Assume ’d two-sided test, equal number observations group, ’ll assume observed standard deviation 9 months pilot study hold , well.want sample size required test two sample setting :\\(\\alpha\\) = .10,90% power (\\(\\beta\\) = .10),equal numbers samples placebo group (group 1) treatment group (group 2).’ll plug observed standard deviation 9 months.’ll look detecting change 9 [average placebo group] 13.5 (difference 50%, giving delta = 4.5)using two-sided pooled t-test.appropriate R command :suggests need sample least 70 subjects treated group additional 70 subjects placebo group, total 140 subjects.","code":"\npower.t.test(delta = 4.5, sd = 9, \n             sig.level = 0.10, power = 0.9, \n             type=\"two.sample\", \n             alternative=\"two.sided\")\n     Two-sample t test power calculation \n\n              n = 69.19782\n          delta = 4.5\n             sd = 9\n      sig.level = 0.1\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group"},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"another-scenario","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.17.1 Another Scenario","text":"resources sparse, ’ll forced study 120 subjects, overall? require 90% confidence two-sided test, power ?looks like power circumstances just 86%. Note n = 60 refers half total sample size, since ’ll need 60 drug 60 placebo subjects balanced design.","code":"\npower.t.test(n = 60, delta = 4.5, sd = 9, \n             sig.level = 0.10,\n             type=\"two.sample\", \n             alternative=\"two.sided\")\n     Two-sample t test power calculation \n\n              n = 60\n          delta = 4.5\n             sd = 9\n      sig.level = 0.1\n          power = 0.859484\n    alternative = two.sided\n\nNOTE: n is number in *each* group"},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"power-for-independent-sample-t-tests-with-unbalanced-designs","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.18 Power for Independent Sample T tests with Unbalanced Designs","text":"Using pwr package, R can sample size calculations describe power two-sample t test require balanced design using pwr.t2n.test command.Suppose wanted study described , using 100 “treated” patients “placebo” patients possible. sample size required maintain 90% power? one change – effect size d pwr.t2n.test command specified using difference means \\(\\delta\\) used previously, divided standard deviation s used previously. , old setup, assumed delta = 4.5, sd = 9, now ’ll assume d = 4.5/9 instead.need least 53 subjects “placebo” group.","code":"\npwr::pwr.t2n.test(n1 = 100, d = 4.5/9, \n             sig.level = 0.1, power = 0.9,\n             alternative=\"two.sided\")\n     t test power calculation \n\n             n1 = 100\n             n2 = 52.82433\n              d = 0.5\n      sig.level = 0.1\n          power = 0.9\n    alternative = two.sided"},{"path":"hypothesis-testing-what-is-it-good-for.html","id":"the-most-efficient-design-for-an-independent-samples-comparison-will-be-balanced.","chapter":"19 Hypothesis Testing: What is it good for?","heading":"19.18.1 The most efficient design for an independent samples comparison will be balanced.","text":"Note use \\(n_1\\) = 100 subjects treated group, need least \\(n_2\\) = 53 placebo group achieve 90% power, total 153 subjects.Compare balanced design, needed 70 subjects group achieve power, thus, total 140 subjects.saw earlier test 60 subjects group yield just 86% power. Suppose instead built test 80 subjects treated group, 40 placebo group, power ?’d expect, power stronger balanced design unbalanced design overall sample size.Note used two-sided test establish power calculation – general, conservative defensible approach calculation, unless strong specific reason use one-sided approach building power calculation, don’t.","code":"\npwr::pwr.t2n.test(n1 = 80, n2 = 40, d = 4.5/9, \n             sig.level = 0.10,\n             alternative=\"two.sided\")\n     t test power calculation \n\n             n1 = 80\n             n2 = 40\n              d = 0.5\n      sig.level = 0.1\n          power = 0.821823\n    alternative = two.sided"},{"path":"two-examples-comparing-means.html","id":"two-examples-comparing-means","chapter":"20 Two Examples Comparing Means","heading":"20 Two Examples Comparing Means","text":"","code":""},{"path":"two-examples-comparing-means.html","id":"a-study-of-battery-life","chapter":"20 Two Examples Comparing Means","heading":"20.1 A Study of Battery Life","text":"buy generic rather brand-name batteries? Bock, Velleman, De Veaux48 descibe designed experiment test battery life. (male) student obtained six pairs AA alkaline batteries two major battery manufacturers; well-known brand name generic brand, battery brand factor interest.estimate difference mean lifetimes across two manufacturers, student kept battery-powered CD player CD running continuously, volume control fixed 5, measured time music heard headphones. (ran initial trial find approximately long take, didn’t spend first 3 hours run listening CD.) outcome time minutes sound stopped. account changes CD player’s performance time, randomized run order choosing pairs batteries (CD-player required two batteries run) random.results 6 brand name 6 generic tests, minutes, found battery.csv data file, run indicates order tests run…","code":"\nbattery <- read_csv(\"data/battery.csv\",\n                    show_col_types = FALSE)\n\nbattery# A tibble: 12 x 4\n     run  test type        time\n   <dbl> <dbl> <chr>      <dbl>\n 1     1     1 brand name  191.\n 2     2     2 brand name  206.\n 3     6     3 brand name  199.\n 4     8     4 brand name  172.\n 5     9     5 brand name  184 \n 6    12     6 brand name  170.\n 7     3     1 generic     194 \n 8     4     2 generic     204.\n 9     5     3 generic     204.\n10     7     4 generic     206.\n11    10     5 generic     222.\n12    11     6 generic     209."},{"path":"two-examples-comparing-means.html","id":"question-1.-what-is-the-outcome-under-study","chapter":"20 Two Examples Comparing Means","heading":"20.1.1 Question 1. What is the outcome under study?","text":"studying battery lifetimes (time sound stopped) minutes.","code":""},{"path":"two-examples-comparing-means.html","id":"question-2.-what-are-the-treatmentexposure-groups","chapter":"20 Two Examples Comparing Means","heading":"20.1.2 Question 2. What are the treatment/exposure groups?","text":"comparing two brands batteries: well-known vs. generic.","code":""},{"path":"two-examples-comparing-means.html","id":"question-3.-are-the-data-collected-using-paired-or-independent-samples","chapter":"20 Two Examples Comparing Means","heading":"20.1.3 Question 3. Are the data collected using paired or independent samples?","text":"course, different numbers samples two groups, ’d know without thought independent samples required. Since 6 observations brand name group, also 6 observations generic group, .e. balanced design, need pause now decide whether paired independent samples testing appropriate setting.Two samples paired data point one sample naturally linked specific data point sample. , paired independent samples?Despite way ’ve set data table, particular reason pair, say, run #1 (brand name run) particular experimental run generic group. samples independent. matched-pairs design.trial, student either used two well-known batteries, two generic batteries.tests/confidence intervals independent samples methods suggests statistically significant (5% level) difference generic brand name batteries.","code":""},{"path":"two-examples-comparing-means.html","id":"question-4.-are-the-data-a-random-sample-from-the-population-of-interest","chapter":"20 Two Examples Comparing Means","heading":"20.1.4 Question 4. Are the data a random sample from the population of interest?","text":"Probably . data likely come convenient sample batteries. don’t know might bias study, though. seems unlikely particular bias unless, example, well-known batteries substantially older younger generic.","code":""},{"path":"two-examples-comparing-means.html","id":"question-5.-what-significance-level-will-we-use","chapter":"20 Two Examples Comparing Means","heading":"20.1.5 Question 5. What significance level will we use?","text":"reason use 95% confidence level.","code":""},{"path":"two-examples-comparing-means.html","id":"question-6.-are-we-using-a-one-sided-or-two-sided-comparison","chapter":"20 Two Examples Comparing Means","heading":"20.1.6 Question 6. Are we using a one-sided or two-sided comparison?","text":"argue one-sided comparison, ’ll safe use two-sided version.","code":""},{"path":"two-examples-comparing-means.html","id":"question-9.-what-does-the-distribution-of-outcomes-in-each-group-tell-us","chapter":"20 Two Examples Comparing Means","heading":"20.1.7 Question 9. What does the distribution of outcomes in each group tell us?","text":"can generate histograms, , ’s issue, observations.sure looks like generic batteries lasted longer. also look like consistent. sample means 206.6 generic group, 186.9 minutes brand name, point estimate difference 19.7 minutes.question : can confident difference observe just random fluctuation, 5% significance level?","code":"\nggplot(battery, aes(x = type, y = time, fill = type)) +\n  geom_jitter(aes(color = type), alpha = 0.75, width = 0.125) +\n  geom_boxplot(alpha = 0.5) +\n  coord_flip() +\n  guides(fill = \"none\", col = \"none\") +\n  labs(title = \"Battery Running Time, by Manufacturer\",\n       y = \"Running Time (minutes)\", x = \"Manufacturer\")\nggplot(battery, aes(x = time, fill = type)) +\n  geom_histogram(bins = 6, col = \"white\") +\n  facet_wrap(~ type) +\n  guides(fill = \"none\") + \n  labs(title = \"Battery Running Time, by Manufacturer\")\nmosaic::favstats(time ~ type, data = battery)        type   min    Q1 median      Q3   max     mean\n1 brand name 169.5 175.3 187.35 197.075 205.5 186.8833\n2    generic 194.0 203.5 205.00 208.675 222.5 206.5667\n         sd n missing\n1 14.374341 6       0\n2  9.366251 6       0"},{"path":"two-examples-comparing-means.html","id":"inferential-results-for-the-battery-study","chapter":"20 Two Examples Comparing Means","heading":"20.1.8 Inferential Results for the Battery Study","text":"table , summarized two-sided testing results ways looked two sample comparison far, 95% confidence intervals. samples really paired, must choose paired samples comparisons described table. samples really independent, must choose independent samples comparisons.","code":""},{"path":"two-examples-comparing-means.html","id":"paired-samples-approaches","chapter":"20 Two Examples Comparing Means","heading":"20.1.9 Paired Samples Approaches","text":"","code":""},{"path":"two-examples-comparing-means.html","id":"independent-samples-approaches","chapter":"20 Two Examples Comparing Means","heading":"20.1.10 Independent Samples Approaches","text":"","code":""},{"path":"two-examples-comparing-means.html","id":"the-breakfast-study-does-oat-bran-cereal-lower-serum-ldl-cholesterol","chapter":"20 Two Examples Comparing Means","heading":"20.2 The Breakfast Study: Does Oat Bran Cereal Lower Serum LDL Cholesterol?","text":"Norman Streiner49 describe crossover study conducted investigate whether oat bran cereal helps lower serum cholesterol levels hypercholesterolemic males. Fourteen individuals randomly placed diet included either oat bran corn flakes; two weeks, low-density lipoprotein (LDL) cholesterol levels, mmol/l recorded. subject switched alternative diet. second two-week period, LDL cholesterol level subject recorded.","code":"\nbreakfast <- read_csv(\"data/breakfast.csv\",\n                    show_col_types = FALSE)\n\nbreakfast# A tibble: 14 x 3\n   subject cornflakes oatbran\n     <dbl>      <dbl>   <dbl>\n 1       1       4.61    3.84\n 2       2       6.42    5.57\n 3       3       5.4     5.85\n 4       4       4.54    4.8 \n 5       5       3.98    3.68\n 6       6       3.82    2.96\n 7       7       5.01    4.41\n 8       8       4.34    3.72\n 9       9       3.8     3.49\n10      10       4.56    3.84\n11      11       5.35    5.26\n12      12       3.89    3.73\n13      13       2.25    1.84\n14      14       4.24    4.14"},{"path":"two-examples-comparing-means.html","id":"question-1.-what-is-the-outcome-under-study-1","chapter":"20 Two Examples Comparing Means","heading":"20.2.1 Question 1. What is the outcome under study?","text":"studying levels LDL cholesterol, mmol/l. Note wanted convert familiar scale, specifically mg/dl, multiply mmol/l 18, turns .","code":""},{"path":"two-examples-comparing-means.html","id":"question-2.-what-are-the-treatmentexposure-groups-1","chapter":"20 Two Examples Comparing Means","heading":"20.2.2 Question 2. What are the treatment/exposure groups?","text":"comparing subjects two weeks eating corn flakes subjects two weeks eating oat bran.","code":""},{"path":"two-examples-comparing-means.html","id":"question-3.-are-the-data-collected-using-paired-or-independent-samples-1","chapter":"20 Two Examples Comparing Means","heading":"20.2.3 Question 3. Are the data collected using paired or independent samples?","text":"matched pairs, paired subject. subject produced oat bran result corn flakes result.","code":""},{"path":"two-examples-comparing-means.html","id":"question-4.-are-the-data-a-random-sample-from-the-population-of-interest-1","chapter":"20 Two Examples Comparing Means","heading":"20.2.4 Question 4. Are the data a random sample from the population of interest?","text":"Probably . data likely come convenient sample 14 individuals randomly assigned cornflakes first oat bran first, crossed .","code":""},{"path":"two-examples-comparing-means.html","id":"question-5.-what-significance-level-will-we-use-1","chapter":"20 Two Examples Comparing Means","heading":"20.2.5 Question 5. What significance level will we use?","text":"reason use usual 95% confidence level, alpha = 0.05","code":""},{"path":"two-examples-comparing-means.html","id":"question-6.-are-we-using-a-one-sided-or-two-sided-comparison-1","chapter":"20 Two Examples Comparing Means","heading":"20.2.6 Question 6. Are we using a one-sided or two-sided comparison?","text":"argue one-sided comparison, ’ll safe use two-sided version.","code":""},{"path":"two-examples-comparing-means.html","id":"question-7.-did-pairing-help-reduce-nuisance-variation","chapter":"20 Two Examples Comparing Means","heading":"20.2.7 Question 7. Did pairing help reduce nuisance variation?","text":"drop breakfast.csv file breakfast data frame, look correlation cornflakes oatbran results across 14 subjects.sample Pearson correlation coefficient strong positive 0.92, paired samples approach use data far effectively (incorrect) independent samples approach.","code":"\nbreakfast %$% cor(cornflakes, oatbran)[1] 0.9233247"},{"path":"two-examples-comparing-means.html","id":"question-8.-what-does-the-distribution-of-paired-differences-tell-us","chapter":"20 Two Examples Comparing Means","heading":"20.2.8 Question 8. What does the distribution of paired differences tell us?","text":"summarize distribution paired differences (cornflakes - oatbran) .Normal distribution doesn’t look ridiculous case paired (cornflakes-oatbran) differences. Suppose assume Normality run paired t test.Based sample 14 subjects crossover study, observe 95% confidence interval difference LDL cholesterol levels eating corn flakes eating oat bran entirely positive, suggesting LDL levels detectably higher (according t test procedure) eating corn flakes eating oat bran.","code":"\nt.test(breakfast$cornflakes - breakfast$oatbran)\n    One Sample t-test\n\ndata:  breakfast$cornflakes - breakfast$oatbran\nt = 3.3444, df = 13, p-value = 0.005278\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.1284606 0.5972537\nsample estimates:\nmean of x \n0.3628571 "},{"path":"two-examples-comparing-means.html","id":"power-sample-size-and-the-breakfast-study","chapter":"20 Two Examples Comparing Means","heading":"20.3 Power, Sample Size and the Breakfast Study","text":"preview ’s next come, let’s investigate promising results bit . Suppose new study, wish able detect difference LDL cholesterol two exposures: subjects eat cornflakes (original study) subjects continue eat cornflakes also take supplemental dosage believe crucial ingredient oatbran.Suppose believe effect taking new supplement half size effect observed original breakfast study hypercholesterolemic males, males generally may likely take supplement regularly switch cornflakes less appetizing breakfast choice, making supplement attractive.sample size required yield 90% power detect effect half size effect observed breakfast study, new paired samples study using two-tailed 5% significance level? required 80% power?","code":""},{"path":"two-examples-comparing-means.html","id":"the-setup","chapter":"20 Two Examples Comparing Means","heading":"20.3.1 The Setup","text":"want know n, minimum required sample size new study, :specified effect size half saw breakfast study, sample mean difference cornflakes oatbran 0.36 mmol/l, effect size assumed delta = 0.18 mmol/l.assumed standard deviation equal standard deviation differences pilot breakfast study, turns s = 0.41 mmol/l.also pre-specified alpha = 0.05 using two-tailed test.also want power least 90% new study.","code":""},{"path":"two-examples-comparing-means.html","id":"the-r-calculations","chapter":"20 Two Examples Comparing Means","heading":"20.3.2 The R Calculations","text":"Question 1. sample size required yield 90% power detect effect half size effect observed breakfast study, new paired samples study using two-tailed 5% significance level?new study require least 57 subjects (measured two circumstances, 114 total measurements) order achieve least 90% power detect difference 0.18 mmol/l meeting specifications.Question 2. willing accept 80% power?turns require least 43 subjects.","code":"\npower.t.test(delta = 0.18, sd = 0.41, sig.level = 0.05, \n             power = 0.9, type=\"paired\", alternative=\"two.sided\")\n     Paired t test power calculation \n\n              n = 56.47119\n          delta = 0.18\n             sd = 0.41\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs\npower.t.test(delta = 0.18, sd = 0.41, sig.level = 0.05, \n             power = 0.8, type=\"paired\", alternative=\"two.sided\")\n     Paired t test power calculation \n\n              n = 42.68269\n          delta = 0.18\n             sd = 0.41\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs"},{"path":"two-examples-comparing-means.html","id":"independent-samples-instead-of-paired-samples","chapter":"20 Two Examples Comparing Means","heading":"20.3.3 Independent samples, instead of paired samples?","text":"happen , instead paired samples study, one using independent samples? Assuming used balanced design, assigned number different people random either oatbran supplement regular cornflakes alone, study, require many people obtain similar power paired samples study., 220 people required independent samples study (110 exposure group), compared 57 people (measured twice) paired study.","code":"\npower.t.test(delta = 0.18, sd = 0.41, sig.level = 0.05, \n             power = 0.9, type=\"two.sample\", alternative=\"two.sided\")\n     Two-sample t test power calculation \n\n              n = 110\n          delta = 0.18\n             sd = 0.41\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group"},{"path":"estimating-a-population-proportion.html","id":"estimating-a-population-proportion","chapter":"21 Estimating a Population Proportion","heading":"21 Estimating a Population Proportion","text":"’ve focused creating statistical inferences population mean, difference means, care quantitative outcome. Now, ’ll tackle categorical outcomes, estimating confidence interval around population proportion.","code":""},{"path":"estimating-a-population-proportion.html","id":"loading-the-source-code-for-love-boost.r","chapter":"21 Estimating a Population Proportion","heading":"21.1 Loading the Source Code for Love-boost.R","text":"’ll use SAIFS confidence intervals ’ll build shortly.","code":"\nsource(\"data/Love-boost.R\")"},{"path":"estimating-a-population-proportion.html","id":"a-first-example-serum-zinc-in-the-normal-range","chapter":"21 Estimating a Population Proportion","heading":"21.2 A First Example: Serum Zinc in the “Normal” Range?","text":"Recall serum zinc study, 462 teenage male subjects, 395 (85.5%) fell “normal range” 66 110 micrograms per deciliter.Previously, estimated confidence interval mean population zinc levels. Now, want estimate confidence interval proportion population whose serum zinc levels range 66 110. want build point estimate population proportion, confidence interval population proportion.Now, let’s identify 95% confidence interval proportion population whose zinc levels within “normal” range. seen 395 / 462 subjects (proportion 0.855) fall “normal range” sample. now, also point estimate proportion “normal range” across entire population teenagers like sample.’ve created 0-1 variable, several available approaches wrapping confidence interval around proportion.","code":"\nserzinc <- read_csv(\"data/serzinc.csv\", \n                    show_col_types = FALSE)\n\nserzinc <- serzinc %>% \n  mutate(in_range = ifelse(zinc >= 66 & zinc <= 110, 1, 0))\n\nserzinc %>% tabyl(in_range) %>% \n  adorn_totals() %>% adorn_pct_formatting() in_range   n percent\n        0  67   14.5%\n        1 395   85.5%\n    Total 462  100.0%\nserzinc <- serzinc %>%\n    mutate(in_range = ifelse(zinc > 65 & zinc < 111, 1, 0))\n\nserzinc %>% tabyl(in_range) in_range   n   percent\n        0  67 0.1450216\n        1 395 0.8549784"},{"path":"estimating-a-population-proportion.html","id":"using-an-intercept-only-regression-again","chapter":"21 Estimating a Population Proportion","heading":"21.2.1 Using an Intercept-Only Regression Again?","text":"might consider taking approach population mean earlier:powerful approaches estimate confidence interval around proportion, simple approach turns bad, long sample proportion isn’t close either 0 1.","code":"\nmodel_zincprop <- lm(in_range ~ 1, data = serzinc)\n\ntidy(model_zincprop, conf.int = TRUE, conf = 0.90) %>% \n    knitr::kable(digits = 3)"},{"path":"estimating-a-population-proportion.html","id":"a-1001-alpha-confidence-interval-for-a-population-proportion","chapter":"21 Estimating a Population Proportion","heading":"21.2.2 A 100(1-\\(\\alpha\\))% Confidence Interval for a Population Proportion","text":"Suppose want estimate confidence interval unknown population proportion, \\(\\pi\\), basis random sample n observations population yields sample proportion p. Note p sample proportion – ’s p value.serum zinc example, n = 462 observations, sample proportion (“range”) p = 0.855.100(1-\\(\\alpha\\))% confidence interval population proportion \\(\\pi\\) can created using standard normal distribution, sample proportion, p, standard error sample proportion, defined square root p multiplied (1-p) divided sample size, n. standard error estimated serum zinc example :\\[\n\\sqrt{\\frac{p (1-p)}{n}} = \\sqrt{\\frac{0.855(1-0.855)}{462}} = \\sqrt{0.000268} = 0.016\n\\]thus, confidence interval \\(p \\pm Z_{\\alpha/2} \\sqrt{\\frac{p(1-p)}{n}}\\)\\(Z_{\\alpha/2}\\) = value standard Normal distribution cutting top \\(\\alpha/2\\) distribution, obtained R substituting desired \\(\\alpha/2\\) value following command: qnorm(alpha/2, lower.tail=FALSE).Note: interval reasonably accurate long np n(1-p) least 5.serum zinc data, np = (462)(0.855) = 395 n(1-p) = 462(1 - 0.855) = 67, ok.serum zinc data, np = (462)(0.855) = 395 n(1-p) = 462(1 - 0.855) = 67, ok.\\(\\alpha\\) = 0.05, \\(Z_{\\alpha/2}\\) = 1.96, approximately.\\(\\alpha\\) = 0.05, \\(Z_{\\alpha/2}\\) = 1.96, approximately.Thus, serum zinc estimate, confidence interval :\\[\np \\pm Z_{\\alpha/2} \\sqrt{\\frac{p(1-p)}{n}} = \n\\frac{395}{462} \\pm 1.96 \\sqrt{\\frac{0.855(1-0.855)}{462}} = 0.855 \\pm 0.032\n\\](0.823, 0.887).","code":"\nqnorm(0.025, lower.tail = FALSE)[1] 1.959964"},{"path":"estimating-a-population-proportion.html","id":"using-binom.test-from-the-mosaic-package","chapter":"21 Estimating a Population Proportion","heading":"21.3 Using binom.test from the mosaic package","text":"aware least seven different procedures estimating confidence interval population proportion using R. minor weaknesses: none importantly different others many practical situations. Five methods available using binom.test function mosaic package R.general format using binom.test function follows:appropriate ci.method obtained table .","code":"mosaic::binom.test(x = 395,  # substitute in number of successes\n                   n = 462,  # substitute in number of trials\n                   conf.level = 0.95, # default confidence level\n                   p = 0.5,  # default null hypothesis proportion\n                   ci.method = \"XXX\") # see below for XXX options"},{"path":"estimating-a-population-proportion.html","id":"the-wald-test-approach","chapter":"21 Estimating a Population Proportion","heading":"21.3.1 The Wald test approach","text":"Wald approach can used establish similar confidence interval one calculated , based something called Wald test., specify x n values. n total number observations, x number event interest (case, serum zinc levels normal range) occurs. x = 395 n = 462.Wald confidence interval always symmetric around point estimate, can dip 0 1.fit intervals using approaches mosaic::binom_test() usually tidy .elements tidied result shown .","code":"\nm_wald <- mosaic::binom.test(x = 395, n = 462,\n                     conf.level = 0.95,\n                     ci.method = \"Wald\")\nm_wald\n    Exact binomial test (Wald CI)\n\ndata:  395 out of 462\nnumber of successes = 395, number of trials = 462,\np-value < 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.8228698 0.8870869\nsample estimates:\nprobability of success \n             0.8549784 \ntidy(m_wald) %>% \n  select(estimate, conf.low, conf.high, statistic, parameter) %>%\n  kable(digits = 3)\ntidy(m_wald) %>%\n  select(method, alternative, p.value)# A tibble: 1 x 3\n  method                        alternative  p.value\n  <chr>                         <chr>          <dbl>\n1 Exact binomial test (Wald CI) two.sided   1.23e-57"},{"path":"estimating-a-population-proportion.html","id":"the-clopper-pearson-approach","chapter":"21 Estimating a Population Proportion","heading":"21.3.2 The Clopper-Pearson approach","text":"binom.test command can used establish “exact” confidence interval. uses method Clopper Pearson 1934, exact sense guarantees, instance, confidence level associated interval least large nominal level 95%, interval isn’t wider perhaps needs .Clopper-Pearson used stats::binom.test() R well. , guarantees coverage least large nominal coverage rate, may produce wider intervals methods ’ll see. 95% confidence interval method (0.820, 0.886), general range previous estimates.","code":"\nm_clopper <- mosaic::binom.test(x = 395, n = 462,\n                                conf.level = 0.95,\n                                ci.method = \"Clopper-Pearson\")\n\nm_clopper\n\n\ndata:  395 out of 462\nnumber of successes = 395, number of trials = 462,\np-value < 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.8195187 0.8858100\nsample estimates:\nprobability of success \n             0.8549784 \ntidy(m_clopper) %>% \n  select(estimate, conf.low, conf.high, statistic, parameter) %>%\n  kable(digits = 3)"},{"path":"estimating-a-population-proportion.html","id":"the-score-approach","chapter":"21 Estimating a Population Proportion","heading":"21.3.3 The Score approach","text":"Score approach also used stats::prop.test() creates CIs inverting p-values score tests. can applied continuity correction (use ci.method = “prop.test”) without.case, see Score approach Clopper-Pearson approach give similar results.mentioned, score test can also run incorporating something called continuity correction, since using Normal approximation exact binomial distribution establish margin error. R, default, includes continuity correction Score test use prop.test collect .","code":"\nm_score <- mosaic::binom.test(x = 395, n = 462,\n                              conf.level = 0.95,\n                              ci.method = \"Score\")\n\nm_score\n    Exact binomial test (Score CI without continuity\n    correction)\n\ndata:  395 out of 462\nnumber of successes = 395, number of trials = 462,\np-value < 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.8199415 0.8841607\nsample estimates:\nprobability of success \n             0.8549784 \ntidy(m_score) %>% \n  select(estimate, conf.low, conf.high, statistic, parameter) %>%\n  kable(digits = 3)\nm_score_cor <- mosaic::binom.test(x = 395, n = 462,\n                                  conf.level = 0.95,\n                                  ci.method = \"prop.test\")\n\nm_score_cor\n    Exact binomial test (Score CI with continuity\n    correction)\n\ndata:  395 out of 462\nnumber of successes = 395, number of trials = 462,\np-value < 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.8187706 0.8851359\nsample estimates:\nprobability of success \n             0.8549784 \ntidy(m_score_cor) %>% \n  select(estimate, conf.low, conf.high, statistic, parameter) %>%\n  kable(digits = 3)"},{"path":"estimating-a-population-proportion.html","id":"the-agresti-coull-approach","chapter":"21 Estimating a Population Proportion","heading":"21.3.4 The Agresti-Coull Approach","text":"Agresti-Coull approach Wald method adding Z successes Z failures data, Z appropriate quantile standard Normal distribution (1.96 95% CI).","code":"\nm_agresti <- mosaic::binom.test(x = 395, n = 462,\n                                conf.level = 0.95,\n                                ci.method = \"agresti-coull\")\n\nm_agresti\n    Exact binomial test (Agresti-Coull CI)\n\ndata:  395 out of 462\nnumber of successes = 395, number of trials = 462,\np-value < 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.8198094 0.8842928\nsample estimates:\nprobability of success \n             0.8549784 \ntidy(m_agresti) %>% \n  select(estimate, conf.low, conf.high, statistic, parameter) %>%\n  kable(digits = 3)"},{"path":"estimating-a-population-proportion.html","id":"the-plus-4-approach","chapter":"21 Estimating a Population Proportion","heading":"21.3.5 The “Plus 4” approach","text":"approach just Wald method adding 2 successes 2 failures (4 observations) data. similar Agresti-Coull method working 95% confidence interval.","code":"\nm_plus4 <- mosaic::binom.test(x = 395, n = 462,\n                              conf.level = 0.95,\n                              ci.method = \"plus4\")\n\nm_plus4\n    Exact binomial test (Plus 4 CI)\n\ndata:  395 out of 462\nnumber of successes = 395, number of trials = 462,\np-value < 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.8196844 0.8841783\nsample estimates:\nprobability of success \n             0.8549784 \ntidy(m_plus4) %>% \n  select(estimate, conf.low, conf.high, statistic, parameter) %>%\n  kable(digits = 3)"},{"path":"estimating-a-population-proportion.html","id":"saifs-single-augmentation-with-an-imaginary-failure-or-success","chapter":"21 Estimating a Population Proportion","heading":"21.3.6 SAIFS: single augmentation with an imaginary failure or success","text":"SAIFS stands “single augmentation imaginary failure success” method ’ll describe one several similar approaches. next subsection describes R code calculating relevant confidence interval.approach like estimation confidence interval single population proportion/rate estimate lower bound confidence interval imaginary failure added observed data, estimate upper bound confidence interval imaginary success added data.Suppose X successes n trials, want establish confidence interval population proportion successes.Let \\(p_1 = (X+0)/(n+1), p_2 = (X+1)/(n+1), q_1 = 1 - p_1, q_2 = 1 - p_2\\)lower bound 100(1-\\(\\alpha\\))% confidence interval population proportion successes using SAIFS procedure \\(LB_{SAIFS}(x,n,\\alpha) = p_1 - t_{\\alpha/2, n-1}\\sqrt{\\frac{p_1 q_1}{n}}\\)lower bound 100(1-\\(\\alpha\\))% confidence interval population proportion successes using SAIFS procedure \\(LB_{SAIFS}(x,n,\\alpha) = p_1 - t_{\\alpha/2, n-1}\\sqrt{\\frac{p_1 q_1}{n}}\\)upper bound 100(1-\\(\\alpha\\))% confidence interval population proportion successes using SAIFS procedure \\(UB_{SAIFS}(x,n,\\alpha) = p_2 + t_{\\alpha/2, n-1}\\sqrt{\\frac{p_2 q_2}{n}}\\)upper bound 100(1-\\(\\alpha\\))% confidence interval population proportion successes using SAIFS procedure \\(UB_{SAIFS}(x,n,\\alpha) = p_2 + t_{\\alpha/2, n-1}\\sqrt{\\frac{p_2 q_2}{n}}\\)Returning serum zinc example, ’ve got 395 “successes” (value normal range) 462 “trials” (values measured), X = 395 n = 462So \\(p_1 = \\frac{X + 0}{n + 1} = \\frac{395}{463} = 0.8531\\), \\(p_2 = \\frac{X + 1}{n + 1} = \\frac{396}{463} = 0.8553\\), \\(q_1 = 1 - p_1 = 0.1469\\) \\(q_2 = 1 - p_2 = 0.1447\\)\\(n = 462\\) want 95% confidence interval (\\(\\alpha = 0.05\\)), \\(t_{\\alpha/2, n-1} = t_{0.025, 461} = 1.9651\\), determined using R’s qt function:Thus, lower bound 95% confidence interval \\(p_1 - t_{\\alpha/2, n-1}\\sqrt{\\frac{p_1 q_1}{n}}\\), \\(0.8531 - 1.9651 \\sqrt{\\frac{0.8531(0.1469)}{462}}\\), 0.8531 - 0.0324 0.8207.upper bound \\(p_2 + t_{\\alpha/2, n-1}\\sqrt{\\frac{p_2 q_2}{n}}\\), \\(0.8553 - 1.9651 \\sqrt{\\frac{0.8553(0.1447)}{462}}\\), 0.8553 + 0.0323, 0.8876.95% SAIFS confidence interval estimate population proportion, \\(\\pi\\), teenage males whose serum zinc levels fall within “normal range” (0.821, 0.888).","code":"\nqt(0.025, df = 461, lower.tail=FALSE)[1] 1.965123"},{"path":"estimating-a-population-proportion.html","id":"a-function-in-r-to-calculate-the-saifs-confidence-interval","chapter":"21 Estimating a Population Proportion","heading":"21.3.7 A Function in R to Calculate the SAIFS Confidence Interval","text":"built R function, called saifs.ci contained Markdown document well Love-boost.R script web site, takes arguments value X = number successes, n = number trials, conf.level = confidence level, produces sample proportion, SAIFS lower bound upper bound specified two-sided confidence interval population proportion, using equations ., instance, 95%, 90% 99% confidence intervals population proportion \\(\\pi\\) studying serum zinc data.Note final interval, asked machine round five digits rather default three. desktop (probably ), results output:’ve got setting wrong bookdown work doesn’t show function called. Sorry!","code":"\nsaifs.ci(x = 395, n = 462)Sample Proportion             0.025             0.975 \n            0.855             0.821             0.887 \nsaifs.ci(x = 395, n = 462, conf=0.9)Sample Proportion              0.05              0.95 \n            0.855             0.826             0.882 \nsaifs.ci(x = 395, n = 462, conf=0.99, dig=5)Sample Proportion             0.005             0.995 \n          0.85498           0.81054           0.89763 Sample Proportion             0.005             0.995 \n          0.85498           0.81054           0.89763 "},{"path":"estimating-a-population-proportion.html","id":"the-saifs.ci-function-in-r","chapter":"21 Estimating a Population Proportion","heading":"21.3.8 The saifs.ci function in R","text":"","code":"\n`saifs.ci` <- \n  function(x, n, conf.level=0.95, dig=3)\n  {\n    p.sample <- round(x/n, digits=dig)\n    \n    p1 <- x / (n+1)\n    p2 <- (x+1) / (n+1)\n    \n    var1 <- (p1*(1-p1))/n\n    se1 <- sqrt(var1)\n    var2 <- (p2*(1-p2))/n\n    se2 <- sqrt(var2)\n    \n    lowq = (1 - conf.level)/2\n    tcut <- qt(lowq, df=n-1, lower.tail=FALSE)\n    \n    lower.bound <- round(p1 - tcut*se1, digits=dig)\n    upper.bound <- round(p2 + tcut*se2, digits=dig)\n    res <- c(p.sample, lower.bound, upper.bound)\n    names(res) <- c('Sample Proportion',lowq, 1-lowq)\n    res\n  }"},{"path":"estimating-a-population-proportion.html","id":"a-second-example-ebola-mortality-rates-through-9-months-of-the-epidemic","chapter":"21 Estimating a Population Proportion","heading":"21.4 A Second Example: Ebola Mortality Rates through 9 Months of the Epidemic","text":"World Health Organization’s Ebola Response Team published article October 16, 2014 issue New England Journal Medicine, contained data use example, focusing materials Table 2.September 14, 2014, total 4,507 confirmed probable cases Ebola virus disease (EVD) reported West Africa. example, look set 1,737 cases, definitive outcomes, reported Guinea, Liberia, Nigeria Sierra Leone.Across 1,737 cases, total 1,229 cases led death. Based sample data, can said case fatality rate population EVD cases definitive outcomes epidemic?","code":""},{"path":"estimating-a-population-proportion.html","id":"working-through-the-ebola-virus-disease-example","chapter":"21 Estimating a Population Proportion","heading":"21.4.1 Working through the Ebola Virus Disease Example","text":"n = 1,737 subjects, observed death 1,229, sample proportion \\(p = \\frac{1229}{1737} = 0.708\\). standard error sample proportion \\(SE(p) = \\sqrt{\\frac{p(1-p)}{n}} = \\sqrt{\\frac{0.708(1-0.708)}{1737}} = 0.011\\)95% confidence interval (’ll use \\(\\alpha\\) = 0.05) true population proportion, \\(\\pi\\), EVD cases definitive outcomes, die \n\\(p \\pm Z_{.025} \\sqrt{\\frac{p(1-p)}{n}}\\), 0.708 \\(\\pm\\) 1.96(0.011) = \\(0.708 \\pm 0.022\\), (0.686, 0.730)Note simply recalled prior work \\(Z_{0.025} = 1.96\\), can verify :Since np=(1737)(0.708)=1230 n(1-p)=(1737)(1-0.708)=507 substantially greater 5, reasonably accurate confidence interval.95% confidence interval estimate true population proportion falls 0.686 0.730. Equivalently, say ’re 95% confident true case fatality rate expressed percentage rather proportion, 68.6% 73.0%.","code":"\nqnorm(0.025, lower.tail=FALSE)[1] 1.959964"},{"path":"estimating-a-population-proportion.html","id":"using-r-to-estimate-the-ci-for-our-ebola-example","chapter":"21 Estimating a Population Proportion","heading":"21.4.2 Using R to estimate the CI for our Ebola example","text":"way precision can really justify, just want see five results (slightly) different.","code":"\nebola_wald <- mosaic::binom.test(x = 1229, n = 1737, conf.level = 0.95,\n                                 ci.method = \"Wald\") %>%\n  tidy() %>% select(estimate, conf.low, conf.high)\nebola_clop <- mosaic::binom.test(x = 1229, n = 1737, conf.level = 0.95,\n                                 ci.method = \"Clopper-Pearson\") %>%\n  tidy() %>% select(estimate, conf.low, conf.high)\nebola_scor <- mosaic::binom.test(x = 1229, n = 1737, conf.level = 0.95,\n                                 ci.method = \"Score\") %>%\n  tidy() %>% select(estimate, conf.low, conf.high)\nebola_agco <- mosaic::binom.test(x = 1229, n = 1737, conf.level = 0.95,\n                                 ci.method = \"agresti-coull\") %>%\n  tidy() %>% select(estimate, conf.low, conf.high)\nebola_plus <- mosaic::binom.test(x = 1229, n = 1737, conf.level = 0.95,\n                                 ci.method = \"plus4\") %>%\n  tidy() %>% select(estimate, conf.low, conf.high)\n\nebola_res <- bind_rows(ebola_wald, ebola_clop, ebola_scor, \n                       ebola_agco, ebola_plus) %>%\n  mutate(approach = c(\"Wald\", \"Clopper-Pearson\", \"Score\",\n                      \"Agresti-Coull\", \"Plus4\"))\n\nebola_res %>% kable(digits = 6)"},{"path":"estimating-a-population-proportion.html","id":"plotting-the-confidence-intervals-for-the-ebola-virus-disease-example","chapter":"21 Estimating a Population Proportion","heading":"21.4.3 Plotting the Confidence Intervals for the Ebola Virus Disease Example","text":"case, really doesn’t matter one choose. smaller sample, may come conclusion relative merits different approaches.","code":"\nggplot(ebola_res, aes(x = approach, y = estimate)) +\n    geom_point() +\n    geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +\n    coord_flip() +\n    labs(title = \"95% CIs for x = 1229, n = 1737\")"},{"path":"estimating-a-population-proportion.html","id":"what-about-the-saifs.ci-result","chapter":"21 Estimating a Population Proportion","heading":"21.4.4 What about the saifs.ci() result?","text":"","code":"\nsaifs.ci(x = 1229, n = 1737, conf.level=0.95)Sample Proportion             0.025             0.975 \n            0.708             0.686             0.729 "},{"path":"estimating-a-population-proportion.html","id":"can-the-choice-of-confidence-interval-method-matter","chapter":"21 Estimating a Population Proportion","heading":"21.5 Can the Choice of Confidence Interval Method Matter?","text":"Yes. especially case small sample size, probability “success” close either 0 1. instance, suppose run 10 trials, obtain single success, use data estimate true proportion success, \\(\\pi\\).90% confidence intervals circumstance different.Note Wald procedure doesn’t force confidence interval appear (0, 1) range.None three approaches always better others. sample size 100, sample proportion success either 0.10 0.90, caution warranted, although many cases, various methods give similar responses.","code":"\ntidy1 <- mosaic::binom.test(x = 1, n = 10, conf.level = 0.90,\n                            ci.method = \"Wald\") %>% tidy()\ntidy2 <- mosaic::binom.test(x = 1, n = 10, conf.level = 0.90, \n                            ci.method = \"Clopper-Pearson\") %>% tidy()\ntidy3 <- mosaic::binom.test(x = 1, n = 10, conf.level = 0.90, \n                            ci.method = \"Score\") %>% tidy()\ntidy4 <- mosaic::binom.test(x = 1, n = 10, conf.level = 0.90, \n                            ci.method = \"agresti-coull\") %>% tidy()\ntidy5 <- mosaic::binom.test(x = 1, n = 10, conf.level = 0.90, \n                            ci.method = \"plus4\") %>% tidy()\n\nres <- bind_rows(tidy1, tidy2, tidy3, tidy4, tidy5) %>%\n  mutate(approach = c(\"Wald\", \"Clopper-Pearson\", \"Score\",\n                      \"Agresti-Coull\", \"Plus4\")) %>%\n  select(approach, estimate, conf.low, conf.high)\n\nres %>% kable(digits = 3)"},{"path":"comparing-proportions-with-two-independent-samples.html","id":"comparing-proportions-with-two-independent-samples","chapter":"22 Comparing Proportions with Two Independent Samples","heading":"22 Comparing Proportions with Two Independent Samples","text":"Often, analyzing data independent samples design, interested comparing proportions subjects achieve outcome, across two levels exposure, treatment. circumstance, first summarize available data terms 2x2 cross-tabulation, apply series inferential methods 2x2 tables obtain point interval estimates interest.","code":""},{"path":"comparing-proportions-with-two-independent-samples.html","id":"loading-the-source-code-for-love-boost.r-1","chapter":"22 Comparing Proportions with Two Independent Samples","heading":"22.1 Loading the Source Code for Love-boost.R","text":"’ll use twobytwo function ’ll demonstrate shortly.","code":"\nsource(\"data/Love-boost.R\")"},{"path":"comparing-proportions-with-two-independent-samples.html","id":"a-first-example-ibuprofen-and-sepsis-trial","chapter":"22 Comparing Proportions with Two Independent Samples","heading":"22.2 A First Example: Ibuprofen and Sepsis Trial","text":"saw previous Chapter, interested comparing percentage patients arm trial (Ibuprofen vs. Placebo) showed improvement temperature (temp_drop > 0). primary interest comparing percentage Ibuprofen patients whose temperature dropped percentage Placebo patients whose temperature dropped.can summarize data behind two proportions comparing contingency table two rows identify exposure treatment interest, two columns represent outcomes interest.case, comparing two groups subjects based treatment (treat): received Ibuprofen received placebo. outcome interest whether subject’s temperature dropped (temp_drop > 0), .table, place frequency combination row column.rows need mutually exclusive collectively exhaustive: patient must either receive Ibuprofen Placebo. Similarly, columns must meet standard: every patient’s temperature either drops drop.’s contingency table.sample, observe 71.3% 150 Ibuprofen subjects positive temp_drop compared 53.3% 150 Placebo subjects. want compare two probabilities (represented using proportions 0.713 vs. 0.533) estimate size difference proportions point estimate 90% confidence interval.comparisons make, . tricky part multiple ways describe relationship treatment outcome. might compare outcome “risks” directly using difference probabilities, ratio two probabilities, might convert risks odds, compare ratio odds. case, ’ll get different point estimates confidence intervals, help us make conclusions evidence available trial speaking differences Ibuprofen Placebo.","code":"\nsepsis <- read_csv(\"data/sepsis.csv\",\n                   show_col_types = FALSE) %>%\n    mutate(treat = factor(treat),\n           race = factor(race))\n\n\nsepsis <- sepsis %>%\n    mutate(dropped = ifelse(temp_drop > 0, \"Drop\", \"No Drop\"))\n\nsepsis %>% tabyl(treat, dropped) %>% \n    adorn_totals() %>%\n    adorn_percentages(denom = \"row\") %>%\n    adorn_pct_formatting(digits = 1) %>%\n    adorn_ns(position = \"front\")     treat        Drop     No Drop\n Ibuprofen 107 (71.3%)  43 (28.7%)\n   Placebo  80 (53.3%)  70 (46.7%)\n     Total 187 (62.3%) 113 (37.7%)"},{"path":"comparing-proportions-with-two-independent-samples.html","id":"relating-a-treatment-to-an-outcome","chapter":"22 Comparing Proportions with Two Independent Samples","heading":"22.3 Relating a Treatment to an Outcome","text":"question interest whether percentage subjects whose temperature dropped different (probably larger) subjects received Ibuprofen received Placebo.words, relationship treatment outcome?","code":""},{"path":"comparing-proportions-with-two-independent-samples.html","id":"definitions-of-probability-and-odds","chapter":"22 Comparing Proportions with Two Independent Samples","heading":"22.4 Definitions of Probability and Odds","text":"Proportion = Probability = Risk trait = number trait / totalOdds trait = (number trait / number without trait) 1If p proportion subjects trait, odds trait \\(\\frac{p}{1-p}\\) 1., probability good result (temperature drop) case \\(\\frac{107}{150} = 0.713\\) Ibuprofen group. odds good result thus \\(\\frac{0.713}{1 - 0.713} = 2.484\\) 1 Ibuprofen group.","code":""},{"path":"comparing-proportions-with-two-independent-samples.html","id":"defining-the-relative-risk","chapter":"22 Comparing Proportions with Two Independent Samples","heading":"22.5 Defining the Relative Risk","text":"Among Ibuprofen subjects, risk good outcome (drop temperature) 71.3% , stated proportion, 0.713. Among Placebo subjects, risk good outcome 53.3% , stated proportion, 0.533.“crude” estimate relative risk good outcome Ibuprofen subjects compared Placebo subjects, ratio two risks, 0.713/0.533 = 1.338The fact relative risk greater 1 indicates probability good outcome higher Ibuprofen subjects Placebo subjects.relative risk 1 indicate probability good outcome Ibuprofen subjects Placebo subjects.relative risk less 1 indicate probability good outcome lower Ibuprofen subjects Placebo subjects.","code":""},{"path":"comparing-proportions-with-two-independent-samples.html","id":"defining-the-risk-difference","chapter":"22 Comparing Proportions with Two Independent Samples","heading":"22.6 Defining the Risk Difference","text":"“crude” estimate risk difference good outcome Ibuprofen subjects compared Placebo subjects, 0.713 - 0.533 = 0.180 18.0 percentage points.fact risk difference greater 0 indicates probability good outcome higher Ibuprofen subjects Placebo subjects.risk difference 0 indicate probability good outcome Ibuprofen subjects Placebo subjects.risk difference less 0 indicate probability good outcome lower Ibuprofen subjects Placebo subjects.","code":""},{"path":"comparing-proportions-with-two-independent-samples.html","id":"defining-the-odds-ratio-or-the-cross-product-ratio","chapter":"22 Comparing Proportions with Two Independent Samples","heading":"22.7 Defining the Odds Ratio, or the Cross-Product Ratio","text":"Among Ibuprofen subjects, odds good outcome (temperature drop) 2.484. Among placebo subjects, odds good outcome (temperature drop) 1.141.“crude” estimate odds ratio good outcome Ibuprofen subjects compared placebo subjects, 2.484 / 1.141 = 2.18Another way calculate odds ratio calculate cross-product ratio, equal (x d) / (b x c), 2 2 table counts specified shown:, table, = 107, b = 43, c = 80, d = 70, cross-product ratio \\(\\frac{107 x 70}{43 x 80} = \\frac{7490}{3440} = 2.18\\). expected, “crude” odds ratio estimate.fact odds ratio risk greater 1 indicates odds good outcome higher Ibuprofen subjects Placebo subjects.odds ratio 1 indicate odds good outcome Ibuprofen subjects Placebo subjects.odds ratio less 1 indicate odds good outcome lower Ibuprofen subjects Placebo subjects., several different ways compare outcomes across treatments. differences ratios large enough rule chance?","code":""},{"path":"comparing-proportions-with-two-independent-samples.html","id":"comparing-rates-in-a-2x2-table","chapter":"22 Comparing Proportions with Two Independent Samples","heading":"22.8 Comparing Rates in a 2x2 Table","text":"relationship treatment (Ibuprofen vs. Placebo) outcome (drop temperature) following two--two table?","code":""},{"path":"comparing-proportions-with-two-independent-samples.html","id":"the-twobytwo-function-in-r","chapter":"22 Comparing Proportions with Two Independent Samples","heading":"22.9 The twobytwo function in R","text":"built twobytwo function R (based existing functions Epi library, need installed packages list order work) work problem. required single command, two--two table standard epidemiological format (outcomes columns, treatments rows.)command just requires read cells table, followed labels two treatments, two outcomes, specification names rows (exposures) columns (outcomes) table, specification confidence level desire. ’ll use 90% .resulting output follows.","code":"\ntwobytwo(107, 43, 80, 70, \n         \"Ibuprofen\", \"Placebo\", \"Dropped\", \"No Drop\",\n         conf.level = 0.90)2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Dropped \nComparing : Ibuprofen vs. Placebo \n\n          Dropped No Drop    P(Dropped) 90% conf. interval\nIbuprofen     107      43        0.7133    0.6490   0.7701\nPlacebo        80      70        0.5333    0.4661   0.5993\n\n                                   90% conf. interval\n             Relative Risk: 1.3375    1.1492   1.5567\n         Sample Odds Ratio: 2.1773    1.4583   3.2509\nConditional MLE Odds Ratio: 2.1716    1.4177   3.3437\n    Probability difference: 0.1800    0.0881   0.2677\n\n             Exact P-value: 0.0019 \n        Asymptotic P-value: 0.0014 \n------------------------------------------------------"},{"path":"comparing-proportions-with-two-independent-samples.html","id":"standard-epidemiological-format","chapter":"22 Comparing Proportions with Two Independent Samples","heading":"22.9.1 Standard Epidemiological Format","text":"table standard epidemiological format, means :rows table describe “treatment” (’ll take treat). interesting (sometimes also common) “treatment” placed top row. ’s Ibuprofen .columns table describe “outcome” (’ll take whether subject’s temperature dropped.) Typically, common “outcome” placed left.","code":""},{"path":"comparing-proportions-with-two-independent-samples.html","id":"outcome-probabilities-and-confidence-intervals-within-the-treatment-groups","chapter":"22 Comparing Proportions with Two Independent Samples","heading":"22.9.2 Outcome Probabilities and Confidence Intervals Within the Treatment Groups","text":"twobytwo output starts estimates probability (risk) “Dropped” outcome among subjects fall two treatment groups (Ibuprofen Placebo), along 90% confidence intervals probabilities.conditional probability temperature drop given subject Ibuprofen group, symbolized Pr(Dropped | Ibuprofen) = 0.7133, 90% confidence interval around proportion (0.6490, 0.7701).Note two confidence intervals fail overlap, expect see fairly large difference estimated probability temperature drop compare Ibuprofen Placebo.","code":"2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Dropped \nComparing : Ibuprofen vs. Placebo \n\n          Dropped No Drop    P(Dropped) 90% conf. interval\nIbuprofen     107      43        0.7133    0.6490   0.7701\nPlacebo        80      70        0.5333    0.4661   0.5993"},{"path":"comparing-proportions-with-two-independent-samples.html","id":"relative-risk-odds-ratio-and-risk-difference-with-confidence-intervals","chapter":"22 Comparing Proportions with Two Independent Samples","heading":"22.9.3 Relative Risk, Odds Ratio and Risk Difference, with Confidence Intervals","text":"elements followed estimates relative risk, odds ratio, risk difference, associated 90% confidence intervals.relative risk, ratio P(Temperature Drop | Ibuprofen) P(Temperature Drop | Placebo), shown first. Note 90% confidence interval entirely greater 1.odds ratio presented using two different definitions (sample odds ratio cross-product ratio mentioned earlier). Note 90% confidence interval using either approach entirely greater 1.probability (risk) difference [P(Temperature Drop | Ibuprofen) - P(Temperature Drop | Placebo)] presented last. Note 90% confidence interval entirely greater 0.Note carefully difference Ibuprofen Placebo, relative risk odds ratios 1, probability difference zero.","code":"                                   90% conf. interval\n             Relative Risk: 1.3375    1.1492   1.5567\n         Sample Odds Ratio: 2.1773    1.4583   3.2509\nConditional MLE Odds Ratio: 2.1716    1.4177   3.3437\n    Probability difference: 0.1800    0.0881   0.2677"},{"path":"comparing-proportions-with-two-independent-samples.html","id":"estimating-a-rate-more-accurately-use-x-2n-4-rather-than-xn","chapter":"22 Comparing Proportions with Two Independent Samples","heading":"22.10 Estimating a Rate More Accurately: Use (x + 2)/(n + 4) rather than x/n","text":"Suppose data involving n independent tries, x successes. natural estimate “success rate” data x / n. , strangely enough, turns isn’t entirely satisfying estimator. Alan Agresti provides substantial motivation (x + 2)/(n + 4) estimate alternative. sometimes called Bayesian augmentation.big problem x / n estimates p = 0 p = 1 x = 0 x = n. ’s also tricky compute confidence intervals extremes, since usual standard error proportion, \\(\\sqrt{n p (1-p)}\\), gives zero, isn’t quite right.(x + 2)/(n + 4) much cleaner, especially build confidence interval rate.place (x + 2)/(n + 4) go wrong n small true probability close 0 1.example, n = 10, p 1 million, x almost certainly zero, estimate 1/12 much worse simple 0/10. However, big deal ? p might 1 million, ’re going estimate n = 10 experiment.Applying method Ibuprofen Sepsis Trial data, simply add two frequency main four cells 2x2 table.instead usingthe Bayesian augmentation encourage us look atAs can see, odds ratio relative risk estimates (little) closer 1, probability difference also little closer 0. Bayesian augmentation provides slightly conservative set estimates impact Ibuprofen compared Placebo.likely augmented version accurate estimate , two estimates comparable, generally, long either () sample size exposure group , say, 30 subjects, /(b) sample probability outcome 10% 90% exposure group.","code":"\ntwobytwo(107, 43, 80, 70, \n         \"Ibuprofen\", \"Placebo\", \"Dropped\", \"No Drop\",\n         conf.level = 0.90)\ntwobytwo(109, 45, 82, 72, \n         \"Ibuprofen\", \"Placebo\", \"Dropped\", \"No Drop\",\n         conf.level = 0.90)2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Dropped \nComparing : Ibuprofen vs. Placebo \n\n          Dropped No Drop    P(Dropped) 90% conf. interval\nIbuprofen     109      45        0.7078    0.6441   0.7643\nPlacebo        82      72        0.5325    0.4662   0.5977\n\n                                   90% conf. interval\n             Relative Risk: 1.3293    1.1434   1.5453\n         Sample Odds Ratio: 2.1268    1.4337   3.1550\nConditional MLE Odds Ratio: 2.1215    1.3950   3.2421\n    Probability difference: 0.1753    0.0845   0.2622\n\n             Exact P-value: 0.0022 \n        Asymptotic P-value: 0.0016 \n------------------------------------------------------"},{"path":"comparing-proportions-with-two-independent-samples.html","id":"a-second-example-ebola-virus-disease-study-again","chapter":"22 Comparing Proportions with Two Independent Samples","heading":"22.11 A Second Example: Ebola Virus Disease Study, again","text":"instance, recall Ebola Virus Disease study New England Journal Medicine described previous Chapter. Suppose want compare proportion deaths among cases definitive outcome hospitalized proportion deaths among cases definitive outcome hospitalized.article suggests 1,737 cases definitive outcome, 1,153 hospitalized cases. Across 1,153 hospitalized cases, 741 people (64.3%) died, means across remaining 584 non-hospitalized cases, 488 people (83.6%) died.initial contingency table, using numbers previous paragraph.Now, can use arithmetic complete table, since rows columns mutually exclusive collectively exhaustive.want compare fatality risk (probability deceased column) population people hospitalized row population people hospitalized row.can run data R, using Bayesian augmentation (adding death survival hospitalized also hospitalized groups.) ’ll use 95% confidence level time.’ll leave exercise interpret results draw conclusions.","code":"\ntwobytwo(741+2, 412+2, 488+2, 96+2, \n         \"Hospitalized\", \"Not Hospitalized\", \"Deceased\", \"Alive\",\n         conf.level = 0.95)2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Deceased \nComparing : Hospitalized vs. Not Hospitalized \n\n                 Deceased Alive    P(Deceased) 95% conf.\nHospitalized          743   414         0.6422    0.6141\nNot Hospitalized      490    98         0.8333    0.8010\n                 interval\nHospitalized       0.6693\nNot Hospitalized   0.8613\n\n                                    95% conf. interval\n             Relative Risk:  0.7706    0.7285   0.8151\n         Sample Odds Ratio:  0.3589    0.2801   0.4599\nConditional MLE Odds Ratio:  0.3591    0.2772   0.4624\n    Probability difference: -0.1912   -0.2307  -0.1490\n\n             Exact P-value: 0.0000 \n        Asymptotic P-value: 0.0000 \n------------------------------------------------------"},{"path":"power-and-proportions.html","id":"power-and-proportions","chapter":"23 Power and Proportions","heading":"23 Power and Proportions","text":"","code":""},{"path":"power-and-proportions.html","id":"loading-the-source-code-for-love-boost.r-2","chapter":"23 Power and Proportions","heading":"23.1 Loading the Source Code for Love-boost.R","text":", ’ll use twobytwo function ’ll demonstrate shortly.","code":"\nsource(\"data/Love-boost.R\")"},{"path":"power-and-proportions.html","id":"tuberculosis-prevalence-among-iv-drug-users","chapter":"23 Power and Proportions","heading":"23.2 Tuberculosis Prevalence Among IV Drug Users","text":"Consider study investigate factors affecting tuberculosis prevalence among intravenous drug users. original data source Graham NMH et al. (1992) Prevalence Tuberculin Positivity Skin Test Anergy HIV-1-Seropositive Seronegative Intravenous Drug Users. JAMA, 267, 369-373. Among 97 individuals admit sharing needles, 24 (24.7%) positive tuberculin skin test result; among 161 drug users deny sharing needles, 28 (17.4%) positive test result. start, ’ll test null hypothesis proportions intravenous drug users positive tuberculin skin test result identical share needles .conclusions draw?","code":"\ntwobytwo(24, 73, 28, 133, \n         \"Sharing Needles\", \"Not Sharing\", \n         \"TB test+\", \"TB test-\")2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : TB test+ \nComparing : Sharing Needles vs. Not Sharing \n\n                TB test+ TB test-    P(TB test+) 95% conf.\nSharing Needles       24       73         0.2474    0.1717\nNot Sharing           28      133         0.1739    0.1229\n                interval\nSharing Needles   0.3427\nNot Sharing       0.2404\n\n                                   95% conf. interval\n             Relative Risk: 1.4227    0.8772   2.3073\n         Sample Odds Ratio: 1.5616    0.8439   2.8898\nConditional MLE Odds Ratio: 1.5588    0.8014   3.0191\n    Probability difference: 0.0735   -0.0265   0.1807\n\n             Exact P-value: 0.1996 \n        Asymptotic P-value: 0.1557 \n------------------------------------------------------"},{"path":"power-and-proportions.html","id":"designing-a-new-tb-study","chapter":"23 Power and Proportions","heading":"23.3 Designing a New TB Study","text":"Now, suppose wanted design new study many non-sharers needle-sharers participating, suppose wanted detect difference proportion positive skin test results two groups identical data presented larger least 90% power, using two-sided test \\(\\alpha\\) = .05. sample size required accomplish aims?","code":""},{"path":"power-and-proportions.html","id":"using-power.prop.test-for-balanced-designs","chapter":"23 Power and Proportions","heading":"23.4 Using power.prop.test for Balanced Designs","text":"constraints want find sample size two-sample comparison proportions using balanced design, use \\(\\alpha\\) = .05, power = .90, estimate non-sharers .174 proportion positive tests, try detect difference group needle sharers, estimate proportion .247, using two-sided hypothesis test., ’d need least 654 non-sharing subjects, 654 share needles accomplish aims study.","code":"\npower.prop.test(p1 = .174, p2  = .247, sig.level = 0.05, power = 0.90)\n     Two-sample comparison of proportions power calculation \n\n              n = 653.2876\n             p1 = 0.174\n             p2 = 0.247\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group"},{"path":"power-and-proportions.html","id":"how-power.prop.test-works","chapter":"23 Power and Proportions","heading":"23.5 How power.prop.test works","text":"power.prop.test works much like power.t.test saw means., specify 4 following 5 elements comparison, R calculates fifth.sample size (interpreted # group, half total sample size)true probability group 1The true probability group 2The significance level (\\(\\alpha\\))power (1 - \\(\\beta\\))big weakness power.prop.test tool doesn’t allow work unbalanced designs.","code":""},{"path":"power-and-proportions.html","id":"a-revised-scenario","chapter":"23 Power and Proportions","heading":"23.6 A Revised Scenario","text":"Suppose can get exactly 800 subjects total (400 sharing 400 non-sharing). much power detect difference proportion positive skin test results two groups identical data presented larger, using one-sided test, \\(\\alpha\\) = .10?just 90% power detect effect.","code":"\npower.prop.test(n=400, p1=.174, p2=.247, sig.level = 0.10,\n                alternative=\"one.sided\")\n     Two-sample comparison of proportions power calculation \n\n              n = 400\n             p1 = 0.174\n             p2 = 0.247\n      sig.level = 0.1\n          power = 0.8954262\n    alternative = one.sided\n\nNOTE: n is number in *each* group"},{"path":"power-and-proportions.html","id":"using-the-pwr-library-for-unbalanced-designs","chapter":"23 Power and Proportions","heading":"23.7 Using the pwr library for Unbalanced Designs","text":"pwr.2p2n.test function pwr library can help assess power test determine particular effect size using unbalanced design, \\(n_1\\) equal \\(n_2\\)., specify four following five elements comparison, R calculates fifth.n1 = sample size group 1n2 = sample size group 2sig.level = significance level (\\(\\alpha\\))power = power (1 - \\(\\beta\\))h = effect size h, can calculated separately R based two proportions compared: p1 p2.","code":""},{"path":"power-and-proportions.html","id":"calculating-the-effect-size-h","chapter":"23 Power and Proportions","heading":"23.7.1 Calculating the Effect Size h","text":"calculate effect size given set proportions, just use ES.h(p1, p2) available pwr library.instance, comparison, following effect size.","code":"\npwr::ES.h(p1 = .174, p2 = .247)[1] -0.1796783"},{"path":"power-and-proportions.html","id":"using-pwr.2p2n.test-in-r","chapter":"23 Power and Proportions","heading":"23.8 Using pwr.2p2n.test in R","text":"Suppose can 700 samples group 1 (sharing group) half many group 2 (group users share needles). much power detect difference (\\(p_1\\) = .174, \\(p_2\\) = .247) 5% significance level two-sided test?Note headline output actually reads:appears 78% power circumstances.","code":"\npwr::pwr.2p2n.test(h = pwr::ES.h(p1 = .174, p2 = .247), \n              n1 = 700, n2 = 350,\n              sig.level = 0.05)\n     difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.1796783\n             n1 = 700\n             n2 = 350\n      sig.level = 0.05\n          power = 0.7836768\n    alternative = two.sided\n\nNOTE: different sample sizesdifference of proportion power calculation for binomial distribution \n(arcsine transformation) "},{"path":"power-and-proportions.html","id":"comparison-to-balanced-design","chapter":"23 Power and Proportions","heading":"23.8.1 Comparison to Balanced Design","text":"compare results balanced design using 1000 drug users total, 500 patients group?instead used…Note two sample size estimation approaches approximations, use slightly different approaches, ’s surprising answers similar, completely identical.","code":"\npwr::pwr.2p2n.test(h = pwr::ES.h(p1 = .174, p2 = .247), \n                   n1 = 500, n2 = 500, sig.level = 0.05)\n     difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.1796783\n             n1 = 500\n             n2 = 500\n      sig.level = 0.05\n          power = 0.8108416\n    alternative = two.sided\n\nNOTE: different sample sizes\npower.prop.test(p1 = .174, p2 = .247, \n                sig.level = 0.05, n = 500)\n     Two-sample comparison of proportions power calculation \n\n              n = 500\n             p1 = 0.174\n             p2 = 0.247\n      sig.level = 0.05\n          power = 0.8091808\n    alternative = two.sided\n\nNOTE: n is number in *each* group"},{"path":"larger-contingency-tables.html","id":"larger-contingency-tables","chapter":"24 Larger Contingency Tables","heading":"24 Larger Contingency Tables","text":"tables describing data two categories time, returning notion independent (rather paired matched) samples? chi-square tests already seen twobytwo table output extend nicely scenario, especially Pearson \\(\\chi^2\\) (asymptotic) test.","code":""},{"path":"larger-contingency-tables.html","id":"a-2x3-table-comparing-response-to-active-vs.-placebo","chapter":"24 Larger Contingency Tables","heading":"24.1 A 2x3 Table: Comparing Response to Active vs. Placebo","text":"table , containing 2 rows 3 columns data (ignoring marginal totals) specifies number patients show complete, partial, response treatment either active medication placebo.statistically significant association ? say, statistically significant difference treatment groups distribution responses?","code":""},{"path":"larger-contingency-tables.html","id":"getting-the-table-into-r","chapter":"24 Larger Contingency Tables","heading":"24.1.1 Getting the Table into R","text":"answer , ’ll get data contingency table matrix R. ’s one approach…","code":"\nT1 <- matrix(c(16,26,29,24,26,18), ncol=3, nrow=2, byrow=TRUE)\nrownames(T1) <- c(\"Active\", \"Placebo\")\ncolnames(T1) <- c(\"None\", \"Partial\", \"Complete\")\n\nT1        None Partial Complete\nActive    16      26       29\nPlacebo   24      26       18"},{"path":"larger-contingency-tables.html","id":"manipulating-the-tables-presentation","chapter":"24 Larger Contingency Tables","heading":"24.1.2 Manipulating the Table’s presentation","text":"can add margins matrix get table including row column totals.Instead counts, can tabulate proportion patients within cell.Now, actually obtain p value perform significance test \\(H_0\\): rows columns independent vs. \\(H_A\\): rows columns associated, simply run Pearson chi-square test T1 …Thanks p-value 0.13 (using Pearson chi-square test) conclusion retain null hypothesis independence setting.run Fisher’s exact test, , needed .Fisher exact test p value also 0.13. Either way, insufficient evidence conclude (true) difference distributions responses.","code":"\naddmargins(T1)        None Partial Complete Sum\nActive    16      26       29  71\nPlacebo   24      26       18  68\nSum       40      52       47 139\nprop.table(T1)             None   Partial  Complete\nActive  0.1151079 0.1870504 0.2086331\nPlacebo 0.1726619 0.1870504 0.1294964\nchisq.test(T1)\n    Pearson's Chi-squared test\n\ndata:  T1\nX-squared = 4.1116, df = 2, p-value = 0.128\nfisher.test(T1)\n    Fisher's Exact Test for Count Data\n\ndata:  T1\np-value = 0.1346\nalternative hypothesis: two.sided"},{"path":"larger-contingency-tables.html","id":"accuracy-of-death-certificates-a-6x3-table","chapter":"24 Larger Contingency Tables","heading":"24.2 Accuracy of Death Certificates (A 6x3 Table)","text":"table compiles data six studies designed investigate accuracy death certificates. original citation Kircher T, Nelson J, Burdo H (1985) autopsy measure accuracy death certificate. NEJM, 313, 1263-1269. 5373 autopsies compared causes death listed certificates. , 3726 confirmed accurate, 783 either lacked information contained inaccuracies require recoding underlying cause death, 864 incorrect required recoding. results across studies appear consistent?","code":""},{"path":"larger-contingency-tables.html","id":"the-pearson-chi-square-test-of-independence","chapter":"24 Larger Contingency Tables","heading":"24.3 The Pearson Chi-Square Test of Independence","text":"can assess homogeneity confirmation results (columns) observe table using Pearson chi-squared test independence.null hypothesis rows columns independent.alternative hypothesis association rows columns.see potential heterogeneity across rows data, perhaps also look proportions autopsies three accuracy categories study.three studies, approximately 3/4 results confirmed. three, 45%, 58% 61% confirmed. looks like ’s fair amount variation results across studies. see true, formally, run Pearson’s chi-square test independence, null hypothesis rows columns independent, alternative hypothesis association rows columns.chi-square test statistic 200 10 degrees freedom, yielding p < 0.0001.Autopsies performed random; fact, many done cause death listed certificate uncertain. problems may arise attempt use results studies make inference population whole?","code":"\ndeath.tab <- matrix(c(2040,367,327,149,60,48,288,25,70,703,\n                      197,252,425,62,88,121,72,79), byrow=TRUE, nrow=6)\nrownames(death.tab) <- c(\"1955-65\", \"1970\", \"1970-71\", \"1975-77\", \"1977-78\",\n                 \"1980\")\ncolnames(death.tab) <- c(\"Confirmed\", \"Inaccurate\", \"Incorrect\")\n \naddmargins(death.tab)        Confirmed Inaccurate Incorrect  Sum\n1955-65      2040        367       327 2734\n1970          149         60        48  257\n1970-71       288         25        70  383\n1975-77       703        197       252 1152\n1977-78       425         62        88  575\n1980          121         72        79  272\nSum          3726        783       864 5373\naddmargins(round(100*prop.table(death.tab,1),1),2)         Confirmed Inaccurate Incorrect Sum\n1955-65      74.6       13.4      12.0 100\n1970         58.0       23.3      18.7 100\n1970-71      75.2        6.5      18.3 100\n1975-77      61.0       17.1      21.9 100\n1977-78      73.9       10.8      15.3 100\n1980         44.5       26.5      29.0 100\nchisq.test(death.tab)\n    Pearson's Chi-squared test\n\ndata:  death.tab\nX-squared = 209.09, df = 10, p-value < 2.2e-16"},{"path":"larger-contingency-tables.html","id":"three-way-tables-a-2x2xk-table-and-a-mantel-haenszel-analysis","chapter":"24 Larger Contingency Tables","heading":"24.4 Three-Way Tables: A 2x2xK Table and a Mantel-Haenszel Analysis","text":"material discuss section attributable Jeff Simonoff book Analyzing Categorical Data. example taken Section 8.1 book.three-dimensional three-way table counts often reflects situation rows columns refer variables whose association primary interest us, third factor (layer, strata) describes control variable, whose effect primary association something controlling analysis.","code":""},{"path":"larger-contingency-tables.html","id":"smoking-and-mortality-in-the-uk","chapter":"24 Larger Contingency Tables","heading":"24.4.1 Smoking and Mortality in the UK","text":"early 1970s 20 years later, Whickham, United Kingdom, surveys yielded following relationship whether person smoker time original survey whether still alive 20 years later.’s two--two table analysis.detectable association smoking mortality, isn’t one might expect.odds ratio 0.68, implying odds lived 68% large non-smokers smokers.mean smoking good ?likely. key “lurking” variable - variable related smoking mortality obscuring actual relationship - namely, age.","code":"\nwhickham1 <- matrix(c(502, 230, 443, 139), byrow=TRUE, nrow=2)\nrownames(whickham1) <- c(\"Non-Smoker\", \"Smoker\")\ncolnames(whickham1) <- c(\"Alive\", \"Dead\")\naddmargins(whickham1)           Alive Dead  Sum\nNon-Smoker   502  230  732\nSmoker       443  139  582\nSum          945  369 1314\nEpi::twoby2(whickham1)2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Alive \nComparing : Non-Smoker vs. Smoker \n\n           Alive Dead    P(Alive) 95% conf. interval\nNon-Smoker   502  230      0.6858    0.6512   0.7184\nSmoker       443  139      0.7612    0.7248   0.7941\n\n                                    95% conf. interval\n             Relative Risk:  0.9010    0.8427   0.9633\n         Sample Odds Ratio:  0.6848    0.5353   0.8761\nConditional MLE Odds Ratio:  0.6850    0.5307   0.8822\n    Probability difference: -0.0754   -0.1230  -0.0266\n\n             Exact P-value: 0.0030 \n        Asymptotic P-value: 0.0026 \n------------------------------------------------------"},{"path":"larger-contingency-tables.html","id":"the-whickham-data-with-age-too","chapter":"24 Larger Contingency Tables","heading":"24.4.2 The whickham data with age, too","text":"table gives mortality experience separated subtables initial age group.sample odds ratios remaining Alive comparing Non-Smokers Smokers subtables (except last one, odds ratio undefined zero cells) calculated using usual cross-product ratio approach, yields following results.Thus, age groups except 25-34 year olds, smoking appears associated higher odds remaining alive.? surprisingly, strong association age mortality, mortality rates low young people (2.5% 18-24 year olds) increasing 100% 75+ year olds.also association age smoking, smoking rates peaking 45-54 year old range falling rapidly. particular, respondents 65 older time first survey low smoking rates (25.4%) high mortality rates (85.5%). Smoking hardly cause, however, since even among 65-74 year olds mortality higher among smokers (80.6%) among non-smokers (78.3%). flat version table (ftable R) can help us calculations.","code":"\nage <- c(rep(\"18-24\", 4), rep(\"25-34\", 4), \n         rep(\"35-44\", 4), rep(\"45-54\", 4), \n         rep(\"55-64\", 4), rep(\"65-74\", 4), \n         rep(\"75+\", 4))\nsmoking <- c(rep(c(\"Smoker\", \"Smoker\", \"Non-Smoker\", \"Non-Smoker\"), 7))\nstatus <- c(rep(c(\"Alive\", \"Dead\"), 14))\ncounts <- c(53, 2, 61, 1, 121, 3, 152, 5,\n            95, 14, 114, 7, 103, 27, 66, 12,\n            64, 51, 81, 40, 7, 29, 28, 101,\n            0, 13, 0, 64)\n\nwhickham2 <- tibble(smoking, status, age, counts) %>%\n    mutate(smoking = factor(smoking),\n           status = factor(status),\n           age = factor(age))\nwhickham2# A tibble: 28 x 4\n   smoking    status age   counts\n   <fct>      <fct>  <fct>  <dbl>\n 1 Smoker     Alive  18-24     53\n 2 Smoker     Dead   18-24      2\n 3 Non-Smoker Alive  18-24     61\n 4 Non-Smoker Dead   18-24      1\n 5 Smoker     Alive  25-34    121\n 6 Smoker     Dead   25-34      3\n 7 Non-Smoker Alive  25-34    152\n 8 Non-Smoker Dead   25-34      5\n 9 Smoker     Alive  35-44     95\n10 Smoker     Dead   35-44     14\n# ... with 18 more rows\nwhick_t2 <- \n    xtabs(counts ~ smoking + status + age, data = whickham2)\n\nwhick_t2, , age = 18-24\n\n            status\nsmoking      Alive Dead\n  Non-Smoker    61    1\n  Smoker        53    2\n\n, , age = 25-34\n\n            status\nsmoking      Alive Dead\n  Non-Smoker   152    5\n  Smoker       121    3\n\n, , age = 35-44\n\n            status\nsmoking      Alive Dead\n  Non-Smoker   114    7\n  Smoker        95   14\n\n, , age = 45-54\n\n            status\nsmoking      Alive Dead\n  Non-Smoker    66   12\n  Smoker       103   27\n\n, , age = 55-64\n\n            status\nsmoking      Alive Dead\n  Non-Smoker    81   40\n  Smoker        64   51\n\n, , age = 65-74\n\n            status\nsmoking      Alive Dead\n  Non-Smoker    28  101\n  Smoker         7   29\n\n, , age = 75+\n\n            status\nsmoking      Alive Dead\n  Non-Smoker     0   64\n  Smoker         0   13\nftable(whick_t2)                  age 18-24 25-34 35-44 45-54 55-64 65-74 75+\nsmoking    status                                            \nNon-Smoker Alive         61   152   114    66    81    28   0\n           Dead           1     5     7    12    40   101  64\nSmoker     Alive         53   121    95   103    64     7   0\n           Dead           2     3    14    27    51    29  13"},{"path":"larger-contingency-tables.html","id":"checking-assumptions-the-woolf-test","chapter":"24 Larger Contingency Tables","heading":"24.4.3 Checking Assumptions: The Woolf test","text":"can also obtain test (using woolf_test function, vcd library) see common odds ratio estimated Mantel-Haenszel procedure reasonable age groups. words, Woolf test test assumption homogeneous odds ratios across six age groups.Woolf test significant, suggests Cochran-Mantel-Haenszel test appropriate, since odds ratios smoking mortality vary much sub-tables age group. , following log odds ratios (estimated using conditional maximum likelihood, rather cross-product ratios) associated Woolf test.can see, Woolf test close usual standards statistically detectable results, implying common odds ratio least potentially reasonable age groups (least ones ages 75, data available.)","code":"\n## Next two results use the vcd package\nvcd::oddsratio(whick_t2, log = TRUE)log odds ratios for smoking and status by age \n\n      18-24       25-34       35-44       45-54       55-64 \n 0.65018114 -0.22473479  0.84069420  0.34608770  0.47421763 \n      65-74         75+ \n 0.09933253 -1.56397554 \nvcd::woolf_test(whick_t2)\n    Woolf-test on Homogeneity of Odds Ratios (no 3-Way\n    assoc.)\n\ndata:  whick_t2\nX-squared = 3.2061, df = 6, p-value = 0.7826"},{"path":"larger-contingency-tables.html","id":"the-cochran-mantel-haenszel-test","chapter":"24 Larger Contingency Tables","heading":"24.4.4 The Cochran-Mantel-Haenszel Test","text":", marginal table looking smoking mortality combining age groups isn’t meaningful summary relationship smoking mortality. Instead, need look conditional association smoking mortality, given age, address interests.null hypothesis , population, smoking mortality independent within strata formed age group. words, \\(H_0\\) requires smoking value predicting mortality age accounted .alternative hypothesis , population, smoking mortality associated within strata formed age group. words, \\(H_A\\) requires smoking least value predicting mortality even age accounted .can consider evidence helps us choose two hypotheses Cochran-Mantel-Haenszel test, obtained R mantelhaen.test function. test requires us assume , population within age group, smoking-mortality odds ratio . Essentially, means association smoking mortality older younger people.Cochran-Mantel-Haenszel test statistic bit larger 5 (continuity correction) leading p value 0.02, indicating strong rejection null hypothesis conditional independence smoking survival given age.estimated common conditional odds ratio 1.53. implies (adjusting age) non-smoker associated 53% higher odds alive 20 years later smoker.90% confidence interval common odds ratio (1.14, 2.04), reinforcing rejection null hypothesis conditional independence (odds ratio 1).","code":"\nmantelhaen.test(whick_t2, conf.level  = 0.90)\n    Mantel-Haenszel chi-squared test with continuity\n    correction\n\ndata:  whick_t2\nMantel-Haenszel X-squared = 5.435, df = 1, p-value =\n0.01974\nalternative hypothesis: true common odds ratio is not equal to 1\n90 percent confidence interval:\n 1.143198 2.041872\nsample estimates:\ncommon odds ratio \n          1.52783 "},{"path":"larger-contingency-tables.html","id":"without-the-continuity-correction","chapter":"24 Larger Contingency Tables","heading":"24.4.5 Without the Continuity Correction","text":"default, R presents Mantel-Haenszel test continuity correction, used 2x2xK table. virtually cases, go ahead , can see , difference makes case modest.","code":"\nmantelhaen.test(whick_t2, correct=FALSE, conf.level = 0.90)\n    Mantel-Haenszel chi-squared test without continuity\n    correction\n\ndata:  whick_t2\nMantel-Haenszel X-squared = 5.8443, df = 1, p-value =\n0.01563\nalternative hypothesis: true common odds ratio is not equal to 1\n90 percent confidence interval:\n 1.143198 2.041872\nsample estimates:\ncommon odds ratio \n          1.52783 "},{"path":"analysis-of-variance.html","id":"analysis-of-variance","chapter":"25 Analysis of Variance","heading":"25 Analysis of Variance","text":"Recall National Youth Fitness Survey, explored small piece detail earlier notes. ’ll look different part survey - specifically 280 children whose data captured nyfs2 file.","code":"\nnyfs2 <- read_csv(\"data/nyfs2.csv\",\n                  show_col_types = FALSE)\n\nnyfs2# A tibble: 280 x 21\n   subject.id sex    age.exam race.eth   english income.cat3\n        <dbl> <chr>     <dbl> <chr>        <dbl> <chr>      \n 1      73228 Male          4 5 Other o~       1 Low (below~\n 2      72393 Male          4 2 Non-His~       1 Low (below~\n 3      73303 Male          3 2 Non-His~       1 Low (below~\n 4      72786 Male          5 1 Non-His~       1 Low (below~\n 5      73048 Male          3 2 Non-His~       1 Low (below~\n 6      72556 Female        4 2 Non-His~       1 Low (below~\n 7      72580 Female        5 2 Non-His~       1 Low (below~\n 8      72532 Female        4 4 Other H~       0 Low (below~\n 9      73012 Male          4 1 Non-His~       1 Low (below~\n10      72099 Male          6 1 Non-His~       1 Low (below~\n# ... with 270 more rows, and 15 more variables:\n#   income.detail <chr>, inc.to.pov <dbl>, weight.kg <dbl>,\n#   height.cm <dbl>, bmi <dbl>, bmi.group <dbl>,\n#   bmi.cat <chr>, arm.length <dbl>, arm.circ <dbl>,\n#   waist.circ <dbl>, calf.circ <dbl>, calf.skinfold <dbl>,\n#   triceps.skinfold <dbl>, subscap.skinfold <dbl>,\n#   GMQ <dbl>"},{"path":"analysis-of-variance.html","id":"comparing-gross-motor-quotient-scores-by-income-level-3-categories","chapter":"25 Analysis of Variance","heading":"25.1 Comparing Gross Motor Quotient Scores by Income Level (3 Categories)","text":"first analysis, ’ll compare population mean Gross Motor Quotient evaluation kids across three groups defined income level. Higher values GMQ measure indicate improved levels gross motor development, terms locomotor object control. See https://wwwn.cdc.gov/Nchs/Nnyfs/Y_GMX.htm details.Uh, oh. rearrange income categories match natural order low high.working three independent samples, use graphs analogous built two independent samples.addition comparison boxplot, might consider faceted plots, like histograms., want ignore (modest) sample size differences, might consider density functions, perhaps ridgeline plot.","code":"\nnyfs2a <- nyfs2 %>%\n    select(subject.id, income.cat3, GMQ) %>%\n    arrange(subject.id)\nnyfs2a %>%\n    group_by(income.cat3) %>%\n    summarise(n = n(), mean(GMQ), median(GMQ))# A tibble: 3 x 4\n  income.cat3            n `mean(GMQ)` `median(GMQ)`\n  <chr>              <int>       <dbl>         <dbl>\n1 High (65K or more)    92        95.7            97\n2 Low (below 25K)       98        97.0            97\n3 Middle (25 - 64K)     90        95.4            94\nnyfs2a$income.cat3 <- \n    forcats::fct_relevel(nyfs2a$income.cat3,\n                         \"Low (below 25K)\",\n                         \"Middle (25 - 64K)\",\n                         \"High (65K or more)\")\nggplot(nyfs2a, aes(x = income.cat3, y = GMQ, fill = income.cat3)) +\n  geom_jitter(aes(color = income.cat3), alpha = 0.5, width = 0.25) +\n  geom_boxplot(notch = TRUE, alpha = 0.75) +\n  theme_bw() +\n  coord_flip() +\n  guides(fill = FALSE, col = FALSE) +\n  labs(title = \"GMQ Scores for 280 Children in NNYFS\",\n       y = \"GMQ Score, in points\", x = \"Income Category\")Warning: `guides(<scale> = FALSE)` is deprecated. Please use\n`guides(<scale> = \"none\")` instead.\nggplot(nyfs2a, aes(x = GMQ, fill = income.cat3)) +\n  geom_histogram(bins = 15, col = \"white\") +\n  guides(fill = FALSE) +\n  facet_wrap(~ income.cat3)Warning: `guides(<scale> = FALSE)` is deprecated. Please use\n`guides(<scale> = \"none\")` instead.\nggplot(nyfs2a, aes(x = GMQ, y = income.cat3, fill = income.cat3)) +\n    ggridges::geom_density_ridges(scale = 0.9) +\n    guides(fill = FALSE) + \n    labs(title = \"GMQ Score (in points) by Income Group\",\n         x = \"GMQ Score\", y = \"\") +\n    ggridges::theme_ridges()Warning: `guides(<scale> = FALSE)` is deprecated. Please use\n`guides(<scale> = \"none\")` instead.\nby(nyfs2a$GMQ, nyfs2a$income.cat3, mosaic::favstats)nyfs2a$income.cat3: Low (below 25K)\n min Q1 median  Q3 max     mean       sd  n missing\n  55 91     97 106 130 97.03061 14.79444 98       0\n--------------------------------------------- \nnyfs2a$income.cat3: Middle (25 - 64K)\n min Q1 median  Q3 max     mean       sd  n missing\n  67 85     94 106 136 95.36667 14.15123 90       0\n--------------------------------------------- \nnyfs2a$income.cat3: High (65K or more)\n min Q1 median  Q3 max     mean       sd  n missing\n  64 85     97 103 145 95.72826 14.49525 92       0"},{"path":"analysis-of-variance.html","id":"alternative-procedures-for-comparing-more-than-two-means","chapter":"25 Analysis of Variance","heading":"25.2 Alternative Procedures for Comparing More Than Two Means","text":"Now, two independent samples, ’d choosing pooled t test, Welch t test, non-parametric procedure like Wilcoxon-Mann-Whitney rank sum test, even perhaps bootstrap alternative.case two independent samples, methods analogous Welch test, rank sum test, even bootstrap, ’re going far likely select analysis variance (ANOVA) equivalent regression-based approach. extensions pooled t test. Unless sample outcome data clearly Normally distributed, transformation available makes appear approximately Normal groups comparing, stick ANOVA.","code":""},{"path":"analysis-of-variance.html","id":"extending-the-welch-test-to-2-independent-samples","chapter":"25 Analysis of Variance","heading":"25.2.1 Extending the Welch Test to > 2 Independent Samples","text":"possible extend Welch two-sample t test (assuming equal population variances) analogous one-factor analysis comparing population means based independent samples two groups.want compare population mean GMQ levels across three income groups without assuming equal population variances, oneway.test task. hypotheses tested :H0: three means vs.HA: least one population means different others.get p value, isn’t much help, though, don’t measure effect size, confidence intervals. Like analogous Welch t test, approach allows us forego assumption equal population variances three income groups, still requires us assume populations Normally distributed.said, time two levels factor interest, won’t bother worrying equal population variance assumption, just use one-factor ANOVA approach (pooled variances) described , make comparisons interest.","code":"\noneway.test(GMQ ~ income.cat3, data = nyfs2a)\n    One-way analysis of means (not assuming equal\n    variances)\n\ndata:  GMQ and income.cat3\nF = 0.3416, num df = 2.00, denom df = 184.41, p-value\n= 0.7111"},{"path":"analysis-of-variance.html","id":"extending-the-rank-sum-test-to-2-independent-samples","chapter":"25 Analysis of Variance","heading":"25.2.2 Extending the Rank Sum Test to > 2 Independent Samples","text":"also possible extend Wilcoxon-Mann-Whitney two-sample test analogous one-factor analysis called Kruskal-Wallis test comparing population measures location based independent samples two groups.want compare centers distributions population GMQ score across three income groups without assuming Normality, can use kruskal.test.hypotheses tested still , measure location population meanAgain, note isn’t much help, though, don’t measure effect size, confidence intervals.said, time two levels factor interest, won’t bother worrying potential violations Normality assumption unless glaring, just use usual one-factor ANOVA approach (pooled variances) described , make comparisons interest.","code":"\nkruskal.test(GMQ ~ income.cat3, data = nyfs2a)\n    Kruskal-Wallis rank sum test\n\ndata:  GMQ by income.cat3\nKruskal-Wallis chi-squared = 2.3202, df = 2, p-value\n= 0.3135"},{"path":"analysis-of-variance.html","id":"can-we-use-the-bootstrap-to-compare-more-than-two-means","chapter":"25 Analysis of Variance","heading":"25.2.3 Can we use the bootstrap to compare more than two means?","text":"Sure. ANOVA ANCOVA analogues using bootstrap, fact, power calculations based bootstrap, . want see example code, look https://sammancuso.com/2017/11/01/model-based-bootstrapped-anova--ancova/","code":""},{"path":"analysis-of-variance.html","id":"the-analysis-of-variance","chapter":"25 Analysis of Variance","heading":"25.3 The Analysis of Variance","text":"Extending two-sample t test (assuming equal population variances) comparison two samples uses analysis variance ANOVA.analysis continuous outcome variable basis single categorical factor, fact, ’s often called one-factor ANOVA one-way ANOVA indicate outcome split groups defined single factor.null hypothesis population means , alternative case. just two groups, boils F test equivalent Pooled t test.","code":""},{"path":"analysis-of-variance.html","id":"the-oneway.test-approach","chapter":"25 Analysis of Variance","heading":"25.3.1 The oneway.test approach","text":"R produce elements one-factor ANOVA using oneway.test command:isn’t full analysis, though, require complete ANOVA table. two equivalent approaches obtaining full ANOVA table comparing series 2 population means based independent samples.","code":"\noneway.test(GMQ ~ income.cat3, data = nyfs2a, var.equal=TRUE)\n    One-way analysis of means\n\ndata:  GMQ and income.cat3\nF = 0.34687, num df = 2, denom df = 277, p-value =\n0.7072"},{"path":"analysis-of-variance.html","id":"using-the-aov-approach-and-the-summary-function","chapter":"25 Analysis of Variance","heading":"25.3.2 Using the aov approach and the summary function","text":"’s one possible ANOVA table, doesn’t require directly fitting linear model.","code":"\nsummary(aov(GMQ ~ income.cat3, data = nyfs2a))             Df Sum Sq Mean Sq F value Pr(>F)\nincome.cat3   2    146   72.85   0.347  0.707\nResiduals   277  58174  210.01               "},{"path":"analysis-of-variance.html","id":"using-the-anova-function-after-fitting-a-linear-model","chapter":"25 Analysis of Variance","heading":"25.3.3 Using the anova function after fitting a linear model","text":"equivalent way get identical results slightly different format runs linear model behind ANOVA approach directly.","code":"\nanova(lm(GMQ ~ income.cat3, data = nyfs2a))Analysis of Variance Table\n\nResponse: GMQ\n             Df Sum Sq Mean Sq F value Pr(>F)\nincome.cat3   2    146  72.848  0.3469 0.7072\nResiduals   277  58174 210.014               "},{"path":"analysis-of-variance.html","id":"interpreting-the-anova-table","chapter":"25 Analysis of Variance","heading":"25.4 Interpreting the ANOVA Table","text":"","code":""},{"path":"analysis-of-variance.html","id":"what-are-we-testing","chapter":"25 Analysis of Variance","heading":"25.4.1 What are we Testing?","text":"null hypothesis ANOVA table population means outcome across various levels factor interest , two-sided alternative hypothesis level-specific population means .Specifically, grouping factor k levels, testing:H0: k population means .HA: least one population means different others.","code":""},{"path":"analysis-of-variance.html","id":"elements-of-the-anova-table","chapter":"25 Analysis of Variance","heading":"25.4.2 Elements of the ANOVA Table","text":"ANOVA table breaks variation outcome explained k levels factor interest, variation outcome remains (Residual, Error).Specifically, elements ANOVA table :degrees freedom (labeled Df) factor interest Residualsthe sums squares (labeled Sum Sq) factor interest Residualsthe mean square (labeled Mean Sq) factor interest Residualsthe ANOVA F test statistic (labeled F value), used generatethe p value comparison assessed ANOVA model, labeled Pr(>F)","code":""},{"path":"analysis-of-variance.html","id":"the-degrees-of-freedom","chapter":"25 Analysis of Variance","heading":"25.4.3 The Degrees of Freedom","text":"degrees freedom attributable factor interest (, Income category) number levels factor minus 1. , three Income categories (levels), df(income.cat3) = 2.total degrees freedom number observations (across levels factor) minus 1. 280 GMQ scores nyfs2a data, df(Total) must 279, although Total row isn’t shown R output.Residual degrees freedom Total df - Factor df. , , ’s 279 - 2 = 277.","code":"\nanova(lm(GMQ ~ income.cat3, data = nyfs2a))Analysis of Variance Table\n\nResponse: GMQ\n             Df Sum Sq Mean Sq F value Pr(>F)\nincome.cat3   2    146  72.848  0.3469 0.7072\nResiduals   277  58174 210.014               "},{"path":"analysis-of-variance.html","id":"the-sums-of-squares","chapter":"25 Analysis of Variance","heading":"25.4.4 The Sums of Squares","text":"sum squares (often abbreviated SS Sum Sq) represents variation explained.factor SS sum across levels factor sample size level multiplied squared difference level mean overall mean across levels. , SS(income.cat3) = 146The total SS sum across observations square difference individual values overall mean. , 146 + 58174 = 58320Residual SS = Total SS - Factor SS.Also interest calculation called \\(\\eta^2\\), (“eta-squared”), equivalent \\(R^2\\) linear model.\nSS(Factor) / SS(Total) = proportion variation outcome (, GMQ) explained variation groups (, income groups)\ncase, \\(\\eta^2\\) = 146 / (146 + 58174) = 146 / 58320 = 0.0025\n, Income Category alone accounts 0.25% variation GMQ levels observed data.\nSS(Factor) / SS(Total) = proportion variation outcome (, GMQ) explained variation groups (, income groups)case, \\(\\eta^2\\) = 146 / (146 + 58174) = 146 / 58320 = 0.0025So, Income Category alone accounts 0.25% variation GMQ levels observed data.","code":"\nanova(lm(GMQ ~ income.cat3, data = nyfs2a))Analysis of Variance Table\n\nResponse: GMQ\n             Df Sum Sq Mean Sq F value Pr(>F)\nincome.cat3   2    146  72.848  0.3469 0.7072\nResiduals   277  58174 210.014               "},{"path":"analysis-of-variance.html","id":"the-mean-square","chapter":"25 Analysis of Variance","heading":"25.4.5 The Mean Square","text":"Mean Square Sum Squares divided degrees freedom, MS(Factor) = SS(Factor)/df(Factor).case, MS(income.cat3) = SS(income.cat3)/df(income.cat3) = 146 / 2 = 72.848 (notice R maintains decimal places shows calculations) andMS(Residuals) = SS(Residuals) / df(Residuals) = 58174 / 277 = 210.014.\nMS(Residuals) MS(Error) estimate residual variance corresponds \\(\\sigma^2\\) underlying linear model outcome interest, GMQ.\nMS(Residuals) MS(Error) estimate residual variance corresponds \\(\\sigma^2\\) underlying linear model outcome interest, GMQ.","code":"\nanova(lm(GMQ ~ income.cat3, data = nyfs2a))Analysis of Variance Table\n\nResponse: GMQ\n             Df Sum Sq Mean Sq F value Pr(>F)\nincome.cat3   2    146  72.848  0.3469 0.7072\nResiduals   277  58174 210.014               "},{"path":"analysis-of-variance.html","id":"the-f-test-statistic-and-p-value","chapter":"25 Analysis of Variance","heading":"25.4.6 The F Test Statistic and p Value","text":"ANOVA F test obtained calculating MS(Factor) / MS(Residuals). case, F = 72.848 / 210.014 = 0.3469The F test statistic compared specific F distribution obtain p value, shown 0.7072Specifically, observed F test statistic compared F distribution numerator df = Factor df, denominator df = Residual df obtain p value.\n, SS(Factor) = 146 (approximately), df(Factor) = 2, leaving MS(Factor) = 72.848\nSS(Residual) = 58174, df(Residual) = 277, leaving MS(Residual) = 210.014\nMS(Factor) / MS(Residual) = F value = 0.3469, , compared F distribution 2 277 degrees freedom, yields p value 0.7072\n, SS(Factor) = 146 (approximately), df(Factor) = 2, leaving MS(Factor) = 72.848We SS(Residual) = 58174, df(Residual) = 277, leaving MS(Residual) = 210.014MS(Factor) / MS(Residual) = F value = 0.3469, , compared F distribution 2 277 degrees freedom, yields p value 0.7072","code":"\nanova(lm(GMQ ~ income.cat3, data = nyfs2a))Analysis of Variance Table\n\nResponse: GMQ\n             Df Sum Sq Mean Sq F value Pr(>F)\nincome.cat3   2    146  72.848  0.3469 0.7072\nResiduals   277  58174 210.014               "},{"path":"analysis-of-variance.html","id":"the-residual-standard-error","chapter":"25 Analysis of Variance","heading":"25.5 The Residual Standard Error","text":"residual standard error simply square root variance estimate MS(Residual). , MS(Residual) = 210.014, Residual standard error = 14.49 points.","code":""},{"path":"analysis-of-variance.html","id":"the-proportion-of-variance-explained-by-the-factor","chapter":"25 Analysis of Variance","heading":"25.6 The Proportion of Variance Explained by the Factor","text":"often summarize proportion variation explained factor. summary statistic called eta-squared (\\(\\eta^2\\)), equivalent \\(R^2\\) value seen previously linear regression models., \\(\\eta^2\\) = SS(Factor) / SS(Total), \n- SS(income.cat3) = 146 SS(Residuals) = 58174, SS(Total) = 58320\n- Thus, \\(\\eta^2\\) = SS(Factor)/SS(Total) = 146/58320 = 0.0025The income category accounts 0.25% variation GMQ levels: tiny fraction.","code":""},{"path":"analysis-of-variance.html","id":"the-regression-approach-to-compare-population-means-based-on-independent-samples","chapter":"25 Analysis of Variance","heading":"25.7 The Regression Approach to Compare Population Means based on Independent Samples","text":"approach equivalent ANOVA approach, thus also (just two samples compare) pooled-variance t test. run linear regression model predict outcome (, GMQ) basis categorical factor three levels (, income.cat3)","code":"\nsummary(lm(GMQ ~ income.cat3, data=nyfs2a))\nCall:\nlm(formula = GMQ ~ income.cat3, data = nyfs2a)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-42.031  -9.031  -0.031   8.969  49.272 \n\nCoefficients:\n                              Estimate Std. Error t value\n(Intercept)                     97.031      1.464  66.282\nincome.cat3Middle (25 - 64K)    -1.664      2.116  -0.786\nincome.cat3High (65K or more)   -1.302      2.104  -0.619\n                              Pr(>|t|)    \n(Intercept)                     <2e-16 ***\nincome.cat3Middle (25 - 64K)     0.432    \nincome.cat3High (65K or more)    0.536    \n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.49 on 277 degrees of freedom\nMultiple R-squared:  0.002498,  Adjusted R-squared:  -0.004704 \nF-statistic: 0.3469 on 2 and 277 DF,  p-value: 0.7072"},{"path":"analysis-of-variance.html","id":"interpreting-the-regression-output","chapter":"25 Analysis of Variance","heading":"25.7.1 Interpreting the Regression Output","text":"output tells us many things, now, ’ll focus just coefficients output, tells us :point estimate population mean GMQ score across “Low” income subjects 97.03the point estimate (sample mean difference) difference population mean GMQ level “Middle” “Low” income subjects -1.66 (words, Middle income kids lower GMQ scores Low income kids 1.66 points average.)point estimate (sample mean difference) difference population mean GMQ level “High” “Low” income subjects -1.30 (words, High income kids lower GMQ scores Low income kids 1.30 points average.)course, knew already summary sample means.model predicting GMQ based two binary (1/0) indicator variables, specifically, :Estimated GMQ = 97.03 - 1.66 x [1 Middle income 0 ] - 1.30 x [1 High income 0 ]coefficients section also provides standard error t statistic two-sided p value coefficient.","code":"\nnyfs2a %>%\n    group_by(income.cat3) %>%\n    summarise(n = n(), mean(GMQ))# A tibble: 3 x 3\n  income.cat3            n `mean(GMQ)`\n  <fct>              <int>       <dbl>\n1 Low (below 25K)       98        97.0\n2 Middle (25 - 64K)     90        95.4\n3 High (65K or more)    92        95.7"},{"path":"analysis-of-variance.html","id":"the-full-anova-table","chapter":"25 Analysis of Variance","heading":"25.7.2 The Full ANOVA Table","text":"see full ANOVA table corresponding linear regression model, run…","code":"\nanova(lm(GMQ ~ income.cat3, data=nyfs2a))Analysis of Variance Table\n\nResponse: GMQ\n             Df Sum Sq Mean Sq F value Pr(>F)\nincome.cat3   2    146  72.848  0.3469 0.7072\nResiduals   277  58174 210.014               "},{"path":"analysis-of-variance.html","id":"anova-assumptions","chapter":"25 Analysis of Variance","heading":"25.7.3 ANOVA Assumptions","text":"assumptions behind analysis variance behind linear model. specific interest :samples obtained group independent.Ideally, samples group random sample population described group.population, variance outcome group equal. (less issue study involves balanced design.)population, Normal distributions outcome group.Happily, F test fairly robust violations Normality assumption.","code":""},{"path":"analysis-of-variance.html","id":"equivalent-approach-to-get-anova-results","chapter":"25 Analysis of Variance","heading":"25.8 Equivalent approach to get ANOVA Results","text":"pairs means driving differences see?","code":"\nsummary(aov(GMQ ~ income.cat3, data = nyfs2a))             Df Sum Sq Mean Sq F value Pr(>F)\nincome.cat3   2    146   72.85   0.347  0.707\nResiduals   277  58174  210.01               "},{"path":"analysis-of-variance.html","id":"the-problem-of-multiple-comparisons","chapter":"25 Analysis of Variance","heading":"25.9 The Problem of Multiple Comparisons","text":"Suppose compare High Low, using test \\(\\alpha\\) = 0.05Then compare Middle Low outcome, also using \\(\\alpha\\) = 0.05Then compare High Middle, also \\(\\alpha\\) = 0.05What overall \\(\\alpha\\) level across three comparisons?bad 0.05 + 0.05 + 0.05, 0.15.Rather nominal 95% confidence, something low 85% confidence across set simultaneous comparisons.","code":""},{"path":"analysis-of-variance.html","id":"the-bonferroni-solution","chapter":"25 Analysis of Variance","heading":"25.9.1 The Bonferroni solution","text":"Suppose compare High Low, using test \\(\\alpha\\) = 0.05/3Then compare Middle Low outcome, also using \\(\\alpha\\) = 0.05/3Then compare High Middle, also \\(\\alpha\\) = 0.05/3Then across three comparisons, overall \\(\\alpha\\) can (worst)0.05/3 + 0.05/3 + 0.05/3 = 0.05So changing nominal confidence level 95% 98.333% comparison, wind least 95% confidence across set simultaneous comparisons.conservative (worst case) approach.Goal: Simultaneous comparisons White vs AA, AA vs White vs OtherThese p values large.","code":"\npairwise.t.test(nyfs2a$GMQ, nyfs2a$income.cat3, p.adjust=\"bonferroni\")\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  nyfs2a$GMQ and nyfs2a$income.cat3 \n\n                   Low (below 25K) Middle (25 - 64K)\nMiddle (25 - 64K)  1               -                \nHigh (65K or more) 1               1                \n\nP value adjustment method: bonferroni "},{"path":"analysis-of-variance.html","id":"pairwise-comparisons-using-tukeys-hsd-method","chapter":"25 Analysis of Variance","heading":"25.9.2 Pairwise Comparisons using Tukey’s HSD Method","text":"Goal: Simultaneous (less conservative) confidence intervals p values three pairwise comparisons (High vs. Low, High vs. Middle, Middle vs. Low)","code":"\nTukeyHSD(aov(GMQ ~ income.cat3, data = nyfs2a))  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = GMQ ~ income.cat3, data = nyfs2a)\n\n$income.cat3\n                                           diff       lwr\nMiddle (25 - 64K)-Low (below 25K)    -1.6639456 -6.649518\nHigh (65K or more)-Low (below 25K)   -1.3023514 -6.259595\nHigh (65K or more)-Middle (25 - 64K)  0.3615942 -4.701208\n                                          upr     p adj\nMiddle (25 - 64K)-Low (below 25K)    3.321627 0.7116745\nHigh (65K or more)-Low (below 25K)   3.654892 0.8098084\nHigh (65K or more)-Middle (25 - 64K) 5.424396 0.9845073"},{"path":"analysis-of-variance.html","id":"plotting-the-tukey-hsd-results","chapter":"25 Analysis of Variance","heading":"25.9.3 Plotting the Tukey HSD results","text":"Note default positioning y axis plot Tukey HSD results can problematic. longer names, particular, levels factor, R leave labels. can alleviate problem either using fct_recode function forcats package rename factor levels, can use following code reconfigure margins plot.","code":"\nplot(TukeyHSD(aov(GMQ ~ income.cat3, data = nyfs2a)))\nmar.default <- c(5,6,4,2) + 0.1 # save default plotting margins\n\npar(mar = mar.default + c(0, 12, 0, 0)) \nplot(TukeyHSD(aov(GMQ ~ income.cat3, data = nyfs2a)), las = 2)\npar(mar = mar.default) # return to normal plotting margins"},{"path":"analysis-of-variance.html","id":"what-if-we-consider-another-outcome-bmi","chapter":"25 Analysis of Variance","heading":"25.10 What if we consider another outcome, BMI?","text":"’ll look full data set nyfs2 now, can look BMI function income.descriptive numerical summaries:ANOVA table.Let’s consider Tukey HSD results. First, ’ll create factor shorter labels.appears detectable difference bmi means “Low” group “High” “Middle” group 90% confidence level, detectable difference “Middle” “High.” Details confidence intervals pairwise comparisons follow.","code":"\nnyfs2$income.cat3 <- \n    forcats::fct_relevel(nyfs2$income.cat3,\n                         \"Low (below 25K)\",\n                         \"Middle (25 - 64K)\",\n                         \"High (65K or more)\")\n\nggplot(nyfs2, aes(x = bmi, y = income.cat3, fill = income.cat3)) +\n    ggridges::geom_density_ridges(scale = 0.9) +\n    guides(fill = FALSE) + \n    labs(title = \"Body-Mass Index by Income Group\",\n         x = \"Body-Mass Index\", y = \"\") +\n    ggridges::theme_ridges()Warning: `guides(<scale> = FALSE)` is deprecated. Please use\n`guides(<scale> = \"none\")` instead.\nggplot(nyfs2, aes(x = income.cat3, y = bmi, fill = income.cat3)) +\n  geom_jitter(aes(color = income.cat3), alpha = 0.5, width = 0.25) +\n  geom_boxplot(notch = TRUE, alpha = 0.75) +\n  theme_bw() +\n  coord_flip() +\n  guides(fill = FALSE, col = FALSE) +\n  labs(title = \"BMI for 280 Children in NNYFS\",\n       y = \"Body-Mass Index\", x = \"Income Category\")Warning: `guides(<scale> = FALSE)` is deprecated. Please use\n`guides(<scale> = \"none\")` instead.\nmosaic::favstats(bmi ~ income.cat3, data = nyfs2)         income.cat3  min     Q1 median   Q3  max     mean\n1    Low (below 25K) 13.8 15.500  16.50 17.9 25.2 16.98163\n2  Middle (25 - 64K) 13.7 15.225  16.05 16.9 24.1 16.37111\n3 High (65K or more) 13.8 15.300  15.85 17.2 21.8 16.27065\n        sd  n missing\n1 2.194574 98       0\n2 1.898920 90       0\n3 1.614395 92       0\nanova(lm(bmi ~ income.cat3, data = nyfs2))Analysis of Variance Table\n\nResponse: bmi\n             Df  Sum Sq Mean Sq F value  Pr(>F)  \nincome.cat3   2   28.32 14.1583  3.8252 0.02298 *\nResiduals   277 1025.26  3.7013                  \n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nnyfs2$inc.new <- \n    forcats::fct_recode(nyfs2$income.cat3,\n                        \"Low\" = \"Low (below 25K)\",\n                        \"Middle\" = \"Middle (25 - 64K)\",\n                        \"High\" = \"High (65K or more)\")\n\nplot(TukeyHSD(aov(bmi ~ inc.new, data = nyfs2),\n                  conf.level = 0.90))\nTukeyHSD(aov(bmi ~ inc.new, data = nyfs2),\n                  conf.level = 0.90)  Tukey multiple comparisons of means\n    90% family-wise confidence level\n\nFit: aov(formula = bmi ~ inc.new, data = nyfs2)\n\n$inc.new\n                  diff        lwr         upr     p adj\nMiddle-Low  -0.6105215 -1.1893722 -0.03167084 0.0775491\nHigh-Low    -0.7109805 -1.2865420 -0.13541892 0.0306639\nHigh-Middle -0.1004589 -0.6882764  0.48735849 0.9339289"},{"path":"some-r-tips.html","id":"some-r-tips","chapter":"Some R Tips","heading":"Some R Tips","text":"","code":""},{"path":"some-r-tips.html","id":"using-data-from-an-r-package","chapter":"Some R Tips","heading":"Using data from an R package","text":"use data R package, instance, bechdel data fivethirtyeight package, can simply load relevant package library data frame available","code":"\nlibrary(fivethirtyeight)\n\nbechdel# A tibble: 1,794 x 15\n    year imdb  title test  clean_test binary budget domgross\n   <int> <chr> <chr> <chr> <ord>      <chr>   <int>    <dbl>\n 1  2013 tt17~ 21 &~ nota~ notalk     FAIL   1.3 e7 25682380\n 2  2012 tt13~ Dred~ ok-d~ ok         PASS   4.5 e7 13414714\n 3  2013 tt20~ 12 Y~ nota~ notalk     FAIL   2   e7 53107035\n 4  2013 tt12~ 2 Gu~ nota~ notalk     FAIL   6.1 e7 75612460\n 5  2013 tt04~ 42    men   men        FAIL   4   e7 95020213\n 6  2013 tt13~ 47 R~ men   men        FAIL   2.25e8 38362475\n 7  2013 tt16~ A Go~ nota~ notalk     FAIL   9.2 e7 67349198\n 8  2013 tt21~ Abou~ ok-d~ ok         PASS   1.2 e7 15323921\n 9  2013 tt18~ Admi~ ok    ok         PASS   1.3 e7 18007317\n10  2013 tt18~ Afte~ nota~ notalk     FAIL   1.3 e8 60522097\n# ... with 1,784 more rows, and 7 more variables:\n#   intgross <dbl>, code <chr>, budget_2013 <int>,\n#   domgross_2013 <dbl>, intgross_2013 <dbl>,\n#   period_code <int>, decade_code <int>"},{"path":"some-r-tips.html","id":"using-read_rds-to-read-in-an-r-data-set","chapter":"Some R Tips","heading":"Using read_rds to read in an R data set","text":"provided nnyfs.Rds data file course data page.Suppose downloaded data file directory computer called data sub-directory directory plan work, perhaps called 431-nnyfs.Open RStudio create new project 431-nnyfs directory computer. see data subdirectory Files window RStudio project created.Now, read nnyfs.Rds file new tibble R called nnyfs_new following command:results…","code":"\nnnyfs_new <- read_rds(\"data/nnyfs.Rds\")\nnnyfs_new# A tibble: 1,518 x 45\n    SEQN sex    age_child race_eth       educ_child language\n   <dbl> <fct>      <dbl> <fct>               <dbl> <fct>   \n 1 71917 Female        15 3_Black Non-H~          9 English \n 2 71918 Female         8 3_Black Non-H~          2 English \n 3 71919 Female        14 2_White Non-H~          8 English \n 4 71920 Female        15 2_White Non-H~          8 English \n 5 71921 Male           3 2_White Non-H~         NA English \n 6 71922 Male          12 1_Hispanic              6 English \n 7 71923 Male          12 2_White Non-H~          5 English \n 8 71924 Female         8 4_Other Race/~          2 English \n 9 71925 Male           7 1_Hispanic              0 English \n10 71926 Male           8 3_Black Non-H~          2 English \n# ... with 1,508 more rows, and 39 more variables:\n#   sampling_wt <dbl>, income_pov <dbl>, age_adult <dbl>,\n#   educ_adult <fct>, respondent <fct>, salt_used <fct>,\n#   energy <dbl>, protein <dbl>, sugar <dbl>, fat <dbl>,\n#   diet_yesterday <fct>, water <dbl>, plank_time <dbl>,\n#   height <dbl>, weight <dbl>, bmi <dbl>, bmi_cat <fct>,\n#   arm_length <dbl>, waist <dbl>, arm_circ <dbl>, ..."},{"path":"some-r-tips.html","id":"using-read_csv-to-read-in-a-comma-separated-version-of-a-data-file","chapter":"Some R Tips","heading":"Using read_csv to read in a comma-separated version of a data file","text":"provided nnyfs.csv data file course data page.Suppose downloaded data file directory computer called data sub-directory directory plan work, perhaps called 431-nnyfs.Open RStudio create new project 431-nnyfs directory computer. see data subdirectory Files window RStudio project created.Now, read nnyfs.csv file new tibble R called nnyfs_new2 following command:also want convert character variables factors, often want analyzing results, instead use:Note , example, sex race_eth now listed factor (fctr) variables. One place distinction character factor variables matters summarize data.","code":"\nnnyfs_new2 <- read_csv(\"data/nnyfs.csv\")Rows: 1518 Columns: 45-- Column specification ------------------------------------\nDelimiter: \",\"\nchr (18): sex, race_eth, language, educ_adult, responden...\ndbl (27): SEQN, age_child, educ_child, sampling_wt, inco...\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\nnnyfs_new2# A tibble: 1,518 x 45\n    SEQN sex    age_child race_eth       educ_child language\n   <dbl> <chr>      <dbl> <chr>               <dbl> <chr>   \n 1 71917 Female        15 3_Black Non-H~          9 English \n 2 71918 Female         8 3_Black Non-H~          2 English \n 3 71919 Female        14 2_White Non-H~          8 English \n 4 71920 Female        15 2_White Non-H~          8 English \n 5 71921 Male           3 2_White Non-H~         NA English \n 6 71922 Male          12 1_Hispanic              6 English \n 7 71923 Male          12 2_White Non-H~          5 English \n 8 71924 Female         8 4_Other Race/~          2 English \n 9 71925 Male           7 1_Hispanic              0 English \n10 71926 Male           8 3_Black Non-H~          2 English \n# ... with 1,508 more rows, and 39 more variables:\n#   sampling_wt <dbl>, income_pov <dbl>, age_adult <dbl>,\n#   educ_adult <chr>, respondent <chr>, salt_used <chr>,\n#   energy <dbl>, protein <dbl>, sugar <dbl>, fat <dbl>,\n#   diet_yesterday <chr>, water <dbl>, plank_time <dbl>,\n#   height <dbl>, weight <dbl>, bmi <dbl>, bmi_cat <chr>,\n#   arm_length <dbl>, waist <dbl>, arm_circ <dbl>, ...\nnnyfs_new3 <- read_csv(\"data/nnyfs.csv\") %>%\n    mutate(across(where(is.character), as_factor))Rows: 1518 Columns: 45-- Column specification ------------------------------------\nDelimiter: \",\"\nchr (18): sex, race_eth, language, educ_adult, responden...\ndbl (27): SEQN, age_child, educ_child, sampling_wt, inco...\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\nnnyfs_new3# A tibble: 1,518 x 45\n    SEQN sex    age_child race_eth       educ_child language\n   <dbl> <fct>      <dbl> <fct>               <dbl> <fct>   \n 1 71917 Female        15 3_Black Non-H~          9 English \n 2 71918 Female         8 3_Black Non-H~          2 English \n 3 71919 Female        14 2_White Non-H~          8 English \n 4 71920 Female        15 2_White Non-H~          8 English \n 5 71921 Male           3 2_White Non-H~         NA English \n 6 71922 Male          12 1_Hispanic              6 English \n 7 71923 Male          12 2_White Non-H~          5 English \n 8 71924 Female         8 4_Other Race/~          2 English \n 9 71925 Male           7 1_Hispanic              0 English \n10 71926 Male           8 3_Black Non-H~          2 English \n# ... with 1,508 more rows, and 39 more variables:\n#   sampling_wt <dbl>, income_pov <dbl>, age_adult <dbl>,\n#   educ_adult <fct>, respondent <fct>, salt_used <fct>,\n#   energy <dbl>, protein <dbl>, sugar <dbl>, fat <dbl>,\n#   diet_yesterday <fct>, water <dbl>, plank_time <dbl>,\n#   height <dbl>, weight <dbl>, bmi <dbl>, bmi_cat <fct>,\n#   arm_length <dbl>, waist <dbl>, arm_circ <dbl>, ...\nsummary(nnyfs_new2$race_eth)   Length     Class      Mode \n     1518 character character \nsummary(nnyfs_new3$race_eth)  3_Black Non-Hispanic   2_White Non-Hispanic \n                   338                    610 \n            1_Hispanic 4_Other Race/Ethnicity \n                   450                    120 "},{"path":"some-r-tips.html","id":"converting-character-variables-into-factors","chapter":"Some R Tips","heading":"Converting Character Variables into Factors","text":"command want create newdata olddata :factors, visit https://r4ds..co.nz/factors.html","code":"newdata <- olddata %>%\n    mutate(across(where(is.character), as_factor))"},{"path":"some-r-tips.html","id":"converting-data-frames-to-tibbles","chapter":"Some R Tips","heading":"Converting Data Frames to Tibbles","text":"Use as_tibble() simply tibble() assign attributes tibble data frame. Note read_rds read_csv automatically create tibbles.tibbles, visit https://r4ds..co.nz/tibbles.html.","code":""},{"path":"some-r-tips.html","id":"for-more-advice","chapter":"Some R Tips","heading":"For more advice","text":"Consider visiting software tutorials page R Data heading main web site.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
